<!DOCTYPE html>
<html>

  <head>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
        tagSide: "right"
      },
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
      }
    });
    MathJax.Hub.Register.StartupHook("TeX AMSmath Ready", function () {
      MathJax.InputJax.TeX.Stack.Item.AMSarray.Augment({
        clearTag() {
          if (!this.global.notags) {
            this.super(arguments).clearTag.call(this);
          }
        }
      });
    });
  </script>
  <script type="text/javascript" charset="utf-8"
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML">
  </script>


  <!-- <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>JS</title>
  <meta name="description" content="">

  <!-- <link href="/bootstrap/css/bootstrap.css" rel="stylesheet">
  <script src="/boostrap/js/bootstrap.js"></script> -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/">
  <link rel="alternate" type="application/rss+xml" title="JS" href="http://localhost:4000/feed.xml">

  

</head>


  <body>

    <!-- <header class="site-header"> -->
    <!-- <a class="site-title" href="/">JS</a> -->
<!-- </header> -->

    <nav class="navbar navbar-expand-lg navbar-light" style="background-color: #edf3f5;">
      <div class = "container">
        <a class="navbar-brand" href="/">JS</a></span> </a>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav ml-auto">
            <li class = "nav-item active"><a class="nav-link" href="/#research"><i class="fas fa-cogs"></i> Research</a></li>
            <li class = "nav-item active"><a class="nav-link" href="/#puzzles"><i class="fab fa-laravel"></i> Puzzles</a></li>
            <li class = "nav-item active"><a class="nav-link" href="/#posts"><i class="fas fa-seedling"></i> Blog</a></li>
          </ul> 
        </div>
      </div>
    </nav>

    <div class="page-content">
      <div class="wrapper">
        <link rel="stylesheet" href="css/hover_effects.css">
<style>
  a:link {
    color: #225599;
    background-color: transparent;
    text-decoration: none;
  }

  a:visited {
    color: #552299;
    background-color: transparent;
    text-decoration: none;
  }

  .hr-thin {
    border: none;
    height: 1px;
    color: #aaaaaa;
    background-color: #aaaaaa;
  }

  .hr-thick {
    border: none;
    height: 2px;
    color: #aaaaaa;
    background-color: #aaaaaa;
  }

  .blurb {
    font-size: 80%;
  }

  .paper-info {
    font-size: 80%;
    margin-top: -1em;
  }
</style>
<!-- <hr class="hr-thick"> -->
<!-- <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12"><div class="hovereffect"><img class="img-responsive" src="https://james-simon.github.io/img/headshot.png" alt=""><div class="overlay"><a class="info" href="#">link here</a></div></div></div>
 -->
<!-- <div class="container"><img src="https://mdbootstrap.com/img/Photos/Others/forest-sm.jpg" alt="Avatar" class="overlay-image"><div class="overlay"><div class="overlay-text">Hello World</div></div></div>
 -->
<br>
<div class="container">
  <a name="about"></a>

  <div class="row">
    <div class="col-md-3">
      <div class="mr-4">
        <div class="hovereffect">
          <img src="https://james-simon.github.io/img/headshot.png" height="210px">
          <div class="tint"></div>
          <a class="hoverinfo hover1" style="font-size:xx-small;" href="">This is me about to do a 100' rope swing off El Capitan in Yosemite</a>
          <a class="hoverinfo hover2" style="font-size:xx-small;" href="">Actually, I guess this was taken just after swinging, but the rest is right</a>
          <a class="hoverinfo hover3" style="font-size:xx-small;" href="">Well, actually, the swing was only 30', but everything else is definitely the truth</a>
          <a class="hoverinfo hover4" style="font-size:xx-small;" href="">Okay, fine, it wasn't El Capitan, it was just a rock in Delaware, but I swear the rest is real</a>
          <a class="hoverinfo hover5" style="font-size:xx-small;" href="">Fine, I didn't actually swing, I just held a rope, but the idea is the same</a>
          <a class="hoverinfo hover6" style="font-size:xx-small;" href="">Alright, alright, I'll come clean: the original story was true</a>
        </div>
      </div>
    </div>

    <div class="col-md-9">
      <h1>Jamie Simon</h1>
      <p>I'm a final-year PhD student in the physics department at UC Berkeley, aiming to use tools from theoretical physics to build fundamental understanding of deep neural networks. I'm advised by <a href="https://deweeselab.com/">Mike DeWeese</a> and supported by an NSF Graduate Research Fellowship. I'm also a research fellow at <a href="https://imbue.com/">Imbue</a>. In my free time, I like running, puzzles, spending time in forests, and <a href="/blog/gravitrees">balancing things</a>.
      <br>
      <br>
      A lot of my papers have come about from helping people doing (mostly) empirical ML research come up with good minimal theoretical toy models that explain effects they're seeing
      [<a href="https://arxiv.org/abs/2003.10397">1</a>,<a href="https://arxiv.org/abs/2207.06569">2</a>,<a href="https://arxiv.org/abs/2107.11774">3</a>,<a href="https://arxiv.org/abs/2306.13185">4</a>,<a href="https://arxiv.org/abs/2410.04642">5</a>].
      Some of these ideas came about from quick initial conversations! If you've got a curious empirical phenomenon you're trying to explain, feel free to reach out :)
  	  </p>
  	  <p>
  	  If you'd like get emailed when I post new blogposts, you can sign up <a href="https://forms.gle/mioQ8Rhb51JcGGSJ6">here.</a>
  	  </p>
    </div>

  </div>

  <hr class="hr-thin">
  <a name="research"></a>
  <h2>Research</h2>
  <hr class="hr-thick">

  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="/img/more_is_better_mainpage_figure.png" width=100%>
    </div>
    <div class="col-md-9">
      <h3>More is better: when infinite overparameterization is optimal and overfitting is obligatory</h3>
      <strong>Explaining mysteries of modern ML with the toy model of RF regression</strong>
      <p class="blurb"> We give a theory of the generalization of random feature models. We conclude they perform better with more parameters, more data, and less regularization, putting theoretical backing to these observations in modern ML. This gives a fairly solid mathematical picture to replace classical intuitions about the risks of overparameterization and overfitting. Our picture of the generalization of RF models takes the form of some nice closed-form equations we think can be used to answer lots of other questions. </p>
      <p class="paper-info">
        <strong>ICLR '24</strong>
        <a href="https://tinyurl.com/more-is-better">[arXiv]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">

  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="/img/spectral_scaling_mainpage_figure.png" width=100%>
    </div>
    <div class="col-md-9">
      <h3>A spectral condition for feature learning</h3>
      <strong> A simple picture of feature learning in wide nets </strong>
      <p class="blurb"> We give a simple scaling treatment of feature learning in wide networks in terms of the spectral norm of weight matrices. If you want to understand the "mu-parameterization," this is probably the easiest place to start ca. 2024. </p>
      <p class="paper-info">
        <a href="https://tinyurl.com/feature-learning">[arXiv]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">

  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="/img/stepwise_mainpage_figure.png" width=100%>
    </div>
    <div class="col-md-9">
      <h3>On the stepwise nature of self-supervised learning</h3>
      <strong>A theory of the training dynamics of contrastive learning</strong>
      <p class="blurb"> We give a theory of the training dynamics of contrastive self-supervised learning and validate it empirically. We find that representations are learned <i> one dimension at a time in a stepwise fashion </i> -- that is, the rank of the model's final representations increases by one at each step. Our theory is derived for linearized models, but we clearly see our stepwise phenomenon even for ResNets trained on image data. Large-scale ML mostly uses unsupervised and self-supervised training these days, and we describe a behavior we think is fairly generic in SSL. </p>
      <p class="paper-info">
        <strong>ICML '23</strong>
        <a href="https://tinyurl.com/stepwise-ssl">[arXiv]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">

  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="/img/fedex_attack_mainpage_figure.png" width=100%>
    </div>
    <div class="col-md-9">
      <h3>You can just put up a poster at ICML and nobody will stop you</h3>
      <strong>Exposing vulnerabilities in the conference review system</strong>
      <p class="blurb"> A perennial problem in machine learning research has been how to most efficiently have a poster at ICML. In this work, we show that one can bypass OpenReview via a simple â€œFedEx trick," similar to yet entirely different from the kernel trick in machine learning. (I'm putting this here as an easter egg to see if people actually read these. If you see this and saw + were amused by the original poster, feel free to drop me a line :)) </p>
      <p class="paper-info">
        <strong>ICML '23</strong>
        <a href="https://x.com/_dsevero/status/1684677903382982656">[viral tweet]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">

  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="/img/eigenlearning_mainpage_figure.png" width=80%>
    </div>
    <div class="col-md-9">
      <h3>The eigenlearning framework: a conservation law perspective on kernel regression and wide neural networks</h3>
      <p class="blurb"> We give a simple picture of the generalization of kernel ridge regression in terms of task eigenstructure and use it to solve some theoretical problems. </p>
      <p class="paper-info">
        <strong>TMLR</strong>
        <a href="https://arxiv.org/abs/2110.03922">[arXiv]</a>
        <a href="https://github.com/james-simon/eigenlearning">[code]</a>
        <a href="https://bair.berkeley.edu/blog/2021/10/25/eigenlearning/">[blog]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">

  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="https://james-simon.github.io/img/shallow_learning_sketch.png" width=100%>
    </div>
    <div class="col-md-9">
      <h3>Reverse engineering the neural tangent kernel</h3>
      <strong>A first-principles method for the design of fully-connected architectures</strong>
      <p class="blurb"> Much of our understanding of artificial neural networks stems from the fact that, in the infinite-width limit, they turn out to be equivalent to a class of simple models called <i>kernel regression.</i> Given a wide network architecture, it's well-known how to find the equivalent kernel method, allowing us to study popular models in the infinite-width limit. In work with Sajant Anand, we <i>invert this mapping</i> for fully-connected nets (FCNs), allowing one to start from a desired rotation-invariant kernel and design a network (i.e. choose an activation function) to achieve it. Remarkably, achieving any such kernel requires only one hidden layer, raising questions about conventional wisdom on the benefits of depth. This allows surprising experiments, like designing a 1HL FCN that trains and generalizes like a deep ReLU FCN. This ability to design nets with desired kernels is a step towards deriving good net architectures <i>from first principles</i>, a longtime dream of the field of machine learning. </p>
      <p class="paper-info">
        <strong>ICML '22</strong>
        <a href="https://arxiv.org/abs/2106.03186">[arXiv]</a>
        <a href="https://github.com/james-simon/reverse-engineering">[code]</a>
        <a href="https://james-simon.github.io/blog/reverse-engineering/">[blog]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">
  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="https://james-simon.github.io/img/tempered_overfitting.png" width=100%>
    </div>
    <div class="col-md-9">
      <h3>Benign, tempered or catastrophic: a taxonomy of overfitting</h3>
      <strong>How bad is neural network overfitting?</strong>
      <p class="blurb"> Classical wisdom holds that overparameterization is harmful. Neural nets defy this wisdom, generalizing well despite their overparameterization and interpolation of the training data. How can we understand this discrepancy? Recent landmark papers have explored the concept of <i>benign overfitting</i> -- a phenomenon in which certain models can interpolate noisy data without harming generalization -- suggesting that that neural nets may fit benignly. In this work with <a href="https://mallinar.xyz/">Neil Mallinar</a>, <a href="https://preetum.nakkiran.org/">Preetum Nakkiran</a>, and others, we put this idea to the empirical test, giving a new characterization of neural network overfitting and noise sensitivity. We find that neural networks trained to interpolation do <i>not</i> overfit benignly, but neither do they exhibit the catastrophic overfitting foretold by classical wisdom: instead, they usually lie in a third, intermediate regime we call <i>tempered overfitting</i>. I found that we can understand these three regimes of overfitting analytically for kernel regression (a toy model for neural networks), and I proved a simple "trichotomy theorem" relating a kernel's eigenspectrum to its overfitting behavior. </p>
      <p class="paper-info">
        <strong>
          <em>NeurIPS '22</em>
        </strong>
        <a href="https://arxiv.org/abs/2207.06569">[arXiv]</a>
        <!-- <a href="https://github.com/james-simon/shallow-learning">[code]</a> -->
      </p>
    </div>
  </div>

<!--   <hr class="hr-thick">
  <div class="row">
    <div class="col-md-3">
      <br>
      <video autoplay loop muted playsinline width="100%" style="display:block; margin: 0 auto;">
        <source src="https://james-simon.github.io/img/eigenlearning_exp_matches_th_small.mp4" type="video/mp4">
      </video>
    </div>
    <div class="col-md-9">
      <h3>The eigenlearning framework: a conservation law perspective on kernel regression and wide neural networks</h3>
      <strong>A simple theory of the generalization of kernel ridge regression</strong>
      <p class="blurb"> Of all the many mysteries of modern neural networks, perhaps the greatest is the question of generalization: why do the functions learned by neural networks generalize so well to new data? "Why" questions can be difficult to pin down, so in recent joint work with Maddie Dickens, we took up a more scientific question: <i>can we predict, from first principles, how well a given net will generalize on a given function?</i> It turns out that recent breakthroughs have essentially answered this question for kernel ridge regression, furnishing accurate approximations for several key metrics of neural network generalization. In this work, we give a simpler derivation of these important results, ascribe a new interpretation to the final theory, and discuss several new applications of the theory to shed light on various aspects of neural network generalization, including double descent and the hardness of the classic parity problem. Key to our formulation is a simple conservation law, latent in kernel regression, which limits the total "learnability" of any orthonormal basis of target functions. Our theory is transparent enough to give intuitive insights into when and why a neural network generalizes well.
      <p class="paper-info">
        <strong>(2021)</strong>
        <strong>
          <em>TMLR</em>
        </strong>
        <a href="https://arxiv.org/abs/2110.03922">[arXiv]</a>
        <a href="https://github.com/james-simon/eigenlearning">[code]</a>
        <a href="https://bair.berkeley.edu/blog/2021/10/25/eigenlearning/">[blog]</a>
      </p>
    </div>
  </div> -->
  <!--    <hr class="hr-thick"><div class="row"><div  class="col-md-3"><img class="mr-4" src="https://james-simon.github.io/img/GRFtest.gif" width="100%"></div><div class="col-md-9"><h3>A phenomenological theory of high-dimensional optimization</h3><p class="blurb">
            I'm currently working on a phenomenological field-theory-based model of high-dimensional loss surfaces that'll hopefully faithfully capture several emprical features of real neural net loss surfaces. I aim for my model to explain the Hessian loss-index relationship, agree with findings that datasets have <a href="https://arxiv.org/abs/1804.08838">intrinsic dimensions</a>, and shed light on <a href="https://arxiv.org/abs/1802.10026">mode connectivity</a>. This research direction was the subject of my proposal for the NSF-GRFP.
         </p></div></div> -->
  <!--    <hr class="hr-thick"><div class="row"><div class="col-md-3"><img class="mr-4" src="https://james-simon.github.io/img/net_diagrams/net_diagram_rules.png" width="100%"></div><div class="col-md-9"><h3>Alternative neural network formulations</h3><p class="blurb">
            One current direction of my research involves exploring variants on the classic neural network design that are still able to do complex deep learning tasks. Much evidence in the last few years indicates that the magic of neural networks lies in their hierarchical nonlinear structure, not in the low-level details of their mathematical formulation, so this project's aiming to explore other, new deep learning models to build an understanding of what the requirements are for a model to learn complex patterns. I've written up some of my progress on one variant <a href="/deep learning/2020/08/31/multiplicative-neural-nets">here.</a></p></div></div> -->

  <hr class="hr-thick">
  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="https://james-simon.github.io/img/trajectories.png" width="100%">
    </div>
    <div class="col-md-9">
      <h3>Critical point-finding methods reveal gradient-flat regions of deep network losses</h3>
      <strong>Exposing flaws in widely-used critical-point-finding methods</strong>
      <p class="blurb"> Despite how common and useful neural networks are, there are still basic mysteries about how they work, many related to properties of their loss surfaces. In this project, led by <a href="https://charlesfrye.github.io/">Charles Frye</a>, we tested Newton methods (common tools for optimization and exploring function structure) on loss surfaces. We found that, as opposed to finding critical points as designed, in practice Newton methods almost always converged to a different, spurious class of points which we described. Giving simple visualizable examples to illustrate the problem, we showed that some major studies using Newton methods on loss surfaces probably misinterpreted their results. Our paper is <a href="https://arxiv.org/pdf/2003.10397.pdf">here</a>. </p>
      <p class="paper-info">
        <strong>(2021)</strong>
        <a href="https://direct.mit.edu/neco/article/33/6/1469/100574/Critical-Point-Finding-Methods-Reveal-Gradient">[Neural Computation]</a>
        <a href="https://https://arxiv.org/abs/2003.10397">[arXiv]</a>
        <a href="https://github.com/charlesfrye/autocrit">[code]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">
  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="https://james-simon.github.io/img/jj.png" width="100%">
    </div>
    <div class="col-md-9">
      <h3>Simplified Josephson-junction fabrication process for reproducibly high-performance superconducting qubits</h3>
      <strong>A faster method to make Josephson junctions</strong>
      <p class="blurb"> In the spring and summer of 2019 I worked in the lab of Prof. <a href="http://www.chalmers.se/en/staff/Pages/per-delsing.aspx">Per Delsing</a> developing nanofabrication methods for Josephson junctions, ubiquitous components in superconducting circuitry. My main project was a study of how junctions age in the months after fabrication, but my biggest contribution was elsewhere: Anita Fadavi, Amr Osman and I developed a junction design that is faster to fabricate by one lithography step, or potentially several days of work. </p>
      <p class="paper-info">
        <strong>(2021)</strong>
        <a href="https://aip.scitation.org/doi/full/10.1063/5.0037093">[Applied Physics Letters]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">
  <div class="row">
    <div class="col-md-3">
      <img class="mr-4" src="https://james-simon.github.io/img/statediagram.png" width="100%">
    </div>
    <div class="col-md-9">
      <h3>Fast noise-resistant control of donor nuclear spin qubits in silicon</h3>
      <strong>Better control schemes for for spin qubits</strong>
      <p class="blurb"> Qubits decohere and lose their quantum information when uncontrollably coupled to their environment. Nuclear spin qubits in silicon are extremely weakly coupled to their environment, giving them long coherence times (up to minutes), but that same weak coupling makes quickly controlling them difficult. Advised by Prof. <a href="http://www1.phys.vt.edu/~economou/index.html">Sophia Economou</a>, I came up with schemes for driving nuclear spin qubits that give fast, noise-resistant arbitrary single-qubit gates. The most important gate is a long sweep that effectively turns uncertainty in electric field (charge noise) into uncertainty in time, which can be accounted for by corrective gates. We also show two-qubit gates. </p>
      <p class="paper-info">
        <strong>(2020)</strong>
        <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.101.205307">[PRB]</a>
        <a href="https://arxiv.org/pdf/2001.10029.pdf">[arXiv]</a>
      </p>
    </div>
  </div>

  <hr class="hr-thick">
  <!--    <div  class="media"><div  class="pull-left media-top" width=250px><br><img class="mr-4" src="https://james-simon.github.io/img/shallow_learning_sketch.png" width=250px></div><div class="media-body"><h3>On the Power of Shallow Learning</h3><strong>Reverse-engineering the neural network-kernel method equivalence</strong><p class="blurb">
            Much of our understanding of artificial neural networks stems from the fact that, in the infinite-width limit, they turn out to be equivalent to a class of simple models called <i>kernel methods.</i> Given a wide network architecture, it's surprisingly easy to find the equivalent kernel method, allowing us to study popular models in the infinite-width limit. In recent work with Sajant Anand, I showed that, for fully-connected nets (FCNs), this mapping can be run in reverse: given a desired kernel, we can work backwards to find a network that achieves it. Surprisingly, we can always design this network to have only a single hidden layer, and we used that fact to prove that wide shallow FCNs can achieve any kernel a deep FCN can, an analytical conclusion our experiments support. This ability to design nets with desired kernels is a step towards deriving good net architectures <i>from first principles</i>, a longtime dream of the field of machine learning.
         </p><p class="paper-info"><strong>(2021)</strong><strong><em>In Submission</em></strong><a href="https://arxiv.org/abs/2106.03186v1">[arXiv]</a><a href="https://github.com/james-simon/shallow-learning">[code]</a></p></div></div> -->
         
  <a name="puzzles"></a>
  <h2>Puzzles</h2>
  <p>While a senior in undergrad, I started a puzzlehunt called the <a href="vthunt.com">VT Hunt</a> with Bennett Witcher. It became a university tradition, with the 2019-22 VT Hunts each drawing 1000-2000 participants and raising money for charities, and I've stayed involved as a mentor. I've also helped concoct six other puzzle events starting in high school. A few of my favorite puzzles I've made are below. They're roughly ordered from easiest to hardest, so you can pick where to start. </p>
  <!--    
   
   
   
   
   
   <a href="/puzzles/90shades/">90 Shades of Black</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/addersmultiplying/">Adders Multiplying</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/coasters/">Coasters</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/couplets/">Name a More Iconic Set of Couplets</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/flags/">Flags</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/forestson/">Forest Son</a>
   
   
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/maelstrom/">Maelstrom</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/mining/">Mining</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/topology/">Topology</a>
   
   
   
   
   
   
   
   
   â€¢ <a href="/puzzles/transit/">Transit</a>
   
   
   
    -->
  <style>
    .center-cropped {
      object-fit: cover;
      /* Do not scale the image */
      object-position: center;
      /* Center the image within the element */
      height: 200px;
      width: 200px;
    }
  </style>
  <div class="row justify-content-md-center">
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
      
        
      
        
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/coasters.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/coasters/">
              <h4>COASTERS</h4>
            </a>
          </div>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/flags.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/flags/">
              <h4>FLAGS</h4>
            </a>
          </div>
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/forestson.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/forestson/">
              <h4>FOREST SON</h4>
            </a>
          </div>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/addersmultiplying.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/addersmultiplying/">
              <h4>ADDERS MULTIPLYING</h4>
            </a>
          </div>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/90shadesofblack.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/90shades/">
              <h4>90 SHADES OF BLACK</h4>
            </a>
          </div>
        
      
        
      
        
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/nameamoreiconicsetofcouplets.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/couplets/">
              <h4>NAME A MORE ICONIC SET OF COUPLETS</h4>
            </a>
          </div>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/topology.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/topology/">
              <h4>TOPOLOGY</h4>
            </a>
          </div>
        
      
        
      
        
      
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/mining.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/mining/">
              <h4>MINING</h4>
            </a>
          </div>
        
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/transit.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/transit/">
              <h4>TRANSIT</h4>
            </a>
          </div>
        
      
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <div class="hovereffect" style="border:2px solid black; margin: 10px 10px 10px 10px">
            <img class="center-cropped" src="https://james-simon.github.io/img/maelstrom.png">
            <div class="tint"></div>
            <a class="hoverinfo" href="/puzzles/maelstrom/">
              <h4>MAELSTROM</h4>
            </a>
          </div>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
     </div>

  <hr class="hr-thick">
  <a name="posts"></a>
  <h2>Blog</h2>
  <hr>
  <div class="row">
    <div class="col-md-6">
      <h4>Science (research)</h4>
      <!-- <ul class="post-list"> -->   <!-- <li> -->
<h5>
<a class="post-link" href="/blog/on-the-scientific-method/">On the scientific method and its application to the science of deep learning</a> <small> (July 2025, 
	
	
		23 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jul 20, 2025--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/backsolving-classical-bounds/">Backsolving classical generalization bounds from the modern kernel regression eigenframework</a> <small> (April 2025, 
	
	
		4 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Apr 9, 2025--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/one-kernel-many-eigensystems/">One kernel, many eigensystems</a> <small> (April 2025, 
	
	
		6 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Apr 7, 2025--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/the-expressivity-of-shallow-relu-nets/">A complete characterization of the expressivity of shallow, bias-free ReLU networks</a> <small> (April 2025, 
	
	
		9 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Apr 3, 2025--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/gaussian-kernel-1d-eigendecomp/">The eigensystem of the Gaussian kernel w.r.t. a Gaussian measure</a> <small> (March 2025, 
	
	
		7 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Mar 18, 2025--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/low-rank-linear-regression-sol/">The optimal low-rank solution for linear regression</a> <small> (November 2024, 
	
	
		1 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Nov 6, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/autoencoders-are-cursed/">Infinite-width autoencoders are cursed</a> <small> (October 2024, 
	
	
		12 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Oct 8, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/transforming-low-rank-matrices/">It's hard to turn a low-rank matrix into a high-rank matrix</a> <small> (October 2024, 
	
	
		8 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Oct 4, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/gpt2-positional-encs/">Insights into GPT-2's positional encodings</a> <small> (August 2024, 
	
	
		11 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Aug 20, 2024--> 
<!-- </li> -->        <!-- <li> -->
<h5>
<a class="post-link" href="/blog/self-assembly/">Experiments in self-assembly</a> <small> (July 2024, 
	
	
		7 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jul 3, 2024--> 
<!-- </li> -->          <!-- <li> -->
<h5>
<a class="post-link" href="/blog/the-laplacian-and-diffusion/">Using the Laplacian to take a local average of a function</a> <small> (June 2024, 
	
	
		5 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 6, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/1nn-eigenframework/">An eigenframework for the generalization of 1NN</a> <small> (June 2024, 
	
	
		6 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 5, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/lets-solve-learning-rules/">Let's solve more learning rules</a> <small> (June 2024, 
	
	
		6 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 5, 2024--> 
<!-- </li> -->                <!-- <li> -->
<h5>
<a class="post-link" href="/blog/llm-watermarking/">Creating and erasing AI watermarks</a> <small> (March 2024, 
	
	
		9 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Mar 6, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/neuro-learnings/">Reflections on introductory neuroscience reading</a> <small> (March 2024, 
	
	
		22 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Mar 2, 2024--> 
<!-- </li> -->          <!-- <li> -->
<h5>
<a class="post-link" href="/blog/reverse-engineering/">Reverse engineering the NTK</a> <small> (August 2022, 
	
	
		8 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Aug 23, 2022--> 
<!-- </li> -->        <!-- <li> -->
<h5>
<a class="post-link" href="/blog/eigenlearning/">A theory of generalization for wide neural nets</a> <small> (October 2021, 
	
	
		7 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Oct 26, 2021--> 
<!-- </li> -->        <!-- <li> -->
<h5>
<a class="post-link" href="/blog/least_power_dissipation/">The principle of least power dissipation</a> <small> (September 2020, 
	
	
		8 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Sep 6, 2020--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/multiplicative-neural-nets/">Multiplicative neural networks</a> <small> (August 2020, 
	
	
		16 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Aug 31, 2020--> 
<!-- </li> -->              
      <!-- </ul> -->
      <hr>
      <h4>Potpourri</h4>
      <!-- <ul class="post-list"> -->                       <!-- <li> -->
<h5>
<a class="post-link" href="/blog/egg/">The time I caught an egg in my mouth</a> <small> (July 2024, 
	
	
		1 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jul 5, 2024--> 
<!-- </li> -->      <!-- <li> -->
<h5>
<a class="post-link" href="/blog/gravitree-update-2024/">Gravitree update: June 2024</a> <small> (June 2024, 
	
	
		10 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 23, 2024--> 
<!-- </li> -->      <!-- <li> -->
<h5>
<a class="post-link" href="/blog/boolpool-timelapse/">The sixth lake</a> <small> (June 2024, 
	
	
		2 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 10, 2024--> 
<!-- </li> -->            <!-- <li> -->
<h5>
<a class="post-link" href="/blog/cropland-crystallography/">Geometric patterns in croplands</a> <small> (April 2024, 
	
	
		11 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Apr 21, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/quotes-from-l-t-a-y-p/">Favorite quotes from <em>Letters to a Young Poet</em></a> <small> (April 2024, 
	
	
		4 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Apr 17, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/eclipse-baby/">How many babies were born during the 2024 eclipse?</a> <small> (April 2024, 
	
	
		5 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Apr 15, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/fifty-states-roadtrip/">Roadtripping to North Dakota</a> <small> (April 2024, 
	
	
		9 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Apr 14, 2024--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/why-i-talk-to-strangers/">Why I talk to strangers</a> <small> (March 2024, 
	
	
		2 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Mar 15, 2024--> 
<!-- </li> -->                <!-- <li> -->
<h5>
<a class="post-link" href="/blog/bohr-v-einstein/">Einstein vs. Bohr rap battle</a> <small> (January 2022, 
	(5 min watch)
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jan 17, 2022--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/newton-v-leibniz/">Newton vs. Leibniz rap battle</a> <small> (January 2022, 
	(5 min watch)
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jan 17, 2022--> 
<!-- </li> -->                <!-- <li> -->
<h5>
<a class="post-link" href="/blog/mail-shenanigans/">Messing with the postal service</a> <small> (July 2020, 
	
	
		4 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jul 12, 2020--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/common-ground/">Common ground</a> <small> (July 2020, 
	
	
		5 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jul 4, 2020--> 
<!-- </li> -->        <!-- <li> -->
<h5>
<a class="post-link" href="/blog/covid-calculation/">The expected cost of breaking quarantine</a> <small> (May 2020, 
	
	
		12 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- May 17, 2020--> 
<!-- </li> -->  
      <!-- </ul> -->
    </div>
    <div class="col-md-6">
      <h4>Science (fun)</h4>
      <!-- <ul class="post-list"> -->                     <!-- <li> -->
<h5>
<a class="post-link" href="/blog/fractals-in-iterated-maps/">Understanding fractals from iterated maps</a> <small> (July 2024, 
	
	
		11 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jul 31, 2024--> 
<!-- </li> -->          <!-- <li> -->
<h5>
<a class="post-link" href="/blog/household-microscopy/">Household microscopy</a> <small> (June 2024, 
	
	
		2 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 11, 2024--> 
<!-- </li> -->                            <!-- <li> -->
<h5>
<a class="post-link" href="/blog/volumetric-reaction/">Can a chemical reaction measure the size of its container?</a> <small> (October 2022, 
	
	
		15 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Oct 10, 2022--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/cell-fight/">Simulating cells fighting to the death</a> <small> (September 2022, 
	
	
		4 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Sep 24, 2022--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/time-reversed-random-walks/">Time-reversed random walks</a> <small> (September 2022, 
	
	
		16 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Sep 3, 2022--> 
<!-- </li> -->            <!-- <li> -->
<h5>
<a class="post-link" href="/blog/gravitrees/">The gravitree</a> <small> (October 2021, 
	
	
		6 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Oct 11, 2021--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/rocketball/">Could you propel a spacecraft using sports projectiles?</a> <small> (November 2020, 
	
	
		10 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Nov 29, 2020--> 
<!-- </li> -->        <!-- <li> -->
<h5>
<a class="post-link" href="/blog/upside-down-candle/">How would an upside-down candle burn?</a> <small> (August 2020, 
	
	
		4 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Aug 20, 2020--> 
<!-- </li> -->        <!-- <li> -->
<h5>
<a class="post-link" href="/blog/fish-planet/">What would happen if you made a planet out of fish?</a> <small> (June 2020, 
	
	
		11 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 17, 2020--> 
<!-- </li> -->    <!-- <li> -->
<h5>
<a class="post-link" href="/blog/chicken-cooking/">How hard do you have to hit a chicken to cook it?</a> <small> (June 2020, 
	
	
		1 min read
	
) </small>
</h5>

<!-- <span class="post-meta"></span> -->
<!-- Jun 17, 2020--> 
<!-- </li> -->    
      <!-- </ul> -->
    </div>
  </div>
</div>

      </div>
    </div>

    <div class="text-center p-3" style="background-color: #edf3f5;">
  <div class="container ">
    <div class="row justify-content-md-center">

      <div class="col-2">
        <p class="text-center">
          <i class="far fa-envelope"></i>
          <a href="mailto:jsi@berkeley.edu">jsi@berkeley.edu</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          <i class="fab fa-github"></i>
          <a href="https://github.com/james-simon">james-simon</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          <i class="fas fa-graduation-cap"></i>
          <a href=https://scholar.google.com/citations?user=zjGfh3sAAAAJ&hl=en>gScholar</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          <i class="fab fa-instagram"></i>
          <a href="https://instagram.com/sam.simon17">sam.simon17</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          SSN: 314-15-9265
        </p>
      </div>

    </div>
  </div>
</div>
</footer>

  </body>

</html>
