<!DOCTYPE html>
<html>

  <head>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Reverse engineering the NTK: towards first-principles architecture design</title>
  <meta name="description" content=" Foundational works showed how to find the kernel corresponding to a wide network. We find the inverse mapping, showing how to find the wide network correspo...">

  <!-- <link href="/bootstrap/css/bootstrap.css" rel="stylesheet">
  <script src="/boostrap/js/bootstrap.js"></script> -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/deep%20learning,%20research/2022/08/23/reverse-engineering.html">
  <link rel="alternate" type="application/rss+xml" title="JS" href="http://localhost:4000/feed.xml">

  

</head>


  <body>

    <!-- <header class="site-header"> -->
    <!-- <a class="site-title" href="/">JS</a> -->
<!-- </header> -->

    <nav class="navbar navbar-expand-lg navbar-light" style="background-color: #edf3f5;">
      <div class = "container">
        <a class="navbar-brand" href="/">JS</a></span> </a>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav ml-auto">
            <li class = "nav-item active"><a class="nav-link" href="/#research"><i class="fas fa-cogs"></i> Research</a></li>
            <li class = "nav-item active"><a class="nav-link" href="/#puzzles"><i class="fab fa-laravel"></i> Puzzles</a></li>
            <li class = "nav-item active"><a class="nav-link" href="/#posts"><i class="fas fa-seedling"></i> Posts</a></li>
          </ul> 
        </div>
      </div>
    </nav>

    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <div class="container">
    <br>

    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">Reverse engineering the NTK: towards first-principles architecture design</h1>
      <p class="post-meta"><time datetime="2022-08-23T00:00:00-07:00" itemprop="datePublished">Aug 23, 2022</time></p>
    </header>

    <div class="post-content" itemprop="articleBody">
      <p style="text-align:center;">
<img src="https://james-simon.github.io/img/rev_eng_fig1.png" width="40%" />
</p>
<p style="margin-left:20%; margin-right:20%;">
<small>
<i> Foundational works showed how to find the kernel corresponding to a wide network. We find the inverse mapping, showing how to find the wide network corresponding to a given kernel. </i>
</small>
</p>

<p>Deep neural networks have enabled technological wonders ranging from voice recognition to machine transition to protein engineering, but their design and application is nonetheless notoriously unprincipled.
Consider, for perspective, how other fields of engineering operate: we start with a description of a problem, procedurally design a structure or system that solves it, and build it.
We normally find that our system behaves close to how we predicted, and if it doesn’t, we can understand its failings.
Deep learning, by contrast, is basically <a href="https://www.youtube.com/watch?v=x7psGHgatGM">alchemy</a>: despite much research, practitioners still have almost no principled methods for neural architecture design, and SOTA systems are often full of hacks and hyperparameters we might not need if we understood what we were doing.
As a result, the development of new methods is often slow and expensive, and even when we find clever new ideas, we often don’t understand why they work as well as they do.</p>

<!--more-->

<p>In <a href="https://arxiv.org/abs/2106.03186">Reverse Engineering the Neural Tangent Kernel</a>, Sajant Anand, Mike DeWeese, and I propose a paradigm that we find promising for bringing some principle to the art of architecture design.
The field of deep learning theory has recently been transformed by the realization that deep neural networks often become analytically tractable to study in the <em>infinite-width</em> limit.
Take the limit a certain way, and the network in fact converges to ordinary <em>kernel regression</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> using either the architecture’s <a href="https://arxiv.org/abs/1806.07572">“neural tangent kernel” (NTK)</a> or, if only the last layer is trained (a la random feature models), its <a href="https://arxiv.org/abs/1711.00165">“neural network Gaussian process” (NNGP) kernel</a>.
Like the central limit theorem, the NTK limit is often a surprisingly good approximation even far from infinite width (often holding true at widths in the hundreds or thousands), giving a remarkable analytical handle on the mysteries of deep learning.</p>

<p>The original works exploring this net-kernel correspondence gave formulae for going from <em>architecture</em> to <em>kernel</em>: given a description of an architecture (e.g. depth and activation function), they give you the network’s two kernels.
This has allowed great insights into the optimization and generalization of various architectures of interest.
However, if our goal is not merely to understand existing architectures but to design <em>new</em> ones, then we might rather have the mapping in the reverse direction: given a <em>kernel</em> we want, can we find an <em>architecture</em> that gives it to us?
In this work, we derive this inverse mapping for fully-connected networks (FCNs), allowing us to design simple nets in a principled manner by (a) positing a desired kernel and (b) designing an activation function that gives it.</p>

<h2 id="net-kernels-and-their-reverse-engineering"><strong>Net kernels and their reverse engineering</strong></h2>

<p>To get a feel for how this works, let’s first visualize some neural network kernels.
Consider a wide FCN’s NTK \(K(x_1,x_2)\) on two input vectors \(x_1\) and \(x_2\) (which we will for simplicity assume are normalized to the same length).
For a FCN, this kernel is <em>rotation-invariant</em> in the sense that \(K(x_1,x_2) = K(c)\), where \(c\) is the cosine of the angle between the inputs.
Since \(K(c)\) is a scalar function of a scalar argument, we can simply plot it.
Fig. 1 shows the NTK of a four-hidden-layer (4HL) \(\textrm{ReLU}\) FCN.</p>

<p style="text-align:center;">
<img src="https://james-simon.github.io/img/rev_eng_fig2.png" width="50%" />
</p>
<p style="margin-left:20%; margin-right:20%;">
<small>
<i> <b>Fig 1.</b> The NTK of a 4HL ReLU FCN as a function of the cosine between two input vectors x1 and x2. </i>
</small>
</p>

<p>This plot actually contains much information about the learning behavior of the corresponding wide network!
The monotonic increase means that this kernel expects closer points to have more correlated function values.
The steep increase at the end tells us that the correlation length is not too large, and it can fit complicated functions.
The diverging derivative at \(c=1\) tells us about the smoothness of the function we expect to get.
The key point is that <em>none of these facts are apparent from looking at a plot of \(\textrm{ReLU}(z)\).</em>
If we want to understand the effect of a choice of activation function \(\phi\), then its resulting NTK is actually more informative than \(\phi\) itself!
It thus perhaps makes sense to try to design architectures in “kernel space,” then translate them to the typical hyperparameters.</p>

<p>Our paper’s main result is a “reverse engineering theorem” that states the following:</p>

<p><strong>Thm 1:</strong> <em>For any kernel \(K(c)\), we can construct an activation function \(\phi\) such that, when inserted into a <strong>single-hidden-layer</strong> FCN, it yields \(K(c)\) as its NTK.</em></p>

<p>We give an explicit formula for \(\phi\) in terms of Hermite polynomials
(in practice, however, Hermite polynomial nets aren’t easy to train, so we use a different functional form with the help of some numerics).
Our proposed use of this idea is that, in problems with some known structure, it’ll sometimes be possible to write down a good kernel and reverse-engineer it into a trainable network with various advantages over kernel regression, like computational efficiency and the ability to learn features.
As a proof of concept, we test this idea out on the synthetic <em>parity problem</em> (i.e., given a bitstring, is the sum odd or even?), immediately coming up with an activation function that dramatically outperforms \(\text{ReLU}\) on the task.</p>

<h2 id="one-hidden-layer-is-all-you-need"><strong>One hidden layer is all you need?</strong></h2>

<p>Here’s another surprising use of our result.
The kernel curve above is for a 4HL \(\textrm{ReLU}\) FCN, but I claimed that we can achieve any kernel, including that one, with just one hidden layer.
This implies we can come up with some new activation function \(\tilde{\phi}\) that gives this “deep” NTK in a <em>shallow network</em>!</p>

<p style="text-align:center;">
<img src="https://james-simon.github.io/img/rev_eng_fig3.png" width="60%" />
</p>
<p style="margin-left:20%; margin-right:20%;">
<small>
<i> <b>Fig 2.</b> Shallowification of a deep ReLU FCN into a 1HL FCN with an engineered activation function. </i>
</small>
</p>

<p>Surprisingly, this “shallowfication” actually works.
The left plot below shows a “mimic” activation function \(\tilde{\phi}\) that gives virtually the same NTK as a deep \(\textrm{ReLU}\) FCN.
The right plots then show train + test loss + accuracy traces for three FCNs on a standard tabular problem from the UCI dataset.
Note that, while the shallow and deep ReLU nets have very different behaviors, our engineered shallow mimic net tracks the deep network almost exactly!</p>

<p style="text-align:center;">
<img src="https://james-simon.github.io/img/rev_eng_fig4.png" width="70%" />
</p>
<p style="margin-left:20%; margin-right:20%;">
<small>
<i> <b>Fig 3.</b> Left panel: our engineered "mimic" activation function, plotted with ReLU for comparison. Right panels: performance traces for 1HL ReLU, 4HL ReLU, and 1HL mimic FCNs trained on a UCI dataset. Note the close match between the 4HL ReLU net and the 1HL mimic net. </i>
</small>
</p>

<p>This is interesting from an engineering perspective because the shallow net uses fewer parameters than the deep network to achieve the same performance.
It’s also interesting from a theoretical perspective because it raises fundamental questions about the value of depth.
A common belief deep learning belief is that deeper is not only better but <em>qualitatively different</em>: that deep networks will efficiently learn functions that shallow networks simply cannot.
Our shallowification result suggests that, at least for FCNs, this isn’t true: if we know what we’re doing, then depth doesn’t buy us anything<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>.</p>

<h2 id="conclusion"><strong>Conclusion</strong></h2>

<p>This work comes with lots of caveats.
The biggest is that our result only applies to FCNs, which alone are rarely state-of-the-art.
However, work on convolutional NTKs is <a href="https://arxiv.org/abs/2112.05611">fast progressing</a>, and we believe this paradigm of designing networks by designing kernels is ripe for extension in some form to these structured architectures.</p>

<p>Theoretical work has so far furnished relatively few tools for practical deep learning theorists.
We aim for this to be a modest step in that direction.
Even without a science to guide their design, neural networks have already enabled wonders, so just imagine what we’ll be able to do with them once we finally have one.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In case you’re unfamiliar with kernels or kernel regression, a kernel is basically a similarity function between two samples generalizing the dot product, and kernel regression is just linear regression with the dot product replaced by the kernel function. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>(It’s the belief of this author that deeper really is different for CNNs, and so studies aiming to understand the benefits of depth for generalization should focus on CNNs and other structured architectures.) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>

  </div>

</article>

      </div>
    </div>

    <div class="text-center p-3" style="background-color: #edf3f5;">
  <div class="container ">
    <div class="row justify-content-md-center">

      <div class="col-2">
        <p class="text-center">
          <i class="far fa-envelope"></i>
          <a href="mailto:jsi@berkeley.edu">jsi@berkeley.edu</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          <i class="fab fa-github"></i>
          <a href="https://github.com/james-simon">james-simon</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          <i class="fas fa-graduation-cap"></i>
          <a href=https://scholar.google.com/citations?user=zjGfh3sAAAAJ&hl=en>gScholar</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          <i class="fab fa-instagram"></i>
          <a href="https://instagram.com/sam.simon17">sam.simon17</a>
        </p>
      </div>

      <div class="col-2">
        <p class="text-center">
          SSN: 574-05-7179
        </p>
      </div>

    </div>
  </div>
</div>
</footer>

  </body>

</html>
