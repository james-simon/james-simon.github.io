<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JS</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 23 Aug 2022 12:46:09 -0700</pubDate>
    <lastBuildDate>Tue, 23 Aug 2022 12:46:09 -0700</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Einstein vs. Bohr rap battle</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;They say an excellent scientist ought to know not just their field’s science but also its history. In a cynical move intended purely to improve our job prospects, Ashwin Singh and I decided to flaunt our copious knowledge of the history of physics by writing and performing an embarrassingly nerdy rap battle between Niels Bohr and Albert Einstein at our 2021 departmental holiday party and posting it on the internet. Potential employers can find the recording below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;600&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/k7Nj7IUHveY&quot;&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2 align=&quot;center&quot;&gt;
LYRICS
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;[BOHR]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Do you really want to reignite this debate? \ History’s already set the record straight
&lt;/p&gt;
&lt;p&gt;Why you chose to battle me I can only wonder \ It’ll go down as your greatest blunder&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You started anti-quantum but then slowly changed your ways \ I guess that your denial was just a Berry phase
&lt;/p&gt;
&lt;p&gt;Your life was full of sexual activity but \ you married your cousin? That’s “special relativity”!&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
All you know is maths \ and if you built an Einstein Rosen bridge, I bet it would collapse.
&lt;/p&gt;
&lt;p&gt;And your famous shorthand is pure castration \ Cutting off sums? That’s Einstein &lt;em&gt;dumb&lt;/em&gt; notation&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
To prove quantum, look at Einstein at his prime \ one man with two women at the same damn time!
&lt;/p&gt;
&lt;p&gt;But it only works theoretically \ A million extra qubits won’t correct that infidelity&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You’ve many talents, especially misogyny \ People tend to like you, except for your progeny
&lt;/p&gt;
&lt;p&gt;I’m a better father and it isn’t hard to tell \ Your son you didn’t know well; my son won a Nobel!&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m creative, made s p d and f \ and I’ve got *a dagger* but I’m boutta act left
&lt;/p&gt;
&lt;p&gt;Give up Einstein - you’re not a match for me \ or I’ll annihilate you in an ultraviolent catastrophe&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[EINSTEIN]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Einstein’s my name, but “mc” will suffice \ watch out - this rap god does not play nice
&lt;/p&gt;
&lt;p&gt;You’re one great Dane, I have to admit \ but now I’m taking the stage, so roll over and sit&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m General Relativity, the field’s commanding officer \ You? Eh, you’re more of a philosopher
&lt;/p&gt;
&lt;p&gt;I’m king of the cosmos; I fused space and time \ now watch me unify rhythm and rhyme&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
No modern man can match the breadth of things that I wrote on \ I'd still be above your level if I’d omitted the photon
&lt;/p&gt;
&lt;p&gt;I’m a hero of theory, stand a light-year tall \ Even in your rest frame, you’re relatively small&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I solved mysteries from gravity to light \ You couldn’t even get hydrogen right
&lt;/p&gt;
&lt;p&gt;Here’s a new Bohr model (and this one won’t flop): \ physics has levels, and I’m on top.&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m a star on both sides of the ocean \ Every time I speak I cause a Brownian commotion
&lt;/p&gt;
&lt;p&gt;Your lectures are dull, though I’d not known before \ that one could stretch time just by being a Bohr&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
In this deterministic world one can predict what comes to pass \ and I can prove my geodesic leads my foot into your [CENSORED]
&lt;/p&gt;
&lt;p&gt;Oh, and as for women, why don’t you just ask your wife \ if Einsteinium or Bohrium’s got the longer half-life?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[BOHR]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Those lines were so bad I’m nauseous \ Tell me did you learn them at the patent office?
&lt;/p&gt;
&lt;p&gt;I’m Bohr Dissing Einstein, got BDE \ You’re more degenerate than a BEC&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
In history you’re an eccentric defector \ who's easily perturbed like the Runge Lenz vector
&lt;/p&gt;
&lt;p&gt;Your math went missing on many occasions: \ you couldn’t even solve your own field equations&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
So much wrong about how things behave \ like “uh there’s no energy in gravitational waves”
&lt;/p&gt;
&lt;p&gt;Oh, and what do people think of EPR? \ Yeah, that theory’s gonna need some CPR&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You also never understood Schrödinger’s cat \ Imagine tonight at the start of the act
&lt;/p&gt;
&lt;p&gt;they see you and me both behind and ahead \ but after my verse they realize that you’re dead&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Oh my conjugate man \ you must be feeling uncertain cause I know where I stand
&lt;/p&gt;
&lt;p&gt;All eyes are on me, but you’re in superposition \ ‘cause to not observe you is a superb omission&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m done, and there’s one clear conclusion \ I'm the only winner here by Pauli’s exclusion
&lt;/p&gt;
&lt;p&gt;Oh, and for Einsteinium, let’s use a bit of rigor: \ every chemist knows that Bohrium is bigger&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[EINSTEIN]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
When I walk around Princeton, the ladies all stare \ even black holes tell me “ah, I wish I had your hair!”
&lt;/p&gt;
&lt;p&gt;In attractiveness, I am the crest, you’re the trough \ there’s a name for the phenomenon: “the Bohr magnet-off”&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Forget singularities, folks, we’ve found something denser \ scare me all you want, you can’t make Einstein tensor
&lt;/p&gt;
&lt;p&gt;I built my own fame, you relied on your betters; \ guess that explains why you published our letters&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I'm unchallenged master of problems abstract \ I think so fast your thoughts Lorentz-contract
&lt;/p&gt;
&lt;p&gt;Yeah, you might as well call me “c” \ ‘cause you’ve got no chance of catching up with me&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Socially, I’m a champion of simple coexistence \ resisting and assisting, consistently insisting
&lt;/p&gt;
&lt;p&gt;politically, your presence more or less is nonexistent \ you can’t spookily better the world from a distance.&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Here’s a little lesson in relativity \ there’s a twin paradox here between you and me:
&lt;/p&gt;
&lt;p&gt;see, since I ran circles ‘round you all life long \ your work’s long dead, but mine’s still going strong&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m generally special; my work’s especially general \ My greatness endures, Bohr, but yours was ephemeral
&lt;/p&gt;
&lt;p&gt;Your nanoscopic jabs at me are hardly worth refutin’ \ When all’s said and done, bud, you’re Leibniz, I’m Newton.&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Jan 2022 01:01:00 -0800</pubDate>
        <link>http://localhost:4000/physics,%20random/2022/01/17/bohr-v-einstein.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics,%20random/2022/01/17/bohr-v-einstein.html</guid>
        
        
        <category>physics, random</category>
        
      </item>
    
      <item>
        <title>Newton vs. Leibniz rap battle</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;The spiteful grudge between these calculus co-creators is among the best-known disputes in the history of science. In 2019, fellow first-year grad Ashwin Singh and I wrote a rap battle between these men and performed it at the Berkeley physics department’s annual holiday party. A post-facto recording is below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;600&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/COeKdP3EkXU&quot;&gt;
&lt;/iframe&gt;
&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Jan 2022 01:00:00 -0800</pubDate>
        <link>http://localhost:4000/physics,%20random,%20rap-battle/2022/01/17/newton-v-leibniz.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics,%20random,%20rap-battle/2022/01/17/newton-v-leibniz.html</guid>
        
        
        <category>physics, random, rap-battle</category>
        
      </item>
    
      <item>
        <title>A theory of generalization for wide neural nets</title>
        <description>&lt;p&gt;&lt;em&gt;This post also appeared on the &lt;a href=&quot;https://bair.berkeley.edu/blog/2021/10/25/eigenlearning/&quot;&gt;BAIR blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;60%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/eigenlearning_exp_matches_th.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;&lt;b&gt;Fig 1.&lt;/b&gt; Measures of generalization performance for neural networks trained on four different boolean functions (colors) with varying training set size. For both MSE (left) and learnability (right), theoretical predictions (curves) closely match true performance (dots). &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;Deep learning has proven a stunning success for countless problems of interest, but this success belies the fact that, at a fundamental level, we do not understand why it works so well. Many empirical phenomena, well-known to deep learning practitioners, remain mysteries to theoreticians. Perhaps the greatest of these mysteries has been the question of generalization: &lt;em&gt;why do the functions learned by neural networks generalize so well to unseen data?&lt;/em&gt; From the perspective of classical ML, neural nets’ high performance is a surprise given that they are so overparameterized that they could easily represent countless poorly-generalizing functions.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Questions beginning in “why” are difficult to get a grip on, so we instead take up the following quantitative problem: &lt;em&gt;given a network architecture, a target function \(f\), and a training set of \(n\) random examples, can we efficiently predict the generalization performance of the network’s learned function \(\hat{f}\)?&lt;/em&gt; A theory doing this would not only explain why neural networks generalize well on certain functions but would also tell us which function classes a given architecture is well-suited for and potentially even let us choose the best architecture for a given problem from first principles, as well as serving as a general framework for addressing a slew of other deep learning mysteries.&lt;/p&gt;

&lt;p&gt;It turns out this is possible: in our recent &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;paper&lt;/a&gt;, &lt;em&gt;we derive a first-principles theory that allows one to make accurate predictions of neural network generalization&lt;/em&gt; (at least in certain settings). To do so, we make a chain of approximations, first approximating a real network as an idealized infinite-width network, which is known to be equivalent to kernel regression, then deriving new approximate results for the generalization of kernel regression to yield a few simple equations that, despite these approximations, closely predict the generalization performance of the original network.&lt;/p&gt;

&lt;h2 id=&quot;finite-network-approx-infinite-width-network--kernel-regression&quot;&gt;&lt;strong&gt;Finite network \(\approx\) infinite-width network \(=\) kernel regression&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;A major vein of deep learning theory in the last few years has studied neural networks of infinite width. One might guess that adding more parameters to a network would only make it harder to understand, but, by results akin to central limit theorems for neural nets, infinite-width nets actually take very simple analytical forms. In particular, a wide network trained by gradient descent to zero MSE loss will always learn the function&lt;/p&gt;

\[\hat{f}(x) = K(x, \mathcal{D}) K(\mathcal{D}, \mathcal{D})^{-1} f(\mathcal{D}),\]

&lt;p&gt;where \(\mathcal{D}\) is the dataset, \(f\) and \(\hat{f}\) are the target and learned functions respectively, and \(K\) is the network’s &lt;a href=&quot;https://arxiv.org/abs/1806.07572&quot;&gt;“neural tangent kernel” (NTK)&lt;/a&gt;. This is a matrix equation: \(K(x, \mathcal{D})\) is a row vector, \(K(\mathcal{D}, \mathcal{D})\) is the “kernel matrix,” and \(f(\mathcal{D})\) is a column vector. The NTK is different for every architecture class but (at least for wide nets) the same every time you initialize. Because of this equation’s similarity to the normal equation of linear regression, it goes by the name of “kernel regression.”&lt;/p&gt;

&lt;p&gt;The sheer simplicity of this equation might make one suspect that an infinite-width net is an absurd idealization with little resemblance to useful networks, but experiments show that, as with the regular central limit theorem, infinite-width results usually kick in sooner than you’d expect, at widths in only the hundreds. Trusting that this first approximation will bear weight, our challenge now is to understand kernel regression.&lt;/p&gt;

&lt;h2 id=&quot;approximating-the-generalization-of-kernel-regression&quot;&gt;&lt;strong&gt;Approximating the generalization of kernel regression&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In deriving the generalization of kernel regression, we get a lot of mileage from a simple trick: we look at the learning problem in the eigenbasis of the kernel. Viewed as a linear operator, the kernel has eigenvalue/vector pairs $(\lambda_i, \phi_i)$ defined by the condition that&lt;/p&gt;

\[\int\limits_{\text{input space}} \! \! \! \! K(x, x’) \phi_i(x’) d x’ = \lambda_i \phi_i(x).\]

&lt;p&gt;Intuitively speaking, a kernel is a similarity function, and we can interpet its high-eigenvalue eigenfunctions as mapping “similar” points to similar values.&lt;/p&gt;

&lt;p&gt;The centerpiece of our analysis is a measure of generalization we call “learnability” which quantifies the alignment of \(f\) and \(\hat{f}\). With a few minor approximations, we derive the extremely simple result that the learnability of each eigenfunction is given by&lt;/p&gt;

\[\mathcal{L}(\phi_i) = \frac{\lambda_i}{\lambda_i + C},\]

&lt;p&gt;where \(C\) is a constant. Higher learnability is better, and thus this formula tells us that &lt;em&gt;higher-eigenvalue eigenfunctions are easier to learn!&lt;/em&gt; Moreover, we show that, as examples are added to the training set, \(C\) gradually decreases from \(\infty\) to \(0\), which means that each mode’s \(\mathcal{L}(\phi_i)\) gradually increases from \(0\) to \(1\), with higher eigenmodes learned first. Models of this form have a strong inductive bias towards learning higher eigenmodes.&lt;/p&gt;

&lt;p&gt;We ultimately derive expressions for not just learnability but for &lt;em&gt;all first- and second-order statistics of the learned function,&lt;/em&gt; including recovering previous expressions for MSE. We find that these expressions are quite accurate for not just kernel regression but finite networks, too, as illustrated in Fig 1.&lt;/p&gt;

&lt;h2 id=&quot;no-free-lunch-for-neural-networks&quot;&gt;&lt;strong&gt;No free lunch for neural networks&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In addition to approximations for generalization performance, we also prove a simple exact result we call the “no-free-lunch theorem for kernel regression.” The classical &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;no-free-lunch theorem for learning algorithms&lt;/a&gt; roughly states that, averaged over all possible target functions \(f\), any supervised learning algorithm has the same expected generalization performance. This makes intuitive sense - after all, most functions look like white noise, with no discernable patterns - but it is also not very useful since the set of “all functions” is usually enormous. Our extension, specific to kernel regression, essentially states that&lt;/p&gt;

\[\begin{align}
	\sum_i \mathcal{L}(\phi_i) = \text{[training set size]}.
\end{align}\]

&lt;p&gt;That is, the sum of learnabilities across all kernel eigenfunctions equals the training set size. This exact result paints a vivid picture of a kernel’s inductive bias: the kernel has exactly $\text{[training set size]}$ units of learnability to parcel out to its eigenmodes - no more, no less - and thus eigenmodes are locked in a zero-sum competition to be learned. As shown in Fig 2, we find that this basic conservation law holds exactly for NTK regression and even approximately for finite networks. To our knowledge, this is the first result quantifying such a tradeoff in kernel regression or deep learning. It also applies to linear regression, a special case of kernel regression.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/eigenlearning_conservation_of_lrn.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 2.&lt;/b&gt; For four different network architectures (fully-connected &lt;/i&gt;ReLU&lt;i&gt; and &lt;/i&gt;tanh&lt;i&gt; nets with one or four hidden layers), total learnability summed across all eigenfunctions is equal to the size of the training set. Colored components show learnabilities of individual eigenfunctions. For kernel regression with the network’s NTK (left bar in each pair), the sum is exactly the trainset size, while real trained networks (right bar in each pair) sum to approximately the trainset size. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;These results show that, despite neural nets’ notorious inscrutability, we can nonetheless hope to understand when and why they work well. As in other fields of science, if we take a step back, we can find simple rules governing what naively appear to be systems of incomprehensible complexity. More work certainly remains to be done before we truly understand deep learning - our theory only applies to MSE loss, and the NTK’s eigensystem is yet unknown in all but the simplest cases - but our results so far suggest we have the makings of a bona fide theory of neural network generalization on our hands.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;This post is based on &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;the paper&lt;/a&gt; “Neural Tangent Kernel Eigenvalues Accurately Predict Generalization,” which is joint work with labmate Maddie Dickens and advisor Mike DeWeese. We provide &lt;a href=&quot;https://github.com/james-simon/eigenlearning&quot;&gt;code&lt;/a&gt; to reproduce all our results. We’d be delighted to field your questions or comments.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Oct 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/deep%20learning,%20research/2021/10/26/eigenlearning.html</link>
        <guid isPermaLink="true">http://localhost:4000/deep%20learning,%20research/2021/10/26/eigenlearning.html</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>The gravitree</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;I am now in the 3rd year of my physics PhD, a path motivated, more or less, by a strong desire to figure out the basic rules of the universe. There are many reasons we should try to understand nature: in addition to simple curiosity, it’s my sincere hope that the small light I shine into the unknown might help discover things that one day make the world a better place. That, surely, will be the noblest use of the scientific knowledge to which I find myself a fortunate heir. In the meantime, though, I really like to use it to make weird stuff.&lt;/p&gt;

&lt;p&gt;I’m writing this to introduce and explain the &lt;strong&gt;gravitree&lt;/strong&gt;, a type of balancing kinetic sculpture that I’ve been 3D-printing for years. It requires only high school physics to understand (and that was all I knew when I came up with it), but its hypnotic and counterintuitive motion is unlike anything else I’ve seen. It consists of a number of freely-moving pieces, arranged one atop another in a stack, with just the bottommost piece supported. Here’s what it looks like.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_main.jpg&quot; width=&quot;30%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_main.gif&quot; width=&quot;22%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The stack is easy to assemble and absurdly stable; when people first hold it, they’re usually surprised by how easily it stays upright and the way it stays balanced &lt;a href=&quot;https://youtu.be/3jd2jpJoXgA&quot;&gt;even during drastic motions&lt;/a&gt;. I currently have several of them up as room decorations and desk ornaments, and I’m pretty sure they’ll stand through a sizable earthquake. What’s the trick?&lt;/p&gt;

&lt;p&gt;It turns out the gravitree is best understood from the top down: we’ll first take a close look at the top piece, then at the top two together, then the top three, all the way down. As we’ll see, it’s just using the same trick again and again.&lt;/p&gt;

&lt;p&gt;The key to understanding the sculpture is the concept of &lt;em&gt;center of mass&lt;/em&gt;. Think for a moment about how gravity acts on your body. It pulls down on every gram of you, all throughout your volume. The total force is equal to your weight, but the way that force is spread out in space depends on your shape and your pose.&lt;/p&gt;

&lt;p&gt;An object, like you, can have quite a complex shape, so you’d naturally think it would be very complicated to figure out how gravity will pull and spin a particular object. It turns out, however, that as long as the object doesn’t bend much, we can pretend that all the force is pulling at one point, called the object’s center of mass, and we’ll correctly predict its motion. As the name suggests, the center of mass is the average location of mass in the object, and it’s closer to bigger, denser parts.&lt;/p&gt;

&lt;p&gt;Let’s look at the center of mass of the top piece of the gravitree. First, I’ll attach two weights to the bottom, which will bring the center of mass to well below the balance point. This makes the object a lot like a pendulum, which just consists of a point of rotation and a big mass some distance below it. Like a pendulum, it’s very stable - gravity tends to try to pull it back to vertical - and when we poke it, we see it follow quick oscillations.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/superstable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/superstable.gif&quot; width=&quot;17%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;By contrast, if I put the weight at the top, the center of mass is now above the balance point, and it’s &lt;em&gt;unstable&lt;/em&gt;: gravity now tends to pull it away from vertical. If the previous weighting’s like a pendulum, this one’s like a pencil balanced on your finger. It won’t stand on its own.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/unstable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/unstable.gif&quot; width=&quot;16%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;For a balanced object, these are the choices. It can either be stable or unstable, switching from one to the other as the center of mass crosses the pivot point. This raises an natural question, though: what happens if you start with a stable object and push the center of mass closer and closer to the pivot but never crossing it? How would such a barely stable object move?&lt;/p&gt;

&lt;p&gt;That’s the secret of the gravitree. This piece is just barely stable, and because of that, it swings like a pendulum, but &lt;em&gt;very slowly&lt;/em&gt;. The technical reason’s that, since the center of mass is so close to the pivot, the torque from gravity is so small it gives the object only a tiny angular acceleration relative to its moment of inertia.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/stable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/stable.gif&quot; width=&quot;16%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This explains both why the top piece is stable and why it moves so eerily, seeming to move through space without swinging around like you’d expect it to. What about all the other pieces, though? It turns out they all use the same trick, but when balancing a lower piece, you have to imagine the weight of all the higher layers acting on its top. To illustrate that point, let’s look at the second-highest piece. Without that extra weight, it’s very stable, but with that weight, it’s barely stable like the topmost piece.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/layer2_superstable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/layer2_stable.jpg&quot; width=&quot;20%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We can use this trick again and again all the way down. When I designed this sculpture, I used Autodesk Inventor’s center-of-mass-finding feature to balance the pieces from the top down, adjusting the sphere sizes to make each new piece barely stable under the weight of those above it. The end result is an entire tower that’s paradoxically both barely stable and yet very hard to accidentally knock over. This basic idea can take many different forms - here are several other gravitrees I’ve made over the years.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_christmas.jpg&quot; width=&quot;15%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_arrows.webp&quot; width=&quot;27%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_cages.webp&quot; width=&quot;21%&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_centipede.webp&quot; width=&quot;12%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_spider.webp&quot; width=&quot;23%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_spinner.gif&quot; width=&quot;28%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There’s a deeper trick we’re playing here, one that I see in inventions from &lt;a href=&quot;https://en.wikipedia.org/wiki/Reaction_wheel&quot;&gt;reaction wheels&lt;/a&gt; to &lt;a href=&quot;https://www.youtube.com/watch?v=jyQwgBAaBag&quot;&gt;faster-than-wind sailboats&lt;/a&gt;: once we know the precise rules of the universe, we can saunter just up to the edge of impossibility and thumb our noses at Nature like a toddler who knows they’re not &lt;em&gt;technically&lt;/em&gt; breaking their mother’s rules. For all of science’s immense capacity for social good, that sort of thing - the feeling of doing what seemed impossible until you stopped to think about it - is a key part of what compels me to study physics, and I think it’s a potent way to inspire the curious to think a little closer about how the universe really works.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Oct 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/physics,%20fun-science/2021/10/11/gravitrees.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics,%20fun-science/2021/10/11/gravitrees.html</guid>
        
        
        <category>physics, fun-science</category>
        
      </item>
    
      <item>
        <title>Could you propel a spacecraft using sports projectiles?</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;In the fall of 2020, Ryan Roberts and I gave a remote talk for high schoolers through Berkeley Splash, our third exploring real physics through absurd scenarios instead of technical math. His talk discussed methods and consequences of making the Moon as big as the Earth, while mine aimed to find the best way to propel a spacecraft using only sports equipment. This is a writeup of the answer.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since the dawn of civilization, humans have gathered for sports competitions, with rounds of faceoffs ultimately whittling the athletes down to a sole champion. This time-honored tradition has flowered throughout history; the modern Olympic Games involve dozens of sports split into hundreds of events. This explosion of athletic diversity has made determining an ultimate champion a lot more complicated, though; wouldn’t it be nice if there were some way we could determine who, of all the gold medalists, is actually the best? What if there were some grand final competition among the event champions to determine an ultimate victor?&lt;/p&gt;

&lt;p&gt;For the consideration of the International Olympic Committee, I hereby propose the event of Rocketball as the Games’ final event. The rules are simple. To start, every gold medalist will be launched into Earth orbit with nothing but their wits, a spacesuit, and an absurd amount of equipment from their sport. They have no traditional rockets; to change course, the athletes have to hit, throw, fire, bowl, or otherwise propel sports equipment the other way, powered solely by their bodies. Instead of a goal line, there’s a target speed: whoever can accelerate themselves to reach that final speed first wins the Olympics&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This proposal has one&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; minor logistical problem: some sports are better than others. For example, it’d be way easier to accelerate by hitting tennis balls than by hitting ping-pong balls; a hit tennis ball is both heavier and faster, so (as we’ll see) as a rocket propellant it’s strictly better. Actually, in some cases, which sport is best can even depend on how fast the target speed is! To understand this, figure out how matches would play out, and see if we can come up with a fix, first we need to understand a little about how rockets work.&lt;/p&gt;

&lt;p&gt;At their core, all rockets operate on the same principle: they fling matter in one direction, and the recoil force pushes the rocket in the opposite direction. This recoil force is the same sort of pushback you’d feel when firing a gun, holding a fire hose, or doing a fast chest pass in basketball, but in a typical rocket the matter being launched is exhaust from burning fuel moving several kilometers per second. True to this principle, rockets have two main parts: the &lt;em&gt;fuel,&lt;/em&gt; which is gradually fired to propel the rocket, and the &lt;em&gt;payload,&lt;/em&gt; which is the important other bits that the fuel’s there to accelerate. The fuel section also usually includes engines that help burn the fuel but fall off when they’re no longer needed. Here’s a diagram of the Saturn V rocket split into the launch vehicle (fuel + engines) and spacecraft (payload).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/saturn_v.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Our sportsball rockets will work basically the same way, but instead of liquid fuel they carry sports equipment, and instead of the most powerful engines ever built, they’ll be powered by lone humans flinging objects into the vacuum of space.&lt;/p&gt;

&lt;p&gt;If you’re not familiar with rocketry or are very used to cars, you might wonder why the Saturn V has so much fuel for such a small payload. At some level, the reason’s that we want the payload to reach a very high speed (over 10 km/s), so we need a lot of fuel, but there’s another reason specific to rockets: &lt;em&gt;the earlier fuel has to accelerate all the later fuel, so you need more of it.&lt;/em&gt; If a competitor gradually hit away a million golf balls, the first ball would give a very small bump in speed because it has to push the other 999999 balls, while the last one would give a relatively big bump in speed because it’s only accelerating the athlete. A rocket’s acceleration is inversely proportional to its current mass. Because of this, it turns out that the amount of fuel you need is an exponential function of the final speed, and ending up slightly faster can potentially take many times more fuel.&lt;/p&gt;

&lt;p&gt;Rockets are complicated, but for a rocket accelerating in a straight line, it turns out we can find its speed over time if we know just a few numbers. These are the payload mass (which we’ll call \(m\)), the starting mass of the fuel and payload together (\(M\)), the relative speed the propellant’s fired at (\(u\)), and the thrust of the rocket (this is the average kickback force the rocket feels; we’ll call it \(F\)). Here’s a diagram illustrating what some of these are.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocket_math_diagram.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;If you do the rocket science, you find that to reach a final speed \(v\), the rocket needs a starting mass of \(M = m e^{v/u}\), and it runs out of fuel and reaches speed \(v\) at a time \(t = \frac{(M-m)u}{F}\). For a given target speed \(v\), this choice of \(M\) is optimal - with less fuel, it won’t reach the target speed, but with any more, it’ll reach it slower.&lt;/p&gt;

&lt;p&gt;We can use these formulae to figure out roughly what’ll happen in our intersport showdown. For a given sport, \(u\) is just the projectile speed (around 76 m/s for a golf ball and 4 m/s for a pool ball) and the thrust is equal to (projectile speed)\(\times\)(projectile mass)\(\times\)(fire rate). If we assume a constant fire rate of 1 shot/s across all sports for simplicity, we can get the speed and thrust for every sport. Here’s a plot showing the results.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_scatterplot.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;A quick look at this might reveal some oddities. Jai alai (also called Basque pelota) was once an Olympic sport; it’s like wallball but with a giant curved launcher on your throwing arm. Bowling and pool I’ve included even though they’re not Olympic sports. Crossbow shooting I included because our question is ultimately about projectiles launched solely with energy from human muscles, and it’s one of the best examples of that. For the same reason, I didn’t include gun sports. Lastly, running and swimming events don’t have projectiles, so I assumed their respective gold medalists will be throwing shoes and spitting mouthfuls of pool water&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This plot makes it clear that some sports are better than others, but it’s not apparent which is best. To decide, we’ll have to pick a target speed. Let’s say the athlete and their suit are 100 kg, and they’re trying to reach a final speed of 10 m/s. We’ll also assume that the athlete can repeatedly launch projectiles in the same direction without uncontrollably spinning or getting knocked off, but that seems fair; after all, they are professionals. About how long will each sport’s champion take to finish?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_10.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This plot shows that the best sport to reach 10 m/s is shot put. It’d actually only take 24 solid throws to finish, which is 24 seconds with our assumption of one shot per second. Most other sports take a few minutes; the swimmers will be spitting into the void for several days.&lt;/p&gt;

&lt;p&gt;What if we instead want to reach 100 m/s?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_100.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now the best option is to accelerate with 2053 jai alai throws, which takes a little over half an hour. Due to the exponential in our time formula, however, accelerating with pool balls will now take over a hundred million years. Surprisingly, most high-energy projectile sports are all relatively close, with a time less than a few hours. Actually, at a target speed of 130 m/s, the entire swath from crossbow to javelin is about even! This, I propose, is the ideal choice for Rocketball; the sports without high-energy projectiles can get a handicap.&lt;/p&gt;

&lt;p&gt;What if we make the target 1000 m/s?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_1000.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;With this high target speed, the \(e^{v/u}\) term is so dominant that the thrust is basically irrelevant compared to the exhaust speed. The best option now is to use 35 million crossbow bolts, which will take around a year. This is one of the few situations where your best choice is to use a bow in space. All the sports slower than javelin will take longer than the current age of the universe.&lt;/p&gt;

&lt;p&gt;This is a decent analysis for the purposes of Rocketball, but these speeds are miniscule on the scale of the Solar system. What if we want to reach 10000 m/s, roughly Earth escape velocity? As you might guess, exhaust speed is even more dominant here, and crossbow bolts are still the best choice. However, you’d need \(10^{43}\) of them. The good news is that they’d hold together under their own gravity, so you wouldn’t need a quiver. The bad news is that the arrow ball would have a bigger diameter than the orbit of Saturn and would immediately collapse into a black hole.&lt;/p&gt;

&lt;p&gt;There is, however, a sport we’ve overlooked. The tires of a bike contain air under high pressure, and if there’s a hole or open valve, that pressure will accelerate the escaping air to high velocities. Normal bike valves are narrow and slow down the escaping air a lot, but if you designed a much bigger one (or just punched a hole in the tire) it’d come out much faster - a 50 psi tire would, in the ideal case, accelerate air to 760 m/s! It turns out that you could reach 10000 m/s by ideally releasing enough 50-psi air to fill a sphere with a diameter of 270 m.&lt;/p&gt;

&lt;p&gt;How long would that take? By our constraint of only man-powered projectiles, you’d have to pump up the air. This is about 47 billion bike tires’ worth of air, so it’d take around a millenium even if you’re the &lt;a href=&quot;https://www.guinnessworldrecords.com/world-records/419351-fastest-time-to-pump-a-bicycle-tyre&quot;&gt;world’s fastest tire pumper&lt;/a&gt;. The release, however, would be quick.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/air_jetpack.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;and gets to come back to Earth. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(this is a lower bound) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Spacesuit helmets would admittedly block the spit-out pool water, but since humans can survive zero pressure for short periods and swimmers are already good at not breathing, let’s say they take their helmets off to spit. Another option’s to throw chunks of ice, which would fall between “shoe throw” and “shotput.” &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 29 Nov 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/physics,%20fun-science/2020/11/29/rocketball.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics,%20fun-science/2020/11/29/rocketball.html</guid>
        
        
        <category>physics, fun-science</category>
        
      </item>
    
      <item>
        <title>The principle of least power dissipation</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;I’m currently in a class called Physics of Computation on new types of computing paradigms that don’t solve problems in discrete, sequential, logical steps like modern computers.  Instead, the whole computing system goes through a complex analog evolution, with every variable free to change simultaneously, and eventually settles into a stable final state representing the answer.  Naturally, one of the core challenges is in designing a system that predictably settles into a steady state with some desired properties.&lt;/p&gt;

&lt;p&gt;Our professor’s pointed out that many such computing systems can be seen to accomplish this by using a little-known concept called the ‘principle of least energy dissipation.’  When a battery or current source is hooked up to a DC circuit, a sudden flow of current flares through the different branches of the circuit, sometimes much more or less at first than is sustainable.  Eventually, though, these initial errors fade away, and the circuit ends up in a steady state, with currents and voltages everywhere that don’t change in time.  The principle of least energy claims that &lt;strong&gt;of all the possible steady states satisfying the voltage and current constraints, the real steady state dissipates the least amount of power.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’ve solved tons of DC circuits, and I’d never heard of this.  To my shock, though, it worked for a few simple circuits we discussed in class.  However, these simple examples neither showed it always works or explained why it does.  To try to understand, I read &lt;a href=&quot;https://www.feynmanlectures.caltech.edu/II_19.html#footnote_1&quot;&gt;Feynman’s lecture on minimization principles&lt;/a&gt;; it turns out that at the end he mentions this law but doesn’t really explain it, so I decided to fill in the gap.&lt;/p&gt;

&lt;p&gt;It’s actually easier to see that it’s true if you start with a general, continuous 3D system; everything’s captured in one differential equation instead of having to deal with discrete voltages and resistors.  Suppose we have a volume \(V\) filled with a resistive medium with resistivity \(\rho(\mathbf{r})\).  It could have different resistances at different points; that’ll be important later.  Now, suppose there’s an electric potential (a voltage) \(\phi(\mathbf{r})\) throughout the medium.  We also need some way to add a constraint to the system (like plugging a source into a circuit), so we’ll suppose that the potential at the surface is completely fixed; perhaps it’s higher at one end and lower at the other to cause a current flow.  In real circuits, currents cause magnetic fields that can affect charges a little, but we’ll ignore magnetism for this problem.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/electrostatic_setup.png&quot; width=&quot;20%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The currents and voltages throughout a resistive circuit are determined entirely by the sources and the circuit structure, so we expect that \(\phi\) and the current should be determined entrely by boundary conditions and \(\rho\).  How do we find them?  Well, the typical way is to say that the potential corresponds to an electric field \(\mathbf{E}(\mathbf{r}) = - \mathbf{\nabla}\phi\) which then creates a current \(\mathbf{J} = \rho^{-1} \mathbf{E}\) according to the continuous version of Ohm’s Law.  In the steady state, charge isn’t building up anywhere, which implies that \(\mathbf{\nabla} \cdot \mathbf{J} = -\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\).  Solving this to match the boundary conditions on \(\phi\) gives the potential, which gives the current.  Note that this would reduce to \(\nabla^2 \phi = 0\), which you might recognize as the equation voltage satisfies when there’s zero charge density, if \(\rho\) were a constant; the reason it doesn’t is that, even in steady-state DC circuits, there’s actually built-up charge on the interfaces between components of different resistivities.&lt;/p&gt;

&lt;p&gt;Let’s see if we can derive that differential equation not by assuming that \(\mathbf{\nabla} \cdot \mathbf{J} = 0\) but just by assuming that the power dissipation is as small as possible.  The total power dissipation is given by \(P = \int_V (\mathbf{J} \cdot \mathbf{E}) \mathop{}\!\mathrm{ \mathbf{dr} } = \int_V \rho^{-1} (\mathbf{\nabla}\phi)^2 \mathop{}\!\mathrm{ \mathbf{dr} }\).  Feynman’s lecture neatly explains how to find minimal solutions, so I’ll just tear through the steps quickly.  Since we’re looking for a minimum, we want to find a choice of \(\phi\) that makes \(P\) invariant to perturbations to first-order.  If we add a small perturbation \(\delta\phi\) to \(\phi\), so the new potential field is now \(\phi + \delta\phi\), the power dissipation becomes \(P + \delta P\), where \(P\) is the same as before and \(\delta P = 2 \int_V \rho^{-1} \mathbf{\nabla}\phi \cdot \mathbf{\nabla}(\delta\phi) \mathop{}\!\mathrm{ \mathbf{dr} }\).  The usual integration by parts trick gives \(\delta P = -2 \int_V \mathbf{\nabla} \cdot \big[ \rho^{-1} \mathbf{\nabla}\phi \big] \delta\phi \mathop{}\!\mathrm{ \mathbf{dr} }\), and since \(\delta\phi\) was arbitrary, this implies that \(-\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\)!  That’s it - minimizing power dissipated is equivalent to the same steady-state differential equation you get from working through the electrodynamics the normal way.  Any \(\phi\) that minimizes power dissipation is a real physical solution, and any physical solution minimizes power dissipation.&lt;/p&gt;

&lt;p&gt;The math is all well and good, but how can we really &lt;strong&gt;understand&lt;/strong&gt; it?  What’s the physical reason why power should be minimized?  Well, the differential equation we derived from power minimization is just the fact that the current \(\mathbf{J}\) has zero divergence.  That actually makes sense in terms of minimizing power dissipation!  Imagine a point in the medium with nonzero current divergence, so it’s a sink (or a source) of current.  In the vicinity of that point, on top of whatever divergence-free flow there is, there’s an extra part of the current that flows into or out of the point.  In a rough sense, it turns out that you can decrease the dissipated power by just dropping that extra part of the current.&lt;/p&gt;

&lt;p&gt;That’s the case of a problem where the voltage is prescribed.  Circuits can have both voltage and current sources, though; what about a case where the current on the boundary is given?  Well, first, it’s important to note that only the current &lt;em&gt;normal to the boundary&lt;/em&gt; can be prescribed; to illustrate why, if you could prescribe arbitrary currents on the boundary, you could have the current flow in a circle around the boundary, which we know is already unphysical.  We’ll also have to assume that \(\mathbf{\nabla} \cdot \mathbf{J} = 0\) - if we just let charge accumulate in the medium, the minimum power dissipation solution will just be to have zero current everywhere and have charge build up forever on the surface.  However, if we again minimize power dissipated using vector calculus tricks, we get a new, interesting condition: \(\mathbf{\nabla} \times (\rho\mathbf{J}) = 0\); the curl of \(\rho \mathbf{J}\) is zero.  Since curl-free vector fields are gradients of potentials, this gives us the condition that \(\mathbf{J} = -\rho^{-1}\mathbf{\nabla}\phi\) for some potential \(\phi\), and since we assumed the current has zero divergence, this gives us \(-\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\)!  Again, the new condition that minimizing power dissipation bought us actually makes physical sense; if the curl of \(\rho \mathbf{J} = \mathbf{E}\) weren’t zero, there would be current flowing uselessly around in loops and dissipating extra power.&lt;/p&gt;

&lt;p&gt;Alright, now we’ve decently explained why power dissipation is minimized in scenarios when current’s flowing through a continuous resistive medium.  We were originally interested in circuits, though; how does this relate?  We can actually cleverly fashion a resistive circuit by just changing \(\rho(\mathbf{r})\)!  For most of the volume, we’ll let \(\rho \rightarrow \infty\), which turns it into a very good insulator, like the air between the wires in a circuit.  Then we’ll let \(\rho \rightarrow 0\) in some long, thin regions that will become wires.  Along the wires we’ll make some regions have intermediate values of \(\rho\), which makes them resistors.  Lastly, if we want, we can choose our boundary so all of space &lt;em&gt;except&lt;/em&gt; where a source will go is enclosed in it, then specify the voltage or current along the boundary.  This gives a circuit of sources, wires and resistors that might look as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/resistive_circuit.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now we have a circuit.  How does minimal power dissipation explain its current and voltage distribution?  From the point of view of choosing voltages at each node of the circuit, any voltage distribution besides the true one will lead to charges accumulating somewhere, which requires some extra current and dissipates extra power.  From the point of view of choosing how currents branch at every node, any split besides the true one will correspond to extra current running around in a loop, which wastes power, and also an electric field with a nonzero curl, which doesn’t correspond to a potential.&lt;/p&gt;

&lt;p&gt;We’ve shown that the principle of least power dissipation gives the right answers for steady-state circuit problems.  If this is really a general law of nature, though, it’d probably show up outside of just this one narrow context.  It turns out that it can also be applied to &lt;em&gt;fluid circuits&lt;/em&gt;, which are sometimes studied as somewhat-similar sister systems to electric circuits.  In the sort of fluid circuit I’m imagining, there are networks of pipes of different thicknesses instead of resistors with different resistances.  Water flow rate \(Q\) replaces current, pressure \(P\) replaces voltage, the rate of power dissipation is \(PQ\) instead of \(VI\), and pipes have an effective resistance.  An ideal pipe with laminar flow has \(\Delta P\) proportional to \(Q\) in &lt;a href=&quot;https://en.wikipedia.org/wiki/Hagen%E2%80%93Poiseuille_equation&quot;&gt;an equation analogous to Ohm’s Law&lt;/a&gt;, and the quantity analogous to resistance depends on the pipe’s dimensions and the viscosity of the fluid.  The fact that these equations map onto each other so perfectly means that fluid through a network of pipes minimizes power dissipation just as electrical circuits do.  I’d be willing to bet that, just as we derived part of the differential equation for current flow, you could derive part of the Navier-Stokes equations just from minimizing power dissipation.&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Sep 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/physics,%20research/2020/09/06/least_power_dissipation.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics,%20research/2020/09/06/least_power_dissipation.html</guid>
        
        
        <category>physics, research</category>
        
      </item>
    
      <item>
        <title>Multiplicative neural networks</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the past decade, neural networks have proven to be stunningly effective tools for a slew of machine learning tasks as diverse as classifying images, generating 3D graphics, and playing games.  These incredible successes all stem from neural networks’ remarkable ability to perform a central task in machine intelligence: given complex data, find underlying patterns.&lt;/p&gt;

&lt;p&gt;Boiled down to their core, neural networks are just a series of &lt;em&gt;hierarchical nonlinear transformations&lt;/em&gt;.  Each layer of the neural network is one layer in this hierarchy, transforming a feature vector and passing it to the next layer up, until it reaches the output.  Each layer usually involves a step that mixes the elements of the feature vector according to a set of weights followed by a nonlinear function applied to each element of the vector.  All types of neural networks use a series of these layers to do the basic information processing.&lt;/p&gt;

&lt;p&gt;Within that hierarchical nonlinear structure, however, there are many variations.  There are a vast array of different ways layers can be connected and parameterized, giving different architectures like convolutional nets, recurrent nets, and resnets specialized to certain types of task.  However, there are also lower-level things that can be changed.  One example is the nonlinear function, usually denoted \(\sigma\).  The field’s explored many options for \(\sigma\) over the years, and the most popular choice has changed over time.  The \(\tanh\) function (and similar sigmoid) were once the leading choice, but the \(\text{ReLU}\) (defined by \(\sigma(x) = \max(x, 0)\)) is now more popular.  Choosing a different nonlinearity can improve performance slightly, but to me it is more striking that the choice of nonlinearity matters so &lt;em&gt;little&lt;/em&gt;.  On the face of it, the \(\text{ReLU}\) and \(\tanh\) functions are basically as different as they could be within reason.  \(\tanh\) is smooth, saturating, nonlinear on every finite set, and is symmetric about the origin.  \(\text{ReLU}\) has a kink, is unbounded, piecewise-linear, and never takes negative values, not to mention that it outputs zero on half of its domain.  The fact that both of these functions could work reasonably well speaks to the fact that even though the hierarchical nonlinear structure is core to the effectiveness of neural nets, the exact choice of nonlinearity is, surprisingly, not crucial.&lt;/p&gt;

&lt;p&gt;In fact, even the typical layer structure of matrix multiplication + elementwise nonlinearity isn’t crucial to performance.  It has been shown that tensor networks, a completely different type of hierarchical nonlinear model commonly used in physics, can also learn machine learning tasks.  &lt;a href=&quot;https://arxiv.org/abs/1605.05775&quot;&gt;This paper&lt;/a&gt; develops the idea and gets 1% test error on MNIST, which is pretty good for a totally new approach.  Interestingly, neural networks have recently been used instead of tensor networks in computational physics problems and achieved good performance there.  This raises a natural question: how do we know that the matrix multiplication + elementwise nonlinearity structure is really the best one?  To my knowledge, there’s no known fundamental, theoretical reason why it would be better than other options.&lt;/p&gt;

&lt;h2 id=&quot;multiplicative-nets&quot;&gt;Multiplicative nets&lt;/h2&gt;

&lt;p&gt;Here’s an idea for a different type of nonlinear hierarchical model.  What if we took a neural network and replaced the matrix multiplication step \(\sum_j w_{ij} x_j\) with a product step \(\prod_j x_j ^ {w_{ij}}\), with the weights as exponents in a product instead of coefficients in a sum?  This would still mix together the elements of the feature vector \(x\), just like a matrix operation, but in a different, nonlinear way.  Given the way deep learning systems can sometimes be surprisingly invariant to details of implementation, maybe this new, different sort of model could still work well.&lt;/p&gt;

&lt;p&gt;Architectures using this idea are called &lt;strong&gt;multiplicative neural networks&lt;/strong&gt;.  The idea was &lt;a href=&quot;https://dl.acm.org/doi/10.1162/neco.1989.1.1.133&quot;&gt;first proposed in 1989&lt;/a&gt; by neuroscientists seeing multiplication as more biologically plausible and potentially more powerful than addition.  They were &lt;a href=&quot;https://clgiles.ist.psu.edu/papers/NIPS94.product.units.pdf&quot;&gt;tested experimentally&lt;/a&gt; on some very small problems in the next decade and once &lt;a href=&quot;https://sci2s.ugr.es/keel/pdf/specific/articulo/Schmidtt%20on-the-complexity-of.pdf&quot;&gt;analyzed from a complexity standpoint&lt;/a&gt; in the decade after.  The architectures these papers describe are a little more general - they consider combining additive and multiplicative neurons in the same net and using hybrid terms with more weights of the form \(v_1 \prod_j x_j ^ {w_{ij1}} + v_2 \prod_j x_j ^ {w_{ij2}} + ... + v_n \prod_j x_j ^ {w_{ijn}}\) - but their key innovation is the multiplicative form.  However, despite this research, multiplicative nets never broke through to practical use; we still use additive neural networks with normal matrix operations.&lt;/p&gt;

&lt;p&gt;This is pretty understandable - at a first glance, multiplicative nets seem like they’d have some problems.  One immediate question is what to do when \(x_i &amp;lt; 0\) and \(w_{ij}\) isn’t an integer, in which case \(x_i ^ {w_{ij}}\) isn’t  uniquely defined, but to avoid that problem let’s just set things up so that all the \(x_i\) are positive, which I’ll show we can do with the right choice of data and nonlinearity.  Even dealing with this, though, these nets have some pretty prolific potential practical problems.  First, there’s been a lot of research and hardware development into making matrix operations faster, and this wouldn’t use them.  Second, replacing simple matrix operations with something more complex could lead to weird gradients and failed optimization.  Third, as a friend noted, even from a pure computational complexity standpoint, calculating \(x^y\) is more expensive than calculating \(x*y\), so this would be slow.  Finally, the advantages of switching to this architecture are far from clear, so there’s no incentive to overcome these problems.&lt;/p&gt;

&lt;p&gt;The purpose of this post is to explore something interesting that isn’t discussed in the above papers: &lt;strong&gt;the multiplicative nets you get by replacing \(\sum_j w_{ij} x_j\) with \(\prod_j x_j ^ {w_{ij}}\) are basically identical to normal, additive neural nets with a different choice of nonlinearity.&lt;/strong&gt;  The basic reason is that, as the 1989 paper did note, \(\prod_j x_j ^ {w_{ij}} = \exp \big( \sum_j w_{ij} \ln(x_j) \big)\), so a multiplicative layer is the same as an additive layer where you take the logarithm before and an exponential after.  The main trick we’ll use is the fact that you can then absorb the \(\ln\) and \(\exp\) into the nonlinearity to get a new nonlinearity \(\sigma'(x) = \exp\sigma(\ln(x)))\), or, with compositional notation, \(\sigma' = \exp \circ \sigma \circ \ln\).&lt;/p&gt;

&lt;p&gt;My argument could easily be laid out with standard symbolic math.  However, that’s not how I think about it, and I don’t think it’s the clearest way to explain it.  Notation matters; different ways of writing the same thing inspire different kinds of manipulations, and cleaner notations are better to learn with.  Instead of algebra, I’ll use diagrams.&lt;/p&gt;

&lt;p&gt;Basic neural nets are built of a few basic pieces.  There are &lt;em&gt;input nodes&lt;/em&gt;, where input data vectors go into the net.  There are &lt;em&gt;output nodes&lt;/em&gt;, where the net’s output comes out.  There are elementwise &lt;em&gt;nonlinear functions&lt;/em&gt;, mapping real numbers to real numbers.  There are &lt;em&gt;additive&lt;/em&gt; or &lt;em&gt;matrix operations&lt;/em&gt;, each with its own 2D array of parameters \(W\), which in normal neural nets come between applications of nonlinear functions.  Lastly, we’ll include &lt;em&gt;multiplicative operations&lt;/em&gt;, which also each have their own 2D array array of parameters &lt;span style=&quot;color:purple&quot;&gt;\(W\)&lt;/span&gt; as discussed above.  We’ll use color to differentiate between additive and multiplicative operations; additive will be black, multiplicative will be purple.  Note that changing an operation from additive to multiplicative or vice versa doesn’t change its parameter matrix; the same numerical weights are just interpreted differently in a different operation.  We could also add pieces like biases or specialized layers like softmax or maxpooling, but we’ll leave those out for now for simplicity.  We could cast these pieces into diagrammatic form as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_components.png&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We can now build neural nets out of these components.  For example, the following is a standard additive neural net with 3-dimensional input, 3-dimensional output, two hidden layers with width 3, and a nonlinearity \(\sigma\).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_example.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There are a few different ways we can manipulate these diagrams while still keeping the model the same.  The first way is &lt;em&gt;function composition&lt;/em&gt;.  If there are two nonlinear functions in a row, we can just replace them with one new function.  This is just saying that the double function application \(f(g(x))\) is equivalent to the single function application \(h(x)\), defining \(h = f \circ g\).  One special, familiar case is when the two functions are each other’s inverses, and the new function is simply the identity; for example, \(\ln(\exp(x)) = x\).  Instead of writing the identity function explicitly, we can just write a line.  Note that in the notation \(f \circ g\), functions are applied right to left, but in our diagramas, everything flows from left to right, so the ordering of the composed functions might seem backwards at first.  There’s also the multiplicative-additive net identity we talked about earlier: \(\prod_j x_j ^ {w_{ij}} = \exp \big( \sum_j w_{ij} \ln(x_j) \big)\).  A multiplicative layer is the same as an additive layer with logarithms before and exponentials afterwards.  We can write these rules like this:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_diagram_rules.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;These rules lay out ways we can manipulate diagrams.  As a test of this notation, with just these simple rules we can derive a companion rule to the multiplicative-additive identity.  Just as you can take a function of both sides of an algebraic equation, we’re allowed to modify a diagrammatic equation by attaching pieces to dangling ends of both diagrams in the same way.  In this case, we’ll attach exponentials on all dangling ends on the left and logarithms on the right of both diagrams.  This gives us a new diagrammatic equation.  We can then use function composition to simplify the adjacent logarithms and exponentials to get a new, simple identity.  This one tells us that \(\ln \big( \prod_j  \exp(x_j)^{w_{ij}} \big) = \sum_j w_{ij} x_j\), which you can algebraically check is true.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/manipulation_example.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now, let’s put together a multiplicative net and see what we can derive.  Our starting point will be the same simple 3-3-3-3 net as before, but with multiplicative layers; it will be clear that our operations will generalize to different sizes.  First, we use the multiplicative-additive identity to get an additive net.  However, instead of just having one nonlinear function acting on each element of the feature vector, there are now three in succession.  Using function composition, we can just group these into a new nonlinearity we define as \(\sigma' = \ln \circ \sigma \circ \exp\).  We now arrive at an additive net &lt;em&gt;exactly equivalent to the multiplicative net&lt;/em&gt;.  The only major oddity of the new additive net is elementwise logarithms at the start and exponentials at the end.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/multiplicative_net_transformation.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;These logarithms and exponentials at the start and end aren’t surprising.  We’re requiring that multiplicative nets need positive input and give positive output, so since logarithms of negative numbers aren’t real, these logarithms enforce the positivity of the input.  The exponentials similarly enforce the positivity of the output.  Also, as long as \(\sigma\) maps positive numbers to positive numbers, the net only operates on positive numbers intermediately, and we don’t have the undefined power problem.  We can make our diagram even simpler by absorbing the logarithms and exponentials into the inputs as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/end_absorption.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;It turns out that a multiplicative net is basically the same as an additive net &lt;em&gt;with the same weights!&lt;/em&gt;  Our choice of notation emphasizes the similarity of form.  However, is this really a meaningful, useful correspondence?  The main question is this: what’s the nonlinearity \(\sigma'\) like?  It’s possible that \(\sigma' = \ln \circ \sigma \circ \exp\) is some bizarre function that’d be totally nonfunctional in an additive net, which would bode badly for our multiplicative net.  To answer this, a few examples are plotted below.  The blue curves show different choices of nonlinearity for multiplicative nets, defined only for positive inputs, and the orange curves show the corresponding additive nonlinearities.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/nonlinearities.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As shown in (a), when \(\sigma\) is the \(\tanh\) function, \(\sigma'\) looks a lot like a smoothed \(\text{ReLU}\), also called a softplus!  It’s flipped across the x- and y-axes, but that doesn’t change the usefulness of a nonlinearity in an additive neural net.  This would definitely work as an activation function.  Surprisingly, (b) shows that when \(\sigma\) is a sigmoid, \(\sigma'\) is also roughly a sigmoid (but again flipped across both axes), which also works as a nonlinearity.  Shown in (c) is the case when the multiplicative net uses a \(\text{ReLU}\) nonlinearity.  This is a distinctly horrendous choice for only positive inputs, since it’s the same as the identity on positive input, which is reflected in the fact that \(\sigma'(x) = x\).  A multiplicative neural net with \(\text{ReLU}\) nonlinearities is basically linear; adding extra layers doesn’t give it any power since, for additive networks, a multilayer linear net is reducible to a linear model.  With a slight modification, however, we get a very useful nonlinearity.  As shown in (d), if the nonlinear net uses a modified \(\text{ReLU}\) - specifically, \(\sigma(x) = \max(x, 1)\) - the corresponding linear net has exactly \(\text{ReLU}\) activations.&lt;/p&gt;

&lt;p&gt;It might not be clear what, exactly, this means for multiplicative nets.  Let’s suppose that we’re using a multiplicative neural net to do a task - say, classifying images - and we operate it by exponentiating the pixel values at input and taking the log of the output.  We have shown that, for a reasonable nonlinearity, the function the multiplicative net computes as a function of weights and data - say, \(f(X ; W)\) - is exactly the same as for a reasonable additive net.  That means that the loss surface is the same.  That means that the gradients are the same, and that means that the trainability is the same, and all this means that the theoretical usefulness of multiplicative neural nets, including their representational power and optimization behavior, are the same as the standard additive neural nets that have been the focus of such intense study.  Multiplicative nets are just as effective, there’s a correspondence to additive nets, and we have the conversion algorithm.&lt;/p&gt;

&lt;p&gt;This fact is intriguing, but what does it mean in a broader sense?  This doesn’t make multiplicative nets practically useful; it’s still faster to perform a matrix multiplication than a multiplicative step, and since they amount to the same thing there’s no reason to prefer the latter.  To me, the reason this fact bears consideration is the hint it might be towards what truly gives deep learning its unreasonable effectiveness.  Multiplicative and additive nets have starkly different functional forms, and at a first glance I’d guess they have wholly different behavior as models.  The fact that they actually have equivalent power and utility seems like a hint that the fundamental magic of deep learning doesn’t lie in the details of implementation, like the choice of activation function or even the choice of matrix multiplication for mixing; the latter’s just preferred for convenience.  It seems likely that the key to deep learning’s success is something more deep and general about hierarchical nonlinear structures, and wildly disparate hierarchical models, from standard neural nets to multiplicative nets to tensor networks, all succeed because in some deep way they all fit this broad category.  Perhaps efforts to understand deep learning will eventually uncover a mathematical understanding of something like this.&lt;/p&gt;

&lt;h2 id=&quot;extras&quot;&gt;Extras&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Wait,” you might say, “there’s no clean correspondence to an additive net if the multiplicative net has a more complicated form, like involving a sum of product terms.  This analysis only works in one case when it happens to be trivial!”  It’s true that the exact correspondence is easy to break by adding a bell or a whistle to the multiplicative net.  The question, however, is whether these modifications add anything fundamentally different, or whether they’d be incremental changes at most.  I can only conjecture for now - maybe I’ll do experiments if this becomes a paper - but I’d guess it’s likely that there are many ways to superficially change the form of multiplicative nets without fundamentally changing their behavior because the same is true of additive nets.  For example, you can add extra parameters to neural nets by parameterizing the activation functions themselves; &lt;a href=&quot;https://arxiv.org/pdf/2006.03179.pdf&quot;&gt;this paper&lt;/a&gt; not only tried that, but ran an evolutionary algorithm to build complicated activation functions, and their error rates were only a few percent lower than with simple \(\text{ReLU}\)s.  Many such engineering intricacies lead to no big change in performance.  For that reason, I think it’s a decent hypothesis that multiplicative nets behave similarly to additive nets even when there’s no exact correspondence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;In the diagrams I drew, the nets have only weights, not biases.  Since \(\exp \big( \sum_j w_{ij} \ln( x_j ) + b_j \big) = e^{b_j} \prod_j x_j ^ {w_{ij}}\), we can add them by multiplying the product by \(e^{b_j}\), where \(b_j\) is a new bias parameter.&lt;/li&gt;
  &lt;li&gt;The choice of base \(e\) (i.e. \(\exp\) and \(\ln\)) in this article was arbitrary, and any other base would’ve worked.&lt;/li&gt;
  &lt;li&gt;Many types of specialized neural net layer can also be translated into multiplicative nets.  For example, a softmax is similar, but it doesn’t even require taking the exponentals.  \(\exp\) and \(\ln\) are monotonic, so they commute through max operations, so maxpool layers are the same for multiplicate nets.&lt;/li&gt;
  &lt;li&gt;Are there other choices for the mixing operation besides the matrices of additive nets and the multiplication operation shown here?  One way to generalize the concept to include both is to make the mixing transform \(f \big( \sum_j w_{ij} f^{-1}(x_j) \big)\), with \(f\) an invertible function mapping reals to reals.  In the case of \(f = \text{identity}\), this gives additive nets.  In the case of \(f = \exp\), this gives multiplicative nets.  In a case like \(f(x) = x^3\), though, it would give something new, but still equivalent to an additive net by the same sort of proof.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sci2s.ugr.es/keel/pdf/specific/articulo/Schmidtt%20on-the-complexity-of.pdf&quot;&gt;The most recent paper I’ve found on these nets&lt;/a&gt;, which was mentioned above, argues that multiplicative neurons perform operations that need large numbers of summing units, stating that “networks of summing units do not seem to be an adequate substitute for genuine multiplicative units.”  This post basically disproves that!&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 31 Aug 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/deep%20learning,%20research/2020/08/31/multiplicative-neural-nets.html</link>
        <guid isPermaLink="true">http://localhost:4000/deep%20learning,%20research/2020/08/31/multiplicative-neural-nets.html</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>How would an upside-down candle burn?</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;As every child knows, sometimes it’s fun to do something exactly wrong.  In that spirit, what would happen if you burned a candle upside-down?&lt;/p&gt;

&lt;p&gt;To take a guess, we need to understand how candles work.  Their simplicity is deceptive; burning candles are actually pretty complex systems with many interacting parts and processes.  They illustrate a slew of different chemical and physical effects so elegantly that the famous physicist Michael Faraday gave a &lt;a href=&quot;https://www.youtube.com/watch?v=RrHnLXMTOWM&quot;&gt;series of lectures&lt;/a&gt; on candles using them to demonstrate effects from from turbulence to buoyancy to blackbody radiation and to explain the chemistry of fire.  Candle flames are such common sights that we often overlook their complicated anatomy; look at one closely and you’re likely to see a number of features, from the blue glow at the bottom to the tapering of the tip, that everybody recognizes but few people could actually explain if asked.&lt;/p&gt;

&lt;p&gt;Explaining all of candle physics would take a textbook, but for this question, we only need to think about things that change when the candle is turned upside down, which simplifies the problem a lot.  The most obvious is that the heat of the flame, normally drawn up and away because hot air’s less dense than cold air, will flow directly into the tip of the candle.  Under normal operation, the tip of the candle wax is just hot enough to melt a small pool of wax but not hot enough to boil it, putting it at between 50°C and 370°C.  By contrast, the spot one wick-length above the tip of the wick is in the core of the flame, with a temperature in the vicinity of 1000°C.  If you could somehow turn just the flame upside-down without turning the rest of the candle, the flame would melt the wax a lot.&lt;/p&gt;

&lt;p&gt;However, candles are cyclic systems with a lot of feedback - the flame melts the wax which feeds the flame and so on.  We’ve guessed how the change in the flame caused by flipping the candle might affect the wax, but how would that change in the wax then affect the flame in turn?&lt;/p&gt;

&lt;p&gt;The flame’s shape would obviously change since the hot gas would be deflected as it rises by the block of wax in its way.  How would the burning itself change, though?  An important fact is that in candles, the wax only combusts as a vapor.  It starts as a solid, is melted and drawn up into the wick and into the heart of the flame by capillary action, and is vaporized by the high heat.  It’s only then that combusion happens - the hot, vaporized wax molecules react with oxygen and give off heat, perpetuating the process.  In the upside-down candle, more wax would be in the region of high heat, so there would be more hot surface area for vapor to escape from.  This means that wax would vaporize and burn at a higher rate, which, all else being equal, should give a bigger flame.&lt;/p&gt;

&lt;p&gt;However, there’s a problem with this situation.  This augmented flame would melt a lot of wax; what if it melts wax faster than it can vaporize it?  In a normal candle, there’s a small cup in the solid wax that holds the melted wax, but in the upside-down candle, the cup’s turned over.  All the excess wax might run and fall off the candle unburnt, so the candle would both go through wax faster and drip a lot.  There’s a stranger possibility, though.  What if the flame melts wax that floods down over the wick too thickly for the flame to vaporize it in time and snuffs it out?  Like Icarus, it’d die from melting wax and its own zeal.  This was my guess for what would happen to the upside-down candle.&lt;/p&gt;

&lt;p&gt;I also asked friends to guess how an upside-down candle would burn and got around 10 responses.  Most people either gave the self-snuffing scenario - let’s call it scenario (1) - or the second, self-sustaining scenario - say, (2).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/candle_scenarios.png&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;To answer this question, I dangled a candle, set up another for comparison, and lit them.  &lt;a href=&quot;https://www.youtube.com/watch?v=tJv4IOTvCWY&quot;&gt;A video of the result is here.&lt;/a&gt;  Forgive the subpar quality.&lt;/p&gt;

&lt;p&gt;This matched scenario (1) - the candle initially burns, but once it starts to melt wax it’s quickly put out.  Out of curiosity, though, I ran another trial with the same candle.  The result was wholly different, matching scenario (2).  It turns out that the same candle can follow either path, probably depending on the initial shape of the tip and wick.  The phase-space of this upside-down candle has two basins of attraction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XKAtwxw7SWk&quot;&gt;Here’s a video of the second trial.&lt;/a&gt;  Most people were right at least once.  Nobody, however, predicted the stalactite.&lt;/p&gt;

</description>
        <pubDate>Thu, 20 Aug 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/physics,%20fun-science/2020/08/20/upside-down-candle.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics,%20fun-science/2020/08/20/upside-down-candle.html</guid>
        
        
        <category>physics, fun-science</category>
        
      </item>
    
      <item>
        <title>Messing with the postal service</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;The United States’ postal system is an institution of renown, a conduit for objects and information older than the Constitution.  Even now, amid financial difficulties and numerous competing ways to send things, it’s still remarkably effective - neither snow nor hail nor remote destinations nor heavy loads fazes the US’ army of postal workers.  This raises a natural question: can we find something that does?&lt;/p&gt;

&lt;p&gt;Out of related curiosity, over years I’ve mailed a slew of things I’d guess were new to postmen.  These included a letter with a hole in the middle (arrived), a pentagonal letter (arrived), an origami crane (arrived), a pack of sticky notes with an instruction on the back for any postal worker reading it to take one note (arrived missing many), a dollar bill addressed like a postcard (returned, in an envelope, due to not having an envelope), a note in an envelope made of dollar bills (arrived), and a literal stick with an address written on it (arrived in a bag with an apology for any damages).  A piece of crumbling bark, two letters tied together, and a spherical rock the size of a golf ball weren’t successfully delivered.&lt;/p&gt;

&lt;p&gt;Why’d some of these objects arrive while others didn’t?  The USPS’ regulations and pricing seem to assume that anything sent is either a letter or in a large envelope or box, but I haven’t found anything explicitly forbidding sending things without boxes.  The lack of explicit rules probably means postal workers have a lot of leeway in deciding what to do with weird items, which makes sense and fits the data - I don’t see a sharp line that separates the successes from the failures above, but the failed items do seem a little more obnoxious.&lt;/p&gt;

&lt;p&gt;There are an infinite number of ways to mess with the postal service, though, and alone I’ve tried only a handful.  The theme’s worth some more variations.  I once led a group of children at a summer camp in mailing random objects, but only so much is accessible in the middle of the woods.  At the end of the spring 2020 semester, I and 11 other members of my physics cohort formed a group to mail each other weird things in the style of a secret Santa.  The group, dubbed the University Society of Postal Shenanigans (USPS), finished its first round in June.  Here are our results, ordered subjectively by the mailed object’s ridiculousness.&lt;/p&gt;

&lt;h4 id=&quot;halloween-decoration&quot;&gt;Halloween decoration&lt;/h4&gt;
&lt;p&gt;Delivered after several weeks&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/ween.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;mint-box&quot;&gt;Mint box&lt;/h4&gt;
&lt;p&gt;Not accepted via mailbox delivery and rejected at a post office by a persnickety employee&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/mints.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;hand-sanitizer&quot;&gt;Hand sanitizer&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/sanitizer.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;bonne-maman-jam-lid&quot;&gt;Bonne Maman jam lid&lt;/h4&gt;
&lt;p&gt;Not delivered&lt;/p&gt;

&lt;h4 id=&quot;anticannibalism-bracelet&quot;&gt;Anticannibalism bracelet&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/strap.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;glove&quot;&gt;Glove&lt;/h4&gt;
&lt;p&gt;Not delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/glove.jpg&quot; width=&quot;300px&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/gloveback.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;lemon-from-my-backyard-with-address-and-urgent-written-on-it-in-sharpie-mailed-within-berkeley&quot;&gt;Lemon from my backyard with address and URGENT written on it in Sharpie mailed within Berkeley&lt;/h4&gt;
&lt;p&gt;Delivered in a few days&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/lemon.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;lemon-coincidentally-picked-from-another-bay-area-backyard-and-mailed-to-berkeley&quot;&gt;Lemon coincidentally picked from another Bay area backyard and mailed to Berkeley&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/lemon2.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;origami-box-containing-three-origami-cranes&quot;&gt;Origami box containing three origami cranes&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/origami_box.jpg&quot; width=&quot;300px&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/origami_crane.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;lone-stamp-with-address-and-odd-message-on-back&quot;&gt;Lone stamp with address and odd message on back&lt;/h4&gt;
&lt;p&gt;Not delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/stamp_front.jpg&quot; height=&quot;350px&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/stamp_back.jpg&quot; height=&quot;350px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;chicago-flag-socks-with-an-address-and-stamps-sewn-into-one&quot;&gt;Chicago flag socks with an address and stamps sewn into one&lt;/h4&gt;
&lt;p&gt;Delivered (credit to Madeline Bernstein for the quality item)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/sock.png&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;3d-ball-maze&quot;&gt;3D ball maze&lt;/h4&gt;
&lt;p&gt;Delivered despite having too little postage.  As the postman collected perhaps the best $2.55 I’ve ever spent, he made it clear with his manner that he’d never seen something like this and wasn’t going to miss the chance to make fun of it&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/maze.png&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;dehydrated-celery-stalk-partially-wrapped-in-duct-tape&quot;&gt;Dehydrated celery stalk partially wrapped in duct tape&lt;/h4&gt;
&lt;p&gt;Not Delivered&lt;/p&gt;

&lt;p&gt;There are a few clear conclusions we can draw from these experiments.  First, the sheer probability of the postal service delivering something weird is shockingly high.  Second, they’re more likely to deliver something when there’s no way to give it back to the sender - the mint box was refusable, and my single dollar had a return address, and both failed, while almost nothing else risky had a return address.  Third, though object ridiculousness wasn’t very correlated with success, the apparent value of objects was - the ball maze, socks, and hand sanitizer all have some obvious use, while the celery, jam lid, and single glove don’t.  A natural combination of these conclusions is that the postal service will probably deliver anything weird they can’t return as long as it seems important enough.  I’d bet you could mail basically anything - a seat cushion, a balloon, a constantly-beeping speaker - if you permanently attached it to something that looks like a bill with no return address.&lt;/p&gt;

&lt;p&gt;The USPS’ experiments will continue, but I’d love to hear from anyone else who’s tried pushing the envelope of the postal system.  Please tell me of your experiments by writing a message on the exterior of a full-size replica of a blue public mailbox and mailing it to the Berkeley physics department.&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Jul 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/mail,%20random/2020/07/12/mail-shenanigans.html</link>
        <guid isPermaLink="true">http://localhost:4000/mail,%20random/2020/07/12/mail-shenanigans.html</guid>
        
        
        <category>mail, random</category>
        
      </item>
    
      <item>
        <title>Common ground</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;America’s polarization is increasingly dire, and with national focus perpetually on divisive issues, both liberals and conservatives often see the other as genuine enemies with whom cooperation’s impossible. There’s a severe lack of sincere dialogue between the left and the right, but that fact makes honest, humanizing conversations with the opposition more precious now than ever. To that end, and to prove that common ground could be found, I, quite liberal, messaged a friend who’s quite conservative for a discussion. Though we began firing off opposing views on divisive topics, with a little thought we found much unexpected agreement and eventually reached a reasonably shared view of pressing problems. Here’s an outline of everything we agreed on, a map of some oft-undiscussed common ground most arguing liberals and conservatives might not realize they both stand on. My hope’s that it might serve as a blueprint for others’ conversations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Police brutality is wrong and should be punished&lt;/li&gt;
  &lt;li&gt;Racism exists in 2020 and is a problem&lt;/li&gt;
  &lt;li&gt;From Hollywood to academia to boardrooms, there are many sectors of society in which whiteness is implicitly the default, and even in 2020 this often leads to minorities feeling like outsiders due to their race&lt;/li&gt;
  &lt;li&gt;There are many policies from redlining to the War on Drugs to school funding mechanisms that disproportionately hurt poor communities, which are often largely black or hispanic&lt;/li&gt;
  &lt;li&gt;Peaceful protesting is commendable and should never be met with violence&lt;/li&gt;
  &lt;li&gt;The word ‘racist’ has extreme connotations, and it’s usually more productive to call someone’s implicit racism ‘ignorance’ or ‘insensitivity’&lt;/li&gt;
  &lt;li&gt;Police obviously do some good, and we shouldn’t rush to abolish all police&lt;/li&gt;
  &lt;li&gt;The slogan ‘fuck the police’ is unproductive&lt;/li&gt;
  &lt;li&gt;Issues often get politicized when they don’t need to be&lt;/li&gt;
  &lt;li&gt;Be careful not to share fake news&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Donating to groups like Equal Opportunity Schools, Children of Promise, and Wings for Kids is a clear, apolitical way to make a difference&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Police brutality isn’t the doing of a small fraction of ‘bad cops’ among the ‘good cops’; what’re the odds four for four of George Floyd’s attackers happened to be ‘bad’? They’re questions of group psychology and police culture. Real solutions should involve changing training, increasing police accountability and oversight, reversing unnecessary police militarization, and resolving the noted problem of using police to address too wide a variety of needs. The four-for-four probability argument is one I feel liberals could make more clearly against the ‘few bad cops’ statement.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Media severely biases people’s perceptions of police and protester violence. I’d seen a headline about a pregnant woman hit in the stomach by a rubber bullet, while she’d seen a story about protesters who’d refused to let a car with a sick child through. Given that there were protests in every major city in the US, it’s no shock things like these happened, but because they’re emotionally potent, it’s easy to get riled up when a news outlet or FB algorithm shows you one. It’s more important to focus on broad trends and statistics when possible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Much of racial inequality is due to racist history and cycles of poverty as opposed to current racism. She thought the national focus should be on investing in poor black communities to fix problems at their roots instead of pinning guilt on police and current racism. While plenty of studies have shown proof of current racism and that fact should be broadly understood, and police abuse of power is prevalent and merits reform, I basically agree; police reform alone won’t fix racial inequality. The question of police blame has become divisive, with conservatives largely defending the police; how’d that happen when there are clearly problems with law enforcement? Maybe it’s because police brutality is more glaring and simpler to address than broad socioeconomic issues, so it’s an easy target for both reform efforts and anger, making it look like the dominant liberal view is that police are the main problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rioting and looting shouldn’t be excused. Our society considers crimes less severe if they’re done in the heat of passion, but they’re still crimes, and though the recent riots are perhaps more understandable because of their motivation and the emotion they’ve brought to the nation’s attention, they’re still highly damaging and often selfish. Excusing these completely is a minority view among liberals but still seems to cause a lot of understandable frustration among conservatives who see it as a mainstream view, which detracts from discussion of more important issues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Liberals can be very intolerant of conservatives. I’d noticed this but was shocked at the extent my friend described; in her words, “if I shared a [BLM] post but shared the portions I disagree with I’d be called out for not agreeing wholeheartedly, but if I’m silent I’m called out for being silent.” “I fear that putting a conservative sticker will almost definitely lead to vandalism on my car in Richmond.” “I know conservatives who have to put up with liberal rants all day at work and can’t say a thing or else they’ll lose their work relationships and promotion opportunities or even their job.” That sort of thing’s much less obvious from a liberal perspective, so I, for one, believe her. There’s definitely a perception among some liberals that some ideas are so clearly right that anyone with an objection or caveat is automatically morally inferior, with no need for further investigation, and though I agree with most of said ideas, that attitude’s a problem and being aware of it might help give liberals some understanding of conservatives’ perspectives.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When a prominent leader makes a comment that’s widely interpreted as racist, if it wasn’t meant that way, that leader should be clear in explaining the intended meaning and disavowing the racist interpretation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;It’s worth listening to each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When factions face off, spears bristling, perhaps the boldest, most important place to stand is in the middle with hands extended to either side. Polarization in the US is as real a crisis as the coronavirus, but unlike the virus, you have significant power to fight it directly.&lt;/p&gt;

</description>
        <pubDate>Sat, 04 Jul 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/society,%20random/2020/07/04/common-ground.html</link>
        <guid isPermaLink="true">http://localhost:4000/society,%20random/2020/07/04/common-ground.html</guid>
        
        
        <category>society, random</category>
        
      </item>
    
  </channel>
</rss>
