<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JS</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 20 Jul 2025 19:10:34 -0800</pubDate>
    <lastBuildDate>Sun, 20 Jul 2025 19:10:34 -0800</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>On the scientific method and its application to the science of deep learning</title>
        <description>&lt;p&gt;I have recently found myself involved in many discussions centered on the question of how we might develop a satisfactory science of modern machine learning. It certainly seems likely that such a science should be possible, as every important human feat of engineering has eventually admitted an explanatory science. Given such a science, engineering artifacts are revealed as points in a space of viable constructions: with an understanding of mechanics and materials science, we see the Taj Mahal and the Colosseum are two points in the space of stable freestanding structures made of stone, and we can describe the boundaries of this set. It is of course one of the great scientific questions of our time whether current foundation models and their inevitable successors may be located within such a scientific framework.&lt;/p&gt;

&lt;p&gt;While there is widespread support for work developing the “science of deep learning,” there is little consensus as to what this science will look like or what constitutes meaningful progress. Much of this disarray is of course inevitable and healthy: deep learning is complex, complementary approaches will be necessary, and we do not know what, exactly, we are looking for. Much of the confusion, however, is evitable and unhelpful: even when searching for an unknown object, it helps to search methodically. This essay is a discussion of the method of search.&lt;/p&gt;

&lt;p&gt;It appears to me that the present disorder lies mostly downstream from confusion about some basic questions: what is science, and how do you do it? Actually, we don’t care about science merely because it is Science, but rather because it is a technique we may use, so what we are really asking is: how do you make useful sense of something mysterious in the world, and when the mystery is great, how do you go about making progress anyways? While these questions are basic, they are by no means easy. I would like to share some thoughts on these questions informed by my experience and my own process of trial and error.&lt;/p&gt;

&lt;p&gt;Many great thinkers of the last century have offered insightful discussions of the scientific mindset and process, and I strongly recommend sitting down with &lt;a href=&quot;https://en.wikipedia.org/wiki/Falsifiability&quot;&gt;Popper’s notion of falsifiability&lt;/a&gt;, &lt;a href=&quot;https://www.lri.fr/~mbl/Stanford/CS477/papers/Kuhn-SSR-2ndEd.pdf&quot;&gt;Kuhn’s depiction of the scientific process&lt;/a&gt;, &lt;a href=&quot;https://feynman.com/science/what-is-science/&quot;&gt;Feynman’s joyful empiricism&lt;/a&gt;, and &lt;a href=&quot;https://www.lesswrong.com/w/original-sequences&quot;&gt;Yudkowsky’s techniques for clear thinking&lt;/a&gt;. All have greatly shaped my own views, and I have little to say about the general process of science that one of them has not already said better. I would like to contribute just one idea concerning the so-called scientific method.&lt;/p&gt;

&lt;p&gt;We learn in grade school that the process of science follows a defined sequence of steps: observation, hypothesis, experiment (with at least three trials), analysis of data, and acceptance or rejection of the hypothesis. However, any practicing scientist knows that this is not really how science works. The anatomy of a scientific project usually bears little resemblance to this tidy storybook picture.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Useful projects take such a great diversity of forms that it can seem like anything at all goes. One might then fairly wonder: are there, after all, any truly essential steps, or is one approach as good as another? This is an important question for any field that hopes to make material progress towards understanding a great mystery.&lt;/p&gt;

&lt;p&gt;It seems to me that, yes, there are two essential steps to the scientific method. Step A is to figure something out. Step B is to check and make sure you’re not wrong. You can do these steps in any order, but you have to do them both.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/img/scientific_method/scientific_method.png&quot; width=&quot;60%&quot; /&gt;
  &lt;div&gt;
    &lt;small&gt;
      &lt;i&gt;&lt;b&gt;Figure 1:&lt;/b&gt; The scientific method.&lt;/i&gt;
    &lt;/small&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;All the good science I know does both of these steps. If you only do the first step — figuring something out but not adequately checking that you’re right — you’re doing philosophy, or theology, or another speculative practice. Such speculation can be beautiful and useful, but at the end of the day, it can rarely be built upon. If you only do the second thing — performing an empirical check but not figuring anything out — you’re usually doing engineering.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;There are no rules whatsoever as to how you do the first step. You are allowed to figure things out via educated guess, long experience, mathematical derivation, meditation, or divine inspiration. This is often where the ingenuity of the theorist enters into play. There is only one rule with the second step: you have to do a &lt;em&gt;good job&lt;/em&gt; checking you’re not wrong, ideally good enough to convince other people. This, too, can be done in many different ways — direct experiment, elimination of alternatives, checking new predictions — but it is absolutely essential that it is done adequately. It can be very difficult to do this well, or even to figure out how to do it! This is the nerve-wracking step where one makes sure that one isn’t fooling oneself. This is usually where the ingenuity of the experimentalist comes into play.&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Most of the poor science of which I am aware fails in one of these two steps. It is, of course, very common for a study to fail to adequately demonstrate its central claims: this is understandable, as true, interesting facts are quite difficult to find! On the other side, uninteresting, hypertechnical experiments (which have unfortunately become the norm in certain crowded areas of physics) perform their empirical checks just fine, but when the lab equipment is back in its boxes, it is unclear what general fact has been figured out. (Yep, our Rube Goldberg machine of diffraction gratings, modulators, cavities, and heavy atoms works as predicted! Anyone remember what it was all for?) I also see studies that claim to figure something out about a system of interest (e.g., deep learning), but which fail to state a specific enough conjecture to go and test it, which is a partial failure in both steps. (More on that later.)&lt;/p&gt;

&lt;p&gt;It is worth clarifying that a scientist does not have to do both steps of the scientific method in every scientific paper. An individual contribution may be entirely the proposal of a new idea, as in Darwin’s book or most of the &lt;a href=&quot;https://www.chemteam.info/Chem-History/Planck-1901/Planck-1901.html&quot;&gt;seminal&lt;/a&gt; &lt;a href=&quot;https://uni-tuebingen.de/fileadmin/Uni_Tuebingen/Fakultaeten/MathePhysik/Institute/IAP/Forschung/MOettel/Geburt_QM/bohr_PhilMag_26_1_1913.pdf&quot;&gt;theoretical&lt;/a&gt; &lt;a href=&quot;https://link.springer.com/article/10.1007/BF01397280&quot;&gt;physics&lt;/a&gt; &lt;a href=&quot;https://uni-tuebingen.de/fileadmin/Uni_Tuebingen/Fakultaeten/MathePhysik/Institute/IAP/Forschung/MOettel/Geburt_QM/schrodinger_AnnPhys_386_109_1926.pdf&quot;&gt;papers&lt;/a&gt; &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/andp.19053220806&quot;&gt;of the early 1900s&lt;/a&gt;, or it may consist entirely of measurements, as in Tycho Brahe’s meticulous observations of the planets or &lt;a href=&quot;https://www.nature.com/articles/s41586-020-2964-7&quot;&gt;careful modern measurements of fundamental physical constants&lt;/a&gt;. It is quite legitimate for a contribution to &lt;em&gt;dis&lt;/em&gt;prove an existing belief without offering a replacement. The important thing is that a scientist doing one or the other recognizes that they are part of a conversation with the other part: the person proposing the ideas expects other people to go and test them and so tries to make it easy for them to do so, and the people checking the ideas know what the implications are if the experiment comes out one way or the other. Every scientific contribution should understand itself as part of a project that &lt;em&gt;does&lt;/em&gt; do both steps of the scientific method.&lt;/p&gt;

&lt;p&gt;Why are both of these steps necessary for the progress of science? If a line of research does not purport to have figured out any general fact, it is unclear what has been learned or how to build on it. On the other hand, if it does not adequately check its claims, then until it does, it will carry a shadow of fundamental doubt that prevents others from productively building on it. Furthermore, and often more significantly in practice, the act of checking usually contains within it the act of application, as the most convincing way to check a claim is often to operationalize it to do something of interest. Science is an edifice that builds on itself. It usually consists of so many layers that each piece of brickwork must be quite solid to support future building, and each brick must be crafted with future bricks in mind.&lt;/p&gt;

&lt;p&gt;If you really believe me that the essential scientific method has only two steps, you might ask: what’s all this other mumbo jumbo about preregistered hypotheses, repeated trials, cognitive biases and what not? There are a few things going on. First off, while these two steps are simple, actually doing them is hard, so we have some established techniques that sometimes make them easier. Some of these help with the figuring out, but that’s mostly a dark art.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; Most of these are techniques for the &lt;em&gt;checking&lt;/em&gt; of our knowledge: preregistered hypotheses, multiple trials, the inclusion of error bars, blind review, and most of the other rituals of science are &lt;em&gt;ways to do step B.&lt;/em&gt; None is essential, but they are useful techniques for checking our answer and avoiding fooling ourselves.&lt;/p&gt;

&lt;p&gt;Secondly, some of these techniques are field-specific. For example, in psychology, social science, and nutrition, it’s notoriously tempting to choose one’s hypothesis after seeing the data. In these fields, the space of hypotheses is usually large and the resolving power of evidence is usually weak, so choosing the most-supported claim post-hoc often amounts to &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_dredging&quot;&gt;p-hacking&lt;/a&gt;, and preregistering hypotheses makes it more likely to not be totally wrong. In (non-string-theory) physics, the situation is usually the opposite — hypotheses are few and evidence is abundant — so preregistration isn’t necessary.&lt;/p&gt;

&lt;p&gt;These steps apply even in a nascent field that knows very little. While no method is sufficient to guarantee useful progress in the search, failure to do both steps all but guarantees no progress will be made. By way of analogy, when searching a large unfamiliar house for a desired object, one’s chances of success are greatly improved by a methodical search that progressively expands an explored volume. It is possible to learn that &lt;em&gt;it’s not in this cabinet,&lt;/em&gt; say, but only if one first identifies the cabinet as a useful unit of exploration, then does a sufficiently thorough search that the cabinet does not need to be revisited in the future. Failures &lt;em&gt;are&lt;/em&gt; progress, but &lt;em&gt;only&lt;/em&gt; when they are sufficiently clear and careful so as to reassure other searchers that the space of possible truths is meaningfully reduced.&lt;/p&gt;

&lt;p&gt;In summary, the scientific method consists of two steps: you must figure something out, and you must adequately check that you are not mistaken. It seems to me that these are the two things we must demand of any useful scientific project.&lt;/p&gt;

&lt;h3 id=&quot;how-to-recognize-the-scientific-method-in-the-science-of-deep-learning&quot;&gt;How to recognize the scientific method in the science of deep learning&lt;/h3&gt;

&lt;p&gt;Deep learning presents a large number of important mysteries. What exactly we believe these mysteries &lt;em&gt;are&lt;/em&gt; has evolved over time and will continue to do so, but all present agree that the mysteries are there. Certainly it seems that the practice of deep learning involves far more arbitrary choices and yields far more surprising results than it ought to if we knew better. There is thus probably much ground to gain. Because the success of deep learning is an empirical phenomenon and we wish to explain it, this is very much a scientific question, and we will be wise to consciously use the methods of science to structure our search.&lt;/p&gt;

&lt;p&gt;We are gradually learning to do genuine science in the study of deep learning, and the rewards have been proportionate. However, a great deal of research effort ostensibly in service of our understanding of deep learning is expended in directions which are quite far from science and which consequently make little real progress. This is not a personal vendetta of mine: I have found this to be the consensus of virtually every researcher in the field with whom I have discussed the subject. In fact, this number includes many interviewees who have described their &lt;em&gt;own&lt;/em&gt; work to me as of this ineffectual type, usually with a palpable air of despondence! (By contrast, the deep learning researchers I know who have caught the “science bug” tend to be energized and optimistic.) We as a field are due for a serious discussion of our methods and search strategy, and we can be optimistic that effectual methods are quite achievable.&lt;/p&gt;

&lt;p&gt;How can we recognize the scientific method in the study of deep learning? We should look for work which (a) purports to figure out something particular and clear about deep learning, and (b) reports simple, convincing experiments that compellingly support it. The things figured out can really take any form so long as they are clearly stated and seem useful. They may be empirical (”A causes B”; “C phenomenon reliably happens”), mathematical (equations, limits, new mathematical objects), or even metascientific (e.g., “D is a useful proxy model for deep learning”). Qualitative claims are fine, but quantitative claims are best, because they may be verified with great confidence and may usually be applied in a large number of cases. Qualitative claims are rarely verified reliably (and often fold under later scrutiny) because of the sheer number of possible causes in a system as complex as deep learning. In assessing the progress of deep learning, we should count the number of interesting, easily verifiable quantitative claims one can make about deep learning systems, and individual researchers should seek to add to this count. This is how we will mark our progress in our search.&lt;/p&gt;

&lt;p&gt;So, how does most “science of deep learning” work do on this minimal rubric of scientific method? By way of illustration, let me run through some of the major research trends in the last five years of deep learning theory. I will start with some failings before ending with some victories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;On formality and rigor to the detriment of insight.&lt;/strong&gt; It is an item of little controversy (at least when discussing off the record) that a great number of deep learning theory papers are impressively rigorous and mathematically complex, but ultimately shed little to no light on the mystery originally motivating the endeavor. Papers of this sort tend to share certain features: asymptotic notation obscures large hidden constants; theorem statements require significant parsing in order to extract the essence of the result; the problem setup introduces complexity for the sake of a more impressive result rather than simplifying the setup for clarity and insight; few or no experiments are reported, and certainly none with nonlinear networks. Any seasoned deep learning theorist has read numerous such papers.&lt;/p&gt;

&lt;p&gt;It seems to me that this pattern is the result of mistaking the scientific study of deep learning for a discipline of mathematics, which then requires formality and rigor. It emphatically is not: we are faced here with great and glaring empirical mysteries, and experiments are cheap and easy.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; It seems virtually guaranteed that we will first understand deep learning through quick-and-dirty nonrigorous arguments which may later be formalized, as a path through the woods is first blazed and then only later paved over in asphalt. Formality and rigor are a hindrance if they make it harder to understand the real nature of what you have figured out. As the saying goes, all theorems are true, but only some are interesting, and taking stock of the present state of our knowledge, it is greatly preferable to have an interesting nonrigorous result than an uninteresting theorem.&lt;/p&gt;

&lt;p&gt;Papers of this mathematical sort rarely include experiments: sometimes there are experiments tacked on at the end, but they are usually an afterthought. If the study of deep learning were mathematics, this would be understandable, as a proven theorem requires no empirical demonstration. Because the study of deep learning is a science, however, neglecting experiments is utter folly. Unless the proven theorem totally resolves an important question in a completely realistic setting, experiments can extend a result’s scope of applicability, show its limits, check assumptions, or simply make it easier to understand the stated claim. If our goal is to understand deep learning, it sure seems wise to check empirically whether whatever you suppose you have figured out &lt;em&gt;applies to deep learning!&lt;/em&gt; If it doesn’t, or the fit is worse than you expected, this merits explanation. If it does, then the contribution is all the greater.&lt;/p&gt;

&lt;p&gt;The overly mathematical nature of much deep learning theory research is natural and understandable given the field’s history: most workers come from TCS, statistics, or mathematics. Nonetheless, the game has changed, and we should require less rigor, more insight, and more empirics from contributions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Progress in the study of the dynamics of neural network training&lt;/strong&gt; It seems to me that much of the lasting progress of the last five years — the stuff that’s really stuck around and been built on — is essentially all of a particular type which marries theory and empirics. This strain of research is characterized by several trends:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It demands satisfying equations and quantitative predictions, and it is willing to study very simple cases and study dumb-seeming quantities in order to make this happen.&lt;/li&gt;
  &lt;li&gt;While the equations do not usually come &lt;em&gt;from&lt;/em&gt; experiments, they are easily verified &lt;em&gt;by&lt;/em&gt; experiments.&lt;/li&gt;
  &lt;li&gt;Assumptions are checked empirically. Assumptions that are both good and useful are celebrated and kept around.&lt;/li&gt;
  &lt;li&gt;It is humble: it studies only what it can describe well, and does not make premature claims about downstream topics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Almost all of the work in this vein describes the &lt;em&gt;dynamics&lt;/em&gt; of neural network learning rather than the &lt;em&gt;performance,&lt;/em&gt; which is what I mean by humility. Some touchstone topics in this vein include the theories of &lt;a href=&quot;https://arxiv.org/abs/1312.6120&quot;&gt;deep linear networks&lt;/a&gt;, the &lt;a href=&quot;https://arxiv.org/abs/1711.00165&quot;&gt;NNGP&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1806.07572&quot;&gt;neural tangent kernel&lt;/a&gt;, &lt;a href=&quot;https://proceedings.mlr.press/v139/yang21c.html&quot;&gt;the maximal update parameterization&lt;/a&gt;, the &lt;a href=&quot;https://arxiv.org/abs/2103.00065&quot;&gt;edge of stability&lt;/a&gt;, the &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;generalization of kernel ridge regression&lt;/a&gt;, and the study of hyperparameter scaling relationships (see, e.g., &lt;a href=&quot;https://arxiv.org/abs/2309.16620&quot;&gt;here&lt;/a&gt;). All these ideas presented new mathematical quantities derived from model training which closely follow simple equations. All have proven solid enough to build further understanding on top of.&lt;/p&gt;

&lt;p&gt;This variety of deep learning theory does not yet have an accepted name. We should give it one. Because of its similarities to the physical sciences, the “physics of deep learning” is a candidate term, though this carries historical baggage, has already used to describe many things including &lt;a href=&quot;https://arxiv.org/abs/2305.13673&quot;&gt;other research directions&lt;/a&gt;, and risks alientating the mathematicians and statisticians who have contributed to this productive type of work and will do so in the future. “Analytical interpretability” may be a good term for this, conveying both the high bar of analytical results with the promise of interpreting the training process of deep learning. Plus, it abbreviates to “AI.”&lt;sup id=&quot;fnref:b&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:b&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; Since this line of work includes virtually all extant examples of quantitatively predictive theories for deep learning, I like to think of it as “experiment-dots-on-theory-curves” or “dots-on-curves” theory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What about empirical science?&lt;/strong&gt;
The above discussion is centered largely on deep learning theory. The science of deep learning is a rather greater endeavor, and has had some successes. Mechanistic interpretability cannot do what the physics of deep learning could (and vice versa; both are necessary), but it has been quite admirable as a scientific endeavor: it has made good use of the scientific method to coordinate a large-scale search over a difficult space. Deep learning theory should learn from it.&lt;/p&gt;

&lt;p&gt;On the even more empirical side, phenomena like adversarial examples and the lottery-ticket hypothesis were excellent empirical observations, though much of the followup work on these topics makes less use of the scientific method and has accreted into less lasting knowledge. The observation of scaling laws in neural network performance is perhaps the one extant example of a robust and important equation extracted purely from neural network empirics. This was an excellent observation, and it remains unexplained.&lt;/p&gt;

&lt;p&gt;Most “observations” in deep learning are of the type that “X method works for Y task.” Much fruitful dialog could be had between deep learning scientists and practitioners if the practitioners were more proactive in aggregating interesting phenomena and handing them to the scientists, and likewise if the scientists were more proactive in asking for them. Of course, most practitioners are too laser-focused on building AGI to care about theory, so I am dubious this will happen.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Hail Maries”&lt;/strong&gt;
Lastly, there have been a few ideas proffered of a type that I’ll call “hail Maries” after the long-bomb football pass.
These ideas try to take a big step forwards all in one step with a very good guess: they tend to be summarizable by statements of the form “hey, what if deep learning is actually just X?”
A good example of this is the much-embattled &lt;a href=&quot;https://arxiv.org/abs/1503.02406&quot;&gt;“information bottleneck” theory&lt;/a&gt;.
Even though the IB was conclusively disproven soon after its proposal, I strongly applaud the bold, testable hypothesis and honest attempt to figure something out.
Attempts to jump ahead in the story like this are likely to be wrong, but they are very much permitted in science.
Much of the development of quantum mechanics consisted of bold, unprecendent guesses!
Remember that there are no rules as to how one must figure something out: intuition-guided guesswork is quite allowed.
In our field, there are few ideas and much energy available to test them, so I would like to see more bold guesses of this type.
We should expect to see more such leaps as time goes on, and some of them will turn out to be right.&lt;/p&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;What now? We are making steady progress towards a theory of deep learning, a theory that we presumably hope to bend to the benefit of humankind. It has been over a decade since AlexNet, and we have tried much. Most of this has failed, but some of it has succeeded. It is a good time now to step back, notice the patterns in our successes, reassess our strategies, and seriously refocus our effort. Let’s get moving.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;To list some deviations I have seen firsthand: sometimes the hypothesis changes dramatically, or only becomes clear at the end, or is absent altogether. Sometimes a single trial suffices, and sometimes one needs millions. Sometimes the hypothesis and conclusions are obvious and the data gathering is the whole project. Sometimes the conclusion has little to do with the initial aims of the project. I have seen very few scientific projects follow the script of the “science fair scientific method,” and these few usually turned out poorly! For example, I’ve rarely seen an interesting hypothesis confirmed by experiment when the scientists weren’t already damn near sure it was going to be true. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Of course, the creator of any engineered artifact can rightly claim to have “figured out” that such an artifact is possible. Sometimes this is quite an interesting discovery! The boundaries between engineering and science are not clear, and we do not need them to be. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If I were to add a third step, it would be convincing other people. The community is the ultimate judge of whether you have figured something out and whether your experiments show that it is not wrong. A colleague points out that this is similar to Dorothy Sayer’s &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Mind_of_the_Maker&quot;&gt;third step of the creative process&lt;/a&gt;: sharing your work with the world and thereby having an effect on other people. To me, peer review feels like an important part of the scientific process, but feels secondary to the scientific &lt;em&gt;method&lt;/em&gt; – even alone on a desert island, you could do science as I describe it here – and in any case, it’s not like anyone is making important scientific progress and &lt;em&gt;not&lt;/em&gt; sharing it, so I feel comfortable omitting it. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yudkowsky attempts to make this mysterious process more mechanical in some of the Sequences, but it’s still quite difficult to come up with hypotheses. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It is a very interesting question, perhaps worthy of discussion elsewhere, what the merits of rigor and formality are in an exploratory endeavor. Most obviously, a proven theorem is always correct and will not fail unexpectedly, so all else equal, a theorem is preferable to an equivalent nonrigorous claim. When, though, does the extra solidity justify the price in labor? It seems to me that rigor and formality are most useful when the class of objects one wishes to describe is very large and of unknown character, and bizarre or pathological cases are prevalent and important. For example, the space of groups is very large and diverse, and so without axioms to work from, we are lost. Similarly, real analysis requires formality because it turns out the set of all univariate functions on the reals is far stranger than expected, and we cannot rely on our intuitions. On the other hand, when one already has an intuitive feel for the set of objects one wishes to characterize, the guardrails of formality are not so necessary. It is for this reason that you need very little formal math to do physics. This is very much the case in which we find ourselves with deep learning. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:b&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Hat tip to Alex Atanasov for coining the term. &lt;a href=&quot;#fnref:b&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 20 Jul 2025 02:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/on-the-scientific-method/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/on-the-scientific-method/</guid>
        
        
        <category>research</category>
        
      </item>
    
      <item>
        <title>Backsolving classical generalization bounds from the modern kernel regression eigenframework</title>
        <description>&lt;p&gt;&lt;em&gt;This blogpost shows how to use the omniscient KRR eigenframework, which gives a closed-form expression for the test error of KRR in terms of task eigenstructure, to quickly recover a classical generalization bound. I’ve worked with the modern eigenframework a great deal, and this blogpost was the result of a calculation I thought was new but quickly resolved to a classical bound. Even though the old classical bounds have proven fairly useless in modern problems, I still found this interesting: the calculation here presented is a different (and fast) way to arrive at a classical result, and it was clarifying for me to see where the looseness of the classical result slips in in the course of various approximations.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We will take as starting point the KRR eigenframework derived in many recent papers. We will use the notation of &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;our KRR eigenpaper&lt;/a&gt; with little further explanation.&lt;/p&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;We have a target function $f_*(\cdot)$ and a kernel function $K(\cdot,\cdot)$. We know the target function’s RKHS norm $\lvert \! \lvert f_\ast \rvert \! \rvert_K$. We will perform KRR with $n$ samples from an arbitrary measure $\mu$, and we would like to obtain a bound on the test error at optimal ridge&lt;/p&gt;

\[\text{[test MSE]}|_{\delta = \delta_*} \equiv \mathbb{E}_{x \sim \mu}[(f_*(x) - \hat{f}(x))^2].\]

&lt;p&gt;We know $\max_{x \sim \mu} K(x,x)$, where the max runs over the support of $\mu$, or at least we know an upper bound for this quantity.&lt;/p&gt;

&lt;h3 id=&quot;rewriting-known-quantities-in-the-kernel-eigensystem&quot;&gt;Rewriting known quantities in the kernel eigensystem&lt;/h3&gt;

&lt;p&gt;There exists a kernel eigendecomposition $K(x,x’) = \sum_i \lambda_i \phi_i(x) \phi_i(x’)$ which is orthonormal w.r.t. $\mu$, though we will not need to assume that we can find it. There also exists a decomposition $f_*(x) = \sum_i v_i \phi_i(x)$, though we again do not assume we can find it.&lt;/p&gt;

&lt;p&gt;It holds that&lt;/p&gt;

\[|\!| f_* |\!|_K^2 = \sum_i \frac{v_i^2}{\lambda_i}.\]

&lt;p&gt;Note that this is true regardless of the measure $\mu$ — see my note on measure-independent properties of kernel eigensystems.&lt;/p&gt;

&lt;h3 id=&quot;using-the-kernel-eigenframework&quot;&gt;Using the kernel eigenframework&lt;/h3&gt;

&lt;p&gt;The effective ridge $\kappa &amp;gt; 0$ is the unique positive solution to&lt;/p&gt;

\[n = \sum_i \mathcal{L}_i \ + \ \frac{\delta}{\kappa},\]

&lt;p&gt;where $\mathcal{L}_i := \frac{\lambda_i}{\lambda_i + \kappa} \in [0,1]$ is the “learnability” of mode $i$. We will go head and choose $\delta$ such that $\sum_i \mathcal{L}_i = \frac{n}{2}$, which will later give us the best constant prefactor in our bound.&lt;/p&gt;

&lt;p&gt;The eigenframework states that&lt;/p&gt;

\[\text{[test MSE]} \approx \mathcal{E} := \frac{n}{n - \sum_i \mathcal{L}_i^2} \sum_i (1 - \mathcal{L}_i)^2 v_i^2.\]

&lt;p&gt;We may now note three useful bounds. First, we may bound the prefactor above as&lt;/p&gt;

\[\frac{n}{n - \sum_i \mathcal{L}_i^2} \le \frac{n}{n - \sum_i \mathcal{L}_i} = 2,\]

&lt;p&gt;where in the last step we have applied our choice of $\delta$. Second, we may bound the sum as&lt;/p&gt;

\[\sum_i (1 - \mathcal{L}_i)^2 v_i^2 
\ \le \ \sum_i (1 - \mathcal{L}_i) v_i^2
\ = \ \sum_i \frac{v_i^2}{\lambda_i} \frac{\kappa \lambda_i}{\lambda_i + \kappa}
\ \le \ \kappa \cdot \sum_i \frac{v_i^2}{\lambda_i} \le \kappa \cdot |\!| f_* |\!|_K^2.\]

&lt;p&gt;Finally, noting that $\sum_i \frac{\lambda_i}{\kappa} \ge \sum_i \mathcal{L}_i = \frac{n}{2}$, we may bound the constant $\kappa$ as&lt;/p&gt;

\[\kappa &amp;lt; \frac{2}{n} \sum_i \lambda_i = \frac{2 \cdot \text{Tr}[K]}{n} = \frac{2 \cdot \mathbb{E}_{x \sim \mu}[K(x,x)]}{n} \le \frac{2 \cdot \max_{x \sim \mu}K(x,x)}{n}.\]

&lt;p&gt;Putting the above four equations together (and also using the fact that the test MSE at optimal ridge will be less than or equal to the test MSE at our chosen ridge), we find that&lt;/p&gt;

\[\boxed{\mathcal{E}|_{\delta = \delta_*} \le \frac{4 \cdot \max_{x \sim \mu} K(x,x)}{n} \cdot |\!| f_* |\!|_K^2.}\]

&lt;p&gt;This looks like a classical generalization bound! It seems to me like this sort of classical bound often doesn’t have one canonical statement but instead gets written many different ways (which I suppose makes sense if your equation is a fairly loose bound that you could tighten up in some places and loosen in others), and I couldn’t find an exact statement of precisely this form anywhere, so I suppose I’m not actually sure this is equivalent to a classical result (though ChatGPT assures me it is), but it certainly looks like lots of classical bounds I’ve seen.&lt;/p&gt;

&lt;p&gt;If we use the fact that $\lvert ! \lvert f_\ast \rvert ! \rvert_{L^2(\mu)}^2 = \sum_i v_i^2$, we can obtain the (to me more illuminating) result that&lt;/p&gt;

\[\frac{\mathcal{E}|_{\delta = \delta_*}}{\mathcal{E}|_{\delta \rightarrow \infty}} \le \frac{4 \cdot \max_{x \sim \mu} K(x,x)}{n} \cdot \frac{|\!| f_* |\!|_K^2}{|\!| f_* |\!|_{L^2(\mu)}^2}.\]

&lt;p&gt;That is, the ratio of test error with $n$ samples to the naive test error of the zero predictor is controlled by the ratio of the RKHS norm of the target function to the $L^2$ norm of the target function.&lt;/p&gt;
</description>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/backsolving-classical-bounds/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/backsolving-classical-bounds/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>One kernel, many eigensystems</title>
        <description>&lt;p&gt;&lt;em&gt;This is a technical note on a subtlety of kernel theory. Venture in at your own risk!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Many problems in the theory of kernel methods begin with a positive semi-definite kernel function $K(\cdot, \cdot)$ and a probability measure $\mu$ and are solved by first finding the kernel eigenfunctions ${\phi_i}$ and nonnegative eigenvalues ${\lambda_i}$ such that&lt;/p&gt;

\[\ \ \ \qquad\qquad \qquad\qquad K(x,x') = \sum_i \lambda_i \, \phi_i(x) \, \phi_i(x') \qquad\qquad \qquad\qquad (1)\]

&lt;p&gt;and&lt;/p&gt;

\[\qquad\qquad \qquad\qquad\quad \ \ \ \mathbb{E}_{x \sim \mu}[\phi_i(x) \, \phi_j(x')] = \delta_{ij}. \qquad\qquad \qquad\qquad\quad (2)\]

&lt;p&gt;Equation 1 gives the spectral decomposition of the kernel, and Equation 2 states that the eigenfunctions are orthonormal with respect to the measure $\mu$.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;https://en.wikipedia.org/wiki/Mercer%27s_theorem&quot;&gt;Mercer’s Theorem&lt;/a&gt;, there always exists such a decomposition, though it will generally be different for different measures $\mu$. In fact, there are infinitely many unique decompositions of the form $K(x,x’) = \sum_i \psi_i(x) \, \psi_i(x’)$, but only one of them (up to relabeling and some other symmetries) is “eigen” for a particular measure $\mu$. It is somewhat intriguing to note that each such decomposition is still equally &lt;em&gt;correct&lt;/em&gt; no matter the measure (the first equation having no dependence on $\mu$).&lt;/p&gt;

&lt;p&gt;Given that every measure $\mu$ corresponds to a unique eigendecomposition, it is worth asking the inverse question: which kernel decompositions are “eigen” for some measure? That is, given a “candidate” eigensystem ${(\lambda_i, \phi_i)}$ such that $K(x,x’) = \sum_i \lambda_i \, \phi_i(x) \, \phi_i(x’)$, when does there exist a measure $\mu$ such that $\mathbb{E}_{x \sim \mu}[\phi_i(x) \, \phi_j(x’)] = \delta_{ij}$, and can we find the measure?&lt;/p&gt;

&lt;h2 id=&quot;functional-rightarrow-vector-language&quot;&gt;Functional $\rightarrow$ vector language&lt;/h2&gt;

&lt;p&gt;To answer this question, we will find it helpful to first move from a continuous setting to a discrete setting, as we may then forgo functional analysis for the simpler language of linear algebra. Let us suppose our measure has support only over $n$ discrete points ${x_i}_{i=1}^n$, and let us construct the kernel matrix $[\mathbf{K}]_{ij} = K(x_i, x_j)$, measure matrix $\mathbf{M} = \mathrm{diag}(\mu(x_1), \ldots, \mu(x_n))$, and candidate eigenvectors $\mathbf{v}_i = (\phi_i(x_1), \ldots, \phi_i(x_n))$. In this linear algebraic language, we begin with the assurance that&lt;/p&gt;

\[\mathbf{K} = \sum_i \lambda_i \mathbf{v}_i \mathbf{v}_i^\top.\]

&lt;p&gt;We would like to determine necessary and sufficient conditions on ${\lambda_i, \mathbf{v}_i}$ such that there exists a positive-definite&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; diagonal $\mathbf{M}$ such that&lt;/p&gt;

\[\mathbf{v}_i^{\top} \mathbf{M} \mathbf{v}_j = \delta_{ij}\]

&lt;p&gt;and $\mathrm{Tr}[\mathbf{M}] = 1$.
We begin by stacking our candidate vectors into a matrix $\mathbf{V} := (\mathbf{v}_1, \ldots, \mathbf{v}_n)$ and likewise defining $\boldsymbol{\Lambda} := \mathrm{diag}(\lambda_1, \ldots, \lambda_n)$. Note that all the matrices we have defined are invertible. We now have that&lt;/p&gt;

\[\mathbf{K} = \mathbf{V \Lambda V^\top}, \qquad \mathbf{V^\top M V} = \mathbf{I}.\]

&lt;p&gt;From the latter equation, we get that $\mathbf{M V V^\top} = \mathbf{I}$, and thus $\mathbf{K M V V^\top} = \mathbf{V \Lambda V^\top}$, and thus we obtain the eigenequation&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

\[\mathbf{K M V} = \mathbf{V \Lambda}.\]

&lt;p&gt;We now find that $\mathbf{M V} = \mathbf{K}^{-1}\mathbf{V \Lambda}$, and thus that $\mathbf{M v}_i = \lambda_i \mathbf{K}^{-1} \mathbf{v}_i$ for each eigenindex $i$. We are now in a position to make several observations.&lt;/p&gt;

&lt;p&gt;First, since $\mathbf{v}^\top_i \mathbf{M v}_j = \lambda_i \mathbf{v}^\top_i \mathbf{K}^{-1} \mathbf{v}_j = \delta_{ij}$, we find the requirement on our candidate eigensystem that $\lambda_i = \left( \mathbf{v}^\top_i \mathbf{K}^{-1} \mathbf{v}_i \right)^{-1}$. This is best viewed as a normalization condition on the eigenvector: it is required that&lt;/p&gt;

\[\boxed{

|\!| \mathbf{v}_i |\!| = \left( \lambda_i \hat{\mathbf{v}}_i^\top \mathbf{K}^{-1} \hat{\mathbf{v}}_i  \right)^{-1/2}

}.\]

&lt;p&gt;Second, since $\mathbf{M}$ is positive and diagonal, we find that&lt;/p&gt;

\[\mathbf{M v}_i = \mathbf{\mu} \circ \mathbf{v}_i = \lambda_i \mathbf{K}^{-1} \mathbf{v}_i\]

&lt;p&gt;where $\circ$ denotes elementwise multiplication of vectors. Therefore a single candidate eigenpair $(\lambda_1, \mathbf{v}_1)$ is potentially valid only if there exists a positive vector $\mathbf{\mu}$ verifying the above equation — that is, if&lt;/p&gt;

\[\boxed{
m_{ik} := \frac{\lambda_i (\mathbf{K}^{-1} \mathbf{v}_i)[k]}{\mathbf{v}_i[k]} &amp;gt; 0
\qquad \text{if} \ \  \mathbf{v}_i[k] \neq 0,}\]

&lt;p&gt;where we write $\mathbf{u}[k]$ denote the $k$-th element of a vector. We must additionally have that the different eigenvectors verify the same measure — that is,&lt;/p&gt;

\[\boxed{
m_{ik} = m_{ik'} := \mu_i
\qquad
\forall
\ \
k, k'
}\]

&lt;p&gt;except where one of the two is undefined. Finally, we must have that the measure we obtain is normalized:&lt;/p&gt;

\[\boxed{
\sum_i
\mu_i = 1.
}\]

&lt;p&gt;Together, the boxed equations are necessary and sufficient conditions for the existence of a positive measure $\mathbf{\mu}$ such that ${(\lambda_i, \mathbf{v}_i)}$ indeed comprise an orthogonal eigenbasis with respect to $\mathbf{\mu}$. Interestingly, it follows from the above with no additional requirements that&lt;/p&gt;

\[\mathbf{v}_i^\top \mathbf{K}^{-1} \mathbf{v}_j = 0 \qquad \mathrm{if} \ i \neq j.\]

&lt;p&gt;That is, orthogonality with respect to the measure implies orthogonality with respect to $\mathbf{K}^{-1}$.&lt;/p&gt;

&lt;h2 id=&quot;back-to-functional-language&quot;&gt;Back to functional language&lt;/h2&gt;

&lt;p&gt;To phrase these results in functional language, we need to define the &lt;em&gt;inverse kernel operator.&lt;/em&gt; Define the kernel operator $T_K[g](\cdot) = \int K(\cdot, x) g(x) dx$. Let us assume this operator is invertible and construct the inverse $T^{-1}_K$.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; In functional language, we require that&lt;/p&gt;

\[\frac{\lambda_i \, T_K^{-1} [\phi_i](x)}{\phi_i(x)} = \frac{\lambda_j \, T_K^{-1} [\phi_j](x)}{\phi_j(x)} &amp;gt; 0
\qquad \forall \ i, j, x,\]

&lt;p&gt;except when either ratio is $\frac{0}{0}$. When this condition holds, the equated quantity is then equal to the measure $\mu(x)$ w.r.t. which ${\phi_i}$ are orthogonal.
Again, a single candidate eigenfunction $\phi_i$ determines the whole measure $\mu$ except where $\phi_i(x) = 0$.
We also of course require that the measure we find from the above is normalized when integrated over the full domain.&lt;/p&gt;

&lt;p&gt;We also find the (somewhat more inscrutable to me) condition that&lt;/p&gt;

\[\lambda_i = \left(\int \phi_i(x) \, T_K^{-1} [\phi_i](x) dx\right)^{-1},\]

&lt;p&gt;which again may be treated as a normalization condition on the eigenfunctions \(\phi_i\).&lt;/p&gt;

&lt;h2 id=&quot;connection-to-the-rkhs-inner-product&quot;&gt;Connection to the RKHS inner product&lt;/h2&gt;

&lt;p&gt;The usual reproducing kernel Hilbert space (RKHS) inner product is given by&lt;/p&gt;

\[\langle g, h \rangle_K := \int g(x) \, T_K^{-1}[h](x) dx.\]

&lt;p&gt;In our vectorized setting we found that $\mathbf{v}_i^\top \mathbf{K}^{-1} \mathbf{v}_j = 0$ when $i \neq j$, and so we find in the functional setting that &lt;em&gt;for any kernel eigenfunctions w.r.t. any measure, it will hold that&lt;/em&gt; $\langle \phi_i, \phi_j \rangle_K = 0$ when $i \neq j$. That is, no matter which measure you choose, the eigenfunctions which diagonalize the kernel operator will be orthogonal w.r.t. the RKHS. This is remarkable because the RKHS norm does not depend at all on the measure which was chosen! I tend to avoid using RKHSs whenever possible, but this is a nice property.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Thanks to Dhruva Karkada for raising the question that led to this blogpost.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Astute readers might wonder why we do not also need to take as input the ”eigenequation” that $\mathbb{E}_{x \sim \mu}[K(x, x’) \, \phi_j(x’)] = \lambda_i \, \phi_i(x).$ Somewhat surprisingly (to me), this can actually be deduced from Equations 1 and 2, as we will discuss after vectorizing. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Since we have assumed that all eigenvalues are nonnegative, we are assured that $\mathbf{M}$ will be positive &lt;em&gt;definite&lt;/em&gt; instead of merely PSD. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This manipulation answers the question raised by Footnote 1: even with a nontrivial measure, the “right eigenequation” follows from the “kernel-compositional” and “measure-orthogonal” equations. We can in fact obtain any one of these equations from the other two. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;To gain some intuition for these operators, note that if $K(\cdot, \cdot)$ is a Gaussian kernel, then $T_K$ performs Gaussian smoothing, while $T_K^{-1}$ performs “unsmoothing.” &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 07 Apr 2025 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/one-kernel-many-eigensystems/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/one-kernel-many-eigensystems/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>A complete characterization of the expressivity of shallow, bias-free ReLU networks</title>
        <description>&lt;p&gt;In the theoretical study of neural networks, one of the simplest questions one can ask is that of &lt;em&gt;expressivity.&lt;/em&gt; A neural network $f_{\boldsymbol{\theta}}(\cdot)$ can represent (or “express”) many different functions, and which function is expressed is determined by the choice of the parameters $\boldsymbol{\theta}$. The study of expressivity simply asks: which class of functions may be represented?&lt;/p&gt;

&lt;p&gt;Among the pantheon of ML algorithms, neural networks are famous for their high expressivity. This fact is requisite to their power and versatility: practitioners find that a big enough neural network can represent functions complicated enough generate language and images.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; You may have heard of the classic &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot;&gt;universal approximation theorem&lt;/a&gt; which roughly states that an infinitely-wide neural network can represent &lt;em&gt;any function at all.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The fact that nets’ expressivity is so high means that the notion of expressivity alone is usually unhelpful: it’s good to know that your network can represent virtually all functions, but this information is vacuous if you want to know which function it’ll actually learn! The one place where I’ve found the notion frequently useful is in negative cases: if a neural network architecture &lt;em&gt;can’t&lt;/em&gt; express some class of function, it cannot learn it — no need for further questions.&lt;/p&gt;

&lt;p&gt;This blogpost is about one of those cases. The most common activation function used today is $\mathrm{ReLU}$, and so one would be forgiven for thinking that any big enough $\mathrm{ReLU}$ network will be able to express any function. It turns out this reasonable intuition is wrong: there are lots of functions that a shallow $\mathrm{ReLU}$ network with no biases can’t represent, even if it has infinite width. To spoil the answer, such a net can trivially only represent homogeneous functions (i.e., those such that $f(\alpha x) = \alpha \cdot f(x)$ for $\alpha &amp;gt; 0$), but less trivially, it also can’t represent &lt;strong&gt;&lt;em&gt;odd polynomials of order greater than one.&lt;/em&gt;&lt;/strong&gt; This fact’s independently showed up in my research no fewer than three times in the last month, which I take as a pretty strong sign that it’s worth articulating and sharing.&lt;/p&gt;

&lt;p&gt;The structure of this blogpost will be as follows. I’ll begin with a refresher on the universal approximation theorem. We’ll then state a theorem about the expressivity of shallow $\mathrm{ReLU}$ networks and list some cases where it applies. We’ll conclude with some experiments showing $\mathrm{ReLU}$ networks failing to learn simple cubic and quintic monomials.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;review-the-universal-approximation-theorem&quot;&gt;Review: the universal approximation theorem&lt;/h2&gt;

&lt;p&gt;The UAT states that a shallow neural network with biases and a nonpolynomial nonlinearity can, given sufficiently large width, approximate any function to within any desired error. More precisely, suppose we have $d$-dimensional data $\mathbf{x} \in \mathbb{R}^d$ sampled from a distribution $p(\mathbf{x})$ and a feedforward function of the form&lt;/p&gt;

\[f_{\boldsymbol{\theta}}(\mathbf x) = \sum_{i=1}^n a_i \cdot \sigma( \mathbf{b}_i^\top \mathbf{x} + c_i),\]

&lt;p&gt;where $a_i \in \mathbb{R},\ \mathbf{b}_i \in \mathbb{R}^d,\ c_i \in \mathbb{R}$ are free parameters, and we write $\boldsymbol{\theta} \equiv {(a_i,\, \mathbf{b}_i,\, c_i)}_{i=1}^n$ to denote the tuple of all free parameters. Suppose we wish to approximate some desired function $f_* : \mathbb{R}^d \rightarrow \mathbb{R}$ such that the error…&lt;/p&gt;

\[\mathcal{E}_{\boldsymbol{\theta}} := \mathbb{E}_{\mathbf{x} \sim p} \left[ (f_*(\mathbf{x}) - f_{\boldsymbol{\theta}}(\mathbf{x}))^2 \right]\]

&lt;p&gt;is less than some threshold $\epsilon$. So long as $n$ is sufficiently large and the functions $\sigma, f_*, p$ satisfy some natural regularity conditions (see the &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot;&gt;Wikipedia page&lt;/a&gt;), we can always find a way to choose $\boldsymbol{\theta}$ so that $\mathcal{E}_{\boldsymbol{\theta}} &amp;lt; \epsilon$.&lt;/p&gt;

&lt;h2 id=&quot;the-limitations-of-shallow-mathrmrelu-networks&quot;&gt;The limitations of shallow $\mathrm{ReLU}$ networks&lt;/h2&gt;

&lt;p&gt;The nonlinearity $\sigma(z) = \mathrm{ReLU}(z) := \max(z,0)$ is nonpolynomial, so the universal approximation theorem indeed applies to shallow $\mathrm{ReLU}$ networks. All guarantees are off, however, if we remove the biases (i.e. set $c_i = 0$). A $\mathrm{ReLU}$ network without biases has clear expressivity limitations: for example, it can only express homogeneous functions $f(\alpha \mathbf{x}) = \alpha f(\mathbf{x})$. (For this reason, when studying shallow $\mathrm{ReLU}$ nets, it is common to consider only functions on the sphere.) The point of this blogpost is to show that there’s an &lt;em&gt;additional&lt;/em&gt; limitation which is often overlooked.&lt;/p&gt;

&lt;p&gt;We start with an observation about the functional form of a shallow $\mathrm{ReLU}$ net.&lt;/p&gt;

&lt;div style=&quot;border: 2px solid black; padding: 15px; width: 100%; text-align: left;&quot;&gt;
&lt;b&gt;Proposition 1.&lt;/b&gt; Let $f_{\boldsymbol{\theta}}(\mathbf{x})$ be a shallow $\mathrm{ReLU}$ network with no biases. The network function has the linear-plus-even form

$$
f_{\boldsymbol{\theta}}(\mathbf{x}) = \boldsymbol{\beta}^\top \mathbf{x} + |\!| \mathbf{x} |\!| \cdot \mathrm{even}(\mathbf{x}),
$$

where $\mathrm{even}(\cdot)$ is a function satisfying $\mathrm{even}(\mathbf{x}) = \mathrm{even}(-\mathbf{x})$.
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h5 class=&quot;toggle-header2&quot; onclick=&quot;toggleContent2()&quot;&gt;Click for proof&lt;/h5&gt;
&lt;div class=&quot;toggle-content2&quot;&gt;
The function $f_{\boldsymbol{\theta}}(\mathbf{x})$ is a sum of functions of the form $f_i(\mathbf{x}) = a_i \cdot \mathrm{ReLU}(\mathbf{b}_i^\top \mathbf{x}) = \frac{1}{2} \cdot a_i \cdot (\lvert \mathbf{b}_i^\top \mathbf{x} \rvert + \mathbf{b}_i^\top \mathbf{x})$, which has the desired form. The full function $f_{\boldsymbol{\theta}}$ therefore has the desired form.
&lt;/div&gt;

&lt;script&gt;
    function toggleContent2() {
        var content = document.querySelector('.toggle-content2');
        if (content.style.display === 'none' || content.style.display === '') {
            content.style.display = 'block';
        } else {
            content.style.display = 'none';
        }
    }
&lt;/script&gt;

&lt;style&gt;
    .toggle-header2 {
        cursor: pointer;
        background-color: #f1f1f1;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 5px;
    }
    .toggle-content2 {
        display: none;
        margin-top: 10px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 5px;
        background-color: #f9f9f9;
    }
&lt;/style&gt;

&lt;p&gt;Any function that doesn’t have this linear-plus-even form can’t be represented by a shallow ReLU net without biases. The following definition and proposition show that if a function has this form, it &lt;em&gt;can&lt;/em&gt; be represented (or more precisely, approximated to arbitrary accuracy) by a shallow $\mathrm{ReLU}$ net.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 1:&lt;/strong&gt; Let $f_{\boldsymbol{\theta}}(\mathbf{x})$ be a shallow $\mathrm{ReLU}$ network with no biases and width $n$. We will say a function $f_\ast$ is &lt;em&gt;shallow&lt;/em&gt; $\mathrm{ReLU}$ &lt;em&gt;approximable&lt;/em&gt; if, for every probability distribution $p(\cdot)$ with compact support and every $\epsilon &amp;gt; 0$, we may choose $n$, $\boldsymbol{\theta}$ such that $\mathcal{E}_{\boldsymbol{\theta}} &amp;lt; \epsilon$.&lt;/p&gt;

&lt;div style=&quot;border: 2px solid black; padding: 15px; width: 100%; text-align: left;&quot;&gt;

&lt;b&gt;Proposition 2.&lt;/b&gt; A continuous function $f_\star$ is shallow $\mathrm{ReLU}$ approximable &lt;i&gt;if and only if&lt;/i&gt; it has the linear-plus-even form

$$
f_*(\mathbf{x}) = \boldsymbol{\beta}^\top \mathbf{x} + |\!| \mathbf{x} |\!| \cdot \mathrm{even} \left( \frac{\mathbf{x}}{|\!| \mathbf{x} |\!|} \right)
$$

where $\mathrm{even}(\mathbf{z}) = \mathrm{even}(-\mathbf{z})$.

&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h5 class=&quot;toggle-header&quot; onclick=&quot;toggleContent()&quot;&gt;Click for proof sketch&lt;/h5&gt;
&lt;div class=&quot;toggle-content&quot;&gt;
&lt;p&gt;We can prove this by first using homogeneity to only have to consider functions on the sphere, then using Fourier analysis with spherical harmonics to show that the $\mathbf{ReLU}$ basis function is enough to represent all linear and even-order polynomials. Any continuous function on the sphere may be approximated by a polynomial by the Stone-Weierstrass theorem (I think I’m using that right). I’m not going to flesh out the details here, but ChatGPT can probably do it if you’re curious.&lt;/p&gt;
&lt;/div&gt;
&lt;script&gt;
    function toggleContent() {
        var content = document.querySelector('.toggle-content');
        if (content.style.display === 'none' || content.style.display === '') {
            content.style.display = 'block';
        } else {
            content.style.display = 'none';
        }
    }
&lt;/script&gt;

&lt;style&gt;
    .toggle-header {
        cursor: pointer;
        background-color: #f1f1f1;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 5px;
    }
    .toggle-content {
        display: none;
        margin-top: 10px;
        padding: 10px;
        border: 1px solid #ddd;
        border-radius: 5px;
        background-color: #f9f9f9;
    }
&lt;/style&gt;

&lt;p&gt;Out of this we get a nice corollary that gives us a property to look for that tells us that no component of our function is representable:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 1:&lt;/strong&gt; Suppose $p(\mathbf{x}) = p(-\mathbf{x})$ and $f_\ast(\mathbf{x})$ is an odd function satisfying $\mathbb{E}_{x \sim p}[f_\ast(\mathbf{x}) \cdot \boldsymbol{\beta}^\top \mathbf{x}] = 0$ for all $\boldsymbol{\beta}$ (i.e., it’s orthogonal to all linear functions). Then $\mathbb{E}_{x \sim p}[f_\ast(\mathbf{x}) f_{\boldsymbol{\theta}}(\mathbf{x})] = 0$ for all ${\boldsymbol{\theta}}$, and when choosing $\boldsymbol{\theta}$ to minimize the MSE $\mathcal{E}_{\boldsymbol{\theta}}$, the best the network can do is to represent the zero function.&lt;/p&gt;

&lt;p&gt;(We can actually prove this corollary without going through the above machinery, just integrating directly against the measure and using properties of the $\mathrm{ReLU}$ function, but it’s illuminating to go through the above propositions because we get a complete picture of the representable functions.)&lt;/p&gt;

&lt;h2 id=&quot;some-examples&quot;&gt;Some examples&lt;/h2&gt;

&lt;p&gt;So what? Does this ever actually show up? Here are two cases where it’s shown up in my recent research. Notice how you could easily think shallow $\mathrm{ReLU}$ networks could do the job if you didn’t know better.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sinusoids on the unit circle.&lt;/strong&gt; If $\mathbf{x}$ is sampled from the unit circle, either uniformly over all angles or uniformly over evenly-spaced discrete points, then the network can’t achieve nonzero learning on functions like $\sin(k x_1)$ with $k = 3, 5, 7, \ldots$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cubic monomials and higher-order friends.&lt;/strong&gt; Suppose the coordinates of $\mathbf{x}$ are sampled independently (but not necessarily identically) from mearn-zero (say, Gaussian) distributions. The network can’t achieve nonzero learning on any monomial $x_i x_j x_k$ with nonrepeated indices. The same is true of monomials of order $5, 7,$ etc.&lt;/p&gt;

&lt;p&gt;This last one’s pretty surprising to me! To test it, I trained bias-free $\mathrm{ReLU}$ nets, both shallow (i.e. depth 2) and deep (depth 3), on monomial functions of Gaussian distributions. As you can see, the deep net can make progress on monomials of every order, but the shallow net can make no progress on odd monomials of order $\ge 3$. Weird!&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/shallow_relu_expressivity/monomial_training_expts.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding thoughts&lt;/h2&gt;

&lt;p&gt;This doesn’t matter at all for deep learning practitioners, but it’s useful to keep in mind for deep learning theorists. It’s useful to study the simplest model that can perform a learning task, and it’s good to know that the first thing you might try — a shallow, bias-free $\mathrm{ReLU}$ net — can’t even learn every function on the sphere.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Thanks to Joey Turnbull, who first brought the experiment reported above to my attention.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Of course, the more remarkable fact is that neural networks not only &lt;em&gt;can represent&lt;/em&gt; these functions but also &lt;em&gt;do learn&lt;/em&gt; them when trained. This is beyond the power of expressivity to explain. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;As a disclaimer, I’ll say that I’m confident that none of the content in this blogpost is new: I’m sure many researchers over the years have realized all of this. I’m not sure where to find these results in the literature, and it’s easier for me to write this then to find a good reference, so I’m just going to develop it all from first principles, but if you know a reference, do send it my way. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 03 Apr 2025 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/the-expressivity-of-shallow-relu-nets/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/the-expressivity-of-shallow-relu-nets/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>The eigensystem of the Gaussian kernel w.r.t. a Gaussian measure</title>
        <description>&lt;p&gt;&lt;em&gt;This blogpost composed in collaboration with &lt;a href=&quot;https://yuxi-liu-wired.github.io/&quot;&gt;Yuxi Liu&lt;/a&gt;, who did much of the analytical legwork.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A &lt;a href=&quot;https://arxiv.org/abs/2002.02561&quot;&gt;large&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.09796&quot;&gt;number&lt;/a&gt; of &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;recent&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2203.06176&quot;&gt;works&lt;/a&gt; have converged on the conclusion that the behavior and performance of kernel methods (and kernel regression in particular) can be simply described in terms of the eigenstructure of the task with respect to the kernel. To restate: &lt;em&gt;we can understand kernel regression to the degree we understand the eigenstructure of the task.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is generally not too hard to diagonalize the kernel and obtain the task eigenstructure numerically, and this has lead to a great deal of insight into the learning behavior of kernel regression. However, it’s generally quite difficult to find task eigenstructure analytically, even in simple toy cases, as it usually requires computing some complicated integrals. As every physicist knows, there are major advantages to analytical solutions: insight and intuition are often more easily extracted from symbolic solutions, and by taking limits a single analytical solution can give a system’s phenomenology in several different regimes. It thus behooves us to collect analytically solvable cases and sit with them to extract generalizing intuition. &lt;strong&gt;To that end, in this blogpost, we will give the analytical eigenstructure for the Gaussian kernel on a 1D Gaussian distribution.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-exactly-diagonalizable-cases-are-known&quot;&gt;What exactly diagonalizable cases are known?&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The first cases one turns to when one wants an exact analytical eigensystem are settings where the kernel and the domain share a &lt;em&gt;symmetry.&lt;/em&gt;
For example, with a translation-invariant kernel on a translation-invariant domain (e.g. the $d$-torus) or a rotation-invariant kernel on a rotation-invariant-domain (e.g. the $d$-sphere), the eigenfunctions are simply the harmonic functions on the domain (plane waves or spherical harmonics in the two named cases), and the eigenvalues are given by the Fourier transform of the kernel.
Some high-dimensional problems resemble these cases when the number of dimensions goes to infinity (e.g. &lt;a href=&quot;https://arxiv.org/abs/1908.05355&quot;&gt;Mei + Montanari (2019)&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Solvable finite-dimensional cases without exact symmetry are rare.
It seems worth considering such cases, since real data rarely has such symmetry, and the eigenfunctions are only nontrivial if there’s no exact symmetry.
&lt;a href=&quot;https://citeseerx.ist.psu.edu/document?repid=rep1&amp;amp;type=pdf&amp;amp;doi=b2d5fff7600abe81da7b1e5af749ed9511867aed&quot;&gt;Zhu et al. (1997)&lt;/a&gt; treat a very similar case to the one we develop here.
I don’t really know any other examples of exactly solvable cases.
If you know any more, let me know.
It seems worth compiling a list!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;problem-setup-gaussian-kernel-on-a-1d-gaussian-distribution&quot;&gt;Problem setup: Gaussian kernel on a 1D Gaussian distribution&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Consider a scalar normal random variable $x \sim \mathcal{N}(0,\sigma^2)$, where $\sigma^2$ is the variance of the distribution. Consider learning on this distribution with a &lt;em&gt;Gaussian kernel&lt;/em&gt;&lt;/p&gt;

\[K(x,y)=e^{-\frac{(x-y)^2}{2 w^2}}\]

&lt;p&gt;where $w$ is the width of the kernel. We seek an &lt;em&gt;eigendecomposition&lt;/em&gt;&lt;/p&gt;

\[K(x,y) = \sum_k \lambda_k \, \phi_k(x) \, \phi_k(y),\]

&lt;p&gt;where $\lbrace \lambda_k \rbrace$ are nonnegative eigenvalues and $\lbrace \phi_k \rbrace$ are eigenfunctions satisfying the orthonormality relation&lt;/p&gt;

\[\mathbb{E}_x[\phi_k(x) \phi_\ell(x)] = \delta_{k\ell}.\]

&lt;p&gt;We refer to $\lbrace \lambda_k \rbrace$ and $\lbrace \phi_k \rbrace$ together as the kernel &lt;em&gt;eigensystem&lt;/em&gt; or &lt;em&gt;eigenstructure.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-result-analytical-form-of-the-eigenstructure&quot;&gt;Main result: analytical form of the eigenstructure&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hermite-polynomials&quot;&gt;Hermite polynomials&lt;/h3&gt;

&lt;p&gt;Let $h_k(x) \equiv \frac{1}{\sqrt{k!}} \text{He}_k(x)$ denote the (rescaled) &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermite_polynomials&quot;&gt;probabilist’s Hermite polynomials&lt;/a&gt;. The normalization prefactor $\frac{1}{\sqrt{k!}}$ is chosen so that $\mathbb{E}_{x \sim \mathcal{N}(0,1)}[h_k^2(x)] = 1$.&lt;/p&gt;

&lt;h3 id=&quot;the-eigenstructure-in-closed-form&quot;&gt;The eigenstructure in closed form&lt;/h3&gt;

&lt;div style=&quot;border: 2px solid black; padding: 15px; width: 100%; text-align: left;&quot;&gt;

The eigenfunctions and eigenvalues are given by:

$$
\begin{aligned}
    \phi_k(x) &amp;amp;= c \cdot e^{-\frac{x^2}{2 \alpha^2}} \cdot h_k\!\left(\frac{x}{\beta}\right) \qquad &amp;amp;&amp;amp;(1) \\[8pt]
    \lambda_k &amp;amp;= (1 - r)\,r^k \qquad &amp;amp;&amp;amp;(2)
\end{aligned}
$$

for $k \ge 0$, where the scalars $\alpha, \beta, c, r &amp;gt; 0$ are given by:

$$
\begin{aligned}
    \alpha &amp;amp;= \frac{1}{\sqrt{2}}\Bigl(w^2 + w\,\sqrt{w^2 + 4\,\sigma^2}\Bigr)^{\!1/2}, \\[8pt]
    \beta &amp;amp;= \frac{\sigma \,\sqrt{w}}{\bigl(w^2 + 4\,\sigma^2\bigr)^{1/4}}, \\[8pt]
    c &amp;amp;= \Bigl[1 + \frac{4\,\sigma^2}{w^2}\Bigr]^{\!1/8}, \\[8pt]
    r &amp;amp;= \frac{\sigma^2}{\sigma^2 + \alpha^2}.
\end{aligned}
$$

&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Note that the eigenfunctions resemble the eigenfunctions of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_harmonic_oscillator&quot;&gt;quantum harmonic oscillator&lt;/a&gt; but  with different scale factors $\alpha \neq\beta$ on the exponential and Hermite components. The eigenvalues are a geometric series with ratio $r &amp;lt; 1$.&lt;/p&gt;

&lt;h3 id=&quot;obtaining-the-answer&quot;&gt;Obtaining the answer&lt;/h3&gt;

&lt;p&gt;We obtained these eigenfunctions treating Equations 1,2 as an ansatz and solving for $\alpha, \beta, c, r$. This is doable with &lt;a href=&quot;https://en.m.wikipedia.org/wiki/Mehler_kernel&quot;&gt;Mehler’s formula&lt;/a&gt; and algebraic manipulation. &lt;a href=&quot;/assets/gaussian_kernel_eigendecomp_derivation.pdf&quot;&gt;Here&lt;/a&gt; is a step-by-step derivation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;visualizing-the-eigenstructure&quot;&gt;Visualizing the eigenstructure&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We can plot the eigenfunctions and eigenvalues and compare analytical formulae with numerics to make sure we’re right. &lt;a href=&quot;https://colab.research.google.com/drive/1Vz0RZuMxrgGGz9aq9e4l4PH8p9jIIBXG?usp=sharing&quot;&gt;Here’s&lt;/a&gt; a colab notebook that does so, and here’s the resulting plot:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/analytical_gaussian_eigendecompositions/1d_eigenfn_viz.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Each row shows the eigenstructure for a different ratio $\frac{w^2}{\sigma^2}$. The leftmost plot in each row is a simple visualization of the Gaussian PDF (which here always has $\sigma^2 = 1$) and the Gaussian kernel. The middle plot in each row shows the first five kernel eigenfunctions with theoretical curves overlaid for comparison. The rightmost plot in each row shows the kernel eigenvalues. We find that the experimental eigenvalues indeed follow the predicted geometric descent (with small deflections due to numerical error).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;eigenstructure-in-two-limits&quot;&gt;Eigenstructure in two limits&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the top and bottow rows in the above figure, we get the feeling it might be informative to examine the $\sigma^2 \gg w^2$ and $\sigma^2 \ll w^2$ limits of our analytical expressions. Here’s what you get:&lt;/p&gt;

&lt;h3 id=&quot;narrow-kernel-sigma2-gg-w2&quot;&gt;Narrow kernel: $\sigma^2 \gg w^2$&lt;/h3&gt;

\[\begin{aligned}
    \alpha &amp;amp;\approx \sqrt{\sigma w}, \\[6pt]
    \beta &amp;amp;\approx \sqrt{\frac{\sigma w}{2}}, \\[6pt]
    c &amp;amp;\approx \left( \frac{2 \sigma}{w} \right)^{1/4}, \\[6pt]
    r &amp;amp;\approx 1.
\end{aligned}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this regime, the scale factors $\alpha, \beta$ are the same up to a factor of $\sqrt{2}$, and the resulting eigenfunctions $\phi_k$ are exactly the eigenfunctions of the quantum harmonic oscillator (note that we’re using probabilist’s instead of physicist’s Hermite polynomials). This makes sense, as we’d expect the eigenfunctions of a sufficiently narrow Gaussian kernel to converge to the eigenfunctions of the Laplacian. The ratio $r = \frac{\lambda_{k+1}}{\lambda_k}$ approaches $1$, which tells us that the top eigenfunctions have practically identical eigenvalues.&lt;/p&gt;

&lt;h3 id=&quot;wide-kernel-sigma2-ll-w2&quot;&gt;Wide kernel: $\sigma^2 \ll w^2$&lt;/h3&gt;

\[\begin{aligned}
    \alpha &amp;amp;\approx w, \\[6pt]
    \beta &amp;amp;\approx \sigma, \\[6pt]
    c &amp;amp;\approx 1, \\[6pt]
    r &amp;amp;\approx \frac{\sigma^2}{w^2}.
\end{aligned}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This is the more interesting regime. In this case, $\alpha$ and $\beta$ decouple, and in fact $\alpha \approx w \gg \sigma$ is so wide that the exponential factor in the functional form of $\phi_k(x)$ becomes irrelevant, and we’re left with just a Hermite polynomial. You can see this in the bottom row of the figure above: the eigenfunctions start to just look like polynomials of increasing order. The eigenvalue ratio $r \approx \frac{\sigma^2}{w^2}$ approaches &lt;em&gt;the ratio of the distribution variance to the kernel variance.&lt;/em&gt; These two facts are the main things I wanted to get out of this calculation.&lt;/p&gt;

&lt;h3 id=&quot;how-hermite-are-the-eigenfunctions-for-varying-values-of-sigma2--w2&quot;&gt;How “Hermite” are the eigenfunctions for varying values of $\sigma^2 / w^2$?&lt;/h3&gt;

&lt;p&gt;Let us perform a numerical experiment: we shall compute the true kernel eigenfunctions ${\phi_k}$, compute the Hermite polynomials ${h_k}$, and for each $k$, find the cosine similarity 
\(\frac{\langle \phi_k, h_k \rangle}{\| h_k \|}\)
where the inner product 
\(\langle f, g \rangle = \mathbb{E}_x[f(x) g(x)]\)
is the $L^2$ inner product w.r.t. the measure.&lt;/p&gt;

&lt;p&gt;The results are plotted below:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/analytical_gaussian_eigendecompositions/eigenfn_hermite_similarity.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;A few observations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;at larger $k$, we require smaller $\gamma \equiv \frac{\sigma^2}{w^2}$ to approach Hermiteness, and&lt;/li&gt;
  &lt;li&gt;$\gamma &amp;lt; 0.1$ is generally small enough, at least up to $k = 3$ or $4$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;implications-and-next-steps&quot;&gt;Implications and next steps&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The main takeaway here is that so long as $\sigma^2$ is sufficiently smaller than $w^2$ — &lt;em&gt;and we don’t care exactly how small&lt;/em&gt; — the kernel’s “eigenfeatures” are simply the “Hermite features” of the data. In this regime, the kernel eigenvalues are simply powers of $\frac{\sigma^2}{w^2}$. Nice!&lt;/p&gt;

&lt;p&gt;What’s next? It’s likely that a more general class of kernels (including in higher dimension) can be solved using these techniques, and if that’s the case, it’s worth compiling these exactly solvable cases and extracting what intuition we can. Even with this 1D Gaussian case, though, there are probably ways to take this result and apply its intuition to kernel learning problems on (potentially realistic) high-dimensional data.&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Mar 2025 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/gaussian-kernel-1d-eigendecomp/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/gaussian-kernel-1d-eigendecomp/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>The optimal low-rank solution for linear regression</title>
        <description>&lt;p&gt;Consider linear regression with squared error in which we wish to choose a matrix $\mathbf{W}$ to minimize&lt;/p&gt;

\[\mathcal{E} \equiv \mathbb{E}_{(x, y) \sim \mu}\left[ |\!| \mathbf{W}x - y |\!|^2 \right] = \text{Tr}\left[ \mathbf{W} \boldsymbol{\Sigma}_{xx} \mathbf{W}^T - 2 \mathbf{W} \boldsymbol{\Sigma}_{xy} + \boldsymbol{\Sigma_{yy}} \right],\]

&lt;p&gt;where $\boldsymbol{\Sigma}_{xx} = \mathbb{E}[x x^T]$ and so on, under the constraint that $\mathbf{W}$ may be at most rank $k$. Assume that $\boldsymbol{\Sigma}_{xx}$ is full rank (i.e., we are underparameterized). We may rewrite this loss as&lt;/p&gt;

\[\mathcal{E} = |\!| \tilde{\mathbf{W}} - \boldsymbol{\Sigma}_{yx} \boldsymbol{\Sigma}_{xx}^{-1/2} |\!|_F^2,\]

&lt;p&gt;where $\tilde{\mathbf{W}} \equiv \mathbf{W} \boldsymbol{\Sigma}^{1/2}$. It is then clear that the optimal rank-$k$ choice for $\tilde{\mathbf{W}}$ is&lt;/p&gt;

\[\tilde{\mathbf{W}}_*^{(k)} = \text{topsvd}_k(\boldsymbol{\Sigma}_{yx} \boldsymbol{\Sigma}_{xx}^{-1/2} ),\]

&lt;p&gt;where the operator $\text{topsvd}_k(\cdot)$ returns the matrix comprised of the top $k$ singular directions of its argument. We thus conclude that the optimal rank-$k$ model matrix is&lt;/p&gt;

\[\mathbf{W}_*^{(k)} = \text{topsvd}_k(\boldsymbol{\Sigma}_{yx} \boldsymbol{\Sigma}_{xx}^{-1/2} ) \boldsymbol{\Sigma_{xx}}^{-1/2}.\]

&lt;p&gt;When the rank is unconstrained and $k$ is maximal, we find that $\mathbf{W}_* = \boldsymbol{\Sigma}_{yx} \boldsymbol{\Sigma}_{xx}^{-1}$ as expected.&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Nov 2024 00:00:00 -0900</pubDate>
        <link>http://localhost:4000/blog/low-rank-linear-regression-sol/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/low-rank-linear-regression-sol/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>Infinite-width autoencoders are cursed</title>
        <description>&lt;p&gt;&lt;strong&gt;In this blogpost, I show that infinite-width neural networks with a finite-width layer in the middle are cursed: they can’t be parameterized so that, when trained with gradient descent, they (a) train in finite time and (b) have their weight tensors undergo alignment as prescribed by $\mu$P. I conclude by describing two modifications that fix this problem.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Autoencoder? I hardly know her!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;~ Traditional San Francisco saying&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;how-do-you-width-scale-an-autoencoder&quot;&gt;How do you width-scale an autoencoder?&lt;/h2&gt;

&lt;p&gt;Autoencoders are neural networks designed to extract (comparatively) low-dimensional representations from high-dimensional data. They’re widely-used tools for dimensionality reduction, and in the form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Variational_autoencoder&quot;&gt;VAEs&lt;/a&gt;, data &lt;em&gt;generation&lt;/em&gt;. The classic autoencoder design looks like an hourglass: the input layer is the widest, subsequent layers have fewer and fewer layers until they reach a narrowest “bottleneck” layer, and then the sequence is repeated in reverse.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/inf_width_autoencoders/autoencoder_diagram.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;An autoencoder is usually trained with a reconstruction objective — that is, with the goal of learning the identity function $f: x \mapsto x$ on the data.&lt;/p&gt;

&lt;p&gt;Autoencoders are interesting from a scaling perspective because they represent a case where finite width is desirable, at least in one layer. They won’t learn interesting representations if every hidden layer is wide. That said, a predominant lesson from the last few years of deep learning is that, insofar as network architecture is concerned, &lt;a href=&quot;https://proceedings.mlr.press/v139/yang21c.html&quot;&gt;wider&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;is&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.08774&quot;&gt;always&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2311.14646&quot;&gt;better&lt;/a&gt; so long as your hyperparameters are properly tuned. In what sense will this also be true of autoencoders?&lt;/p&gt;

&lt;p&gt;Asked from another direction: &lt;em&gt;how should you take the large-width limit of an autoencoder?&lt;/em&gt; Which layers in the hourglass above do you take to be infinite width? All of them &lt;em&gt;except&lt;/em&gt; the input, bottleneck, and output? Do you preserve the hourglass taper, jumping up to a large width $N$ at the second layer but then gradually &lt;em&gt;decreasing&lt;/em&gt; width down to a constant-width bottleneck? If so, do you taper width down by a constant factor $C$ per layer so the width at layer $\ell$ is $C^{-\ell}N$? Do you take widths $N^{-\alpha(\ell)}$ with $\alpha(\ell)$ interpolating from $1 \rightarrow 0$? It’s unclear!&lt;/p&gt;

&lt;p&gt;In this blogpost, I will show that the answer is that, with the standard notion of neural network parameterization, &lt;strong&gt;you cannot.&lt;/strong&gt; There is no consistent way to take an infinite-width limit such that the net satisfies typical notions of layer alignment and feature learning. I’ll demonstrate this by examining a two-layer slice of a deep net, aiming to convey the general problem by means of this example. I’ll then give some solutions.&lt;/p&gt;

&lt;h2 id=&quot;example-a-width-one-bottleneck&quot;&gt;Example: a width-one bottleneck&lt;/h2&gt;

&lt;p&gt;Consider a deep network which has hidden width $n_k = N$ at most layers but one bottleneck layer $\ell$ with width $n_\ell = 1$. Suppose there is no nonlinearity at the bottleneck layer or adjacent layers. For the weights before and after the bottleneck, let us adopt the shorthand $\mathbf{u}^\top = \mathbf{W}_{\ell-1}$ and $\mathbf{v} = \mathbf{W}_\ell$, with $\mathbf{u}, \mathbf{v} \in \mathbb{R}^N$. Let us write $\mathbf{M} = \mathbf{v}\mathbf{u}^\top \in \mathbb{R}^{N \times N}$ for the full rank-one parameter block comprised of these two weight matrices.&lt;/p&gt;

&lt;p&gt;We will consider training these two weight matrices through several steps of SGD on a single example $x$. Let us denote the hidden vector passed into this bottleneck block by $\mathbf{h} = \mathbf{h}(x) \in \mathbb{R}^N$, denote the output of the block by $\tilde{\mathbf{h}} = \tilde{\mathbf{h}}(x) \in \mathbb{R}^N$, and denote the gradient backpropagated into this block by $\mathbf{g} = -\nabla_{\tilde{\mathbf{h}}} \mathcal{L}$, where $\mathcal{L}$ is our global loss. We will assume for simplicity that $\mathbf{h}$ and $\mathbf{g}$ do not change for these steps. Note that the parameter gradient applied to the whole matrix is $\nabla_\mathbf{M} \mathcal{L} = \mathbf{g} \mathbf{h}^\top$. The following figure illustrates our notation:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/inf_width_autoencoders/autoencoder_slice.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;key-feature-learning-desideratum-weight-alignment&quot;&gt;Key feature learning desideratum: weight alignment&lt;/h3&gt;

&lt;p&gt;Suppose that we have just randomly initialized the parameters $\mathbf{u}, \mathbf{v}$ and will proceed to train for several steps with fixed learning rates. Let us denote the &lt;em&gt;alignment&lt;/em&gt; between the weight vectors and their corresponding signal vectors by&lt;/p&gt;

\[\mathcal{A}_{\text{in}} = \frac{\mathbf{u}^\top \mathbf{h}}{|\!| \mathbf{u} |\!| |\!| \mathbf{h} |\!|},
\quad
\mathcal{A}_{\text{out}} = \frac{\mathbf{v}^\top \mathbf{g}}{|\!| \mathbf{v} |\!| |\!| \mathbf{g} |\!|}.\]

&lt;p&gt;At initialization, when $\mathbf{u}, \mathbf{v}$ are random vectors, we have that $\mathcal{A}_\text{in}, \mathcal{A}_\text{out} = \Theta(N^{-1/2})$.&lt;/p&gt;

&lt;p&gt;Under the precepts of $\mu$P, we desire the following two natural conditions of feature learning:&lt;/p&gt;

&lt;p style=&quot;padding: 10px; border: 2px solid black;&quot;&gt;
&lt;strong&gt;Weight alignment desideratum.&lt;/strong&gt; After $O(1)$ steps of training, we desire that $\mathcal{A}_\text{in}, \mathcal{A}_\text{out} = \Theta(1)$.
&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;No-blowup desideratum.&lt;/strong&gt; After a handful of steps of training, the norms of each weight matrix should have reached their final size, scaling wise. That is, $\frac{|\!| \mathbf{u}_{t+1} |\!|}{|\!| \mathbf{u}_t |\!|} = \Theta(1)$ and likewise for $\mathbf{v}$.
&lt;/p&gt;

&lt;p&gt;The first desideratum captures the intuitive notion that weight matrices should align to (the top singular directions of) their gradients upon proper feature learning. In conventional parlance, feature evolution is about leading-order change in the &lt;em&gt;activations&lt;/em&gt;, not the &lt;em&gt;weight tensors,&lt;/em&gt; but these are in fact two sides of the same coin, as one finds upon a &lt;a href=&quot;https://arxiv.org/abs/2310.17813&quot;&gt;spectral-norm analysis&lt;/a&gt; of deep learning. The second desideratum basically says that we don’t want things to blow up. Obviously, if we take infinite learning rates, these matrices will align just fine, but their norms won’t stabilize. It’s fine to initialize one of them to be very close to zero, but its norm should stabilize after a few steps.&lt;/p&gt;

&lt;p&gt;We’ll now see that these intuitive desiderata are &lt;em&gt;incompatible&lt;/em&gt; for training under SGD.&lt;/p&gt;

&lt;h3 id=&quot;evolution-under-sgd&quot;&gt;Evolution under SGD&lt;/h3&gt;

&lt;p&gt;Suppose we have learning rates $\eta_u$ and $\eta_v$ for $\mathbf{u},\mathbf{v}$, respectively. These vectors then see gradient updates&lt;/p&gt;

\[\delta \mathbf{u} = \eta_u \cdot \mathcal{A}_\text{out} \cdot |\!|\mathbf{h}|\!|
|\!|\mathbf{g}|\!|
|\!|\mathbf{v}|\!|
\cdot
\hat{\mathbf{h}},\]

\[\delta \mathbf{v} = \eta_v \cdot \mathcal{A}_\text{in} \cdot |\!|\mathbf{h}|\!|
|\!|\mathbf{g}|\!|
|\!|\mathbf{u}|\!|
\cdot
\hat{\mathbf{g}},\]

&lt;p&gt;where $\hat{\mathbf{h}}, \hat{\mathbf{g}}$ are unit vectors in the directions of $\mathbf{h}, \mathbf{g}$. We are free to absorb $|\!| \mathbf{h} |\!|$ and $|\!| \mathbf{g} |\!|$ into the definitions of $\eta_u, \eta_v$. More subtly, we are free to absorb the initial scales of $|\!| \mathbf{u}_0 |\!|$ and $|\!| \mathbf{v}_0 |\!|$ into the learning rates, too, and so will henceforth assume that these vectors are of the same size up to a constant factor.&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

\[\delta \mathbf{u} = \tilde \eta_u \cdot \mathcal{A}_\text{out} \cdot 
|\!|\mathbf{v}|\!|
\cdot
\hat{\mathbf{h}},\]

\[\delta \mathbf{v} = \tilde \eta_v \cdot \mathcal{A}_\text{in} \cdot 
|\!|\mathbf{u}|\!|
\cdot
\hat{\mathbf{g}}.\]

&lt;p&gt;From this, we may assess the update norms as&lt;/p&gt;

\[\frac{|\!| \delta \mathbf{u} |\!|}{|\!| \mathbf{u} |\!|} = \tilde \eta_u \cdot \mathcal{A}_\text{out} \cdot 
\frac{|\!|  \mathbf{v} |\!|}{|\!| \mathbf{u} |\!|} \sim \tilde \eta_u \cdot \mathcal{A}_\text{out},\]

\[\frac{|\!| \delta \mathbf{v} |\!|}{|\!| \mathbf{v} |\!|} = \tilde \eta_v \cdot \mathcal{A}_\text{in} \cdot 
\frac{|\!|  \mathbf{u} |\!|}{|\!| \mathbf{v} |\!|} \sim \tilde \eta_v \cdot \mathcal{A}_\text{in},\]

&lt;p&gt;where we have made use of the fact that $\frac{|\!|  \mathbf{v} |\!|}{|\!| \mathbf{u} |\!|} = \Theta(1)$. We now return to our boxed desiderata. To satisfy the weight alignment desideratum, we require $\frac{|\!| \delta \mathbf{u} |\!|}{|\!| \mathbf{u} |\!|} = \Omega(1)$ and likewise for $\mathbf{v}$. To satisfy the no-blowup desideratum, we require $\frac{|\!| \delta \mathbf{u} |\!|}{|\!| \mathbf{u} |\!|} = O(1)$ and likewise for $\mathbf{v}$. Combining both, we find that $\frac{|\!| \delta \mathbf{u} |\!|}{|\!| \mathbf{u} |\!|} = \Theta(1)$ and likewise for $\mathbf{v}$.&lt;/p&gt;

&lt;p&gt;We may now observe a contradiction. After a few steps we will have that $\mathcal{A}_\text{in}, \mathcal{A}_\text{out} = \Theta(1)$, from which we may conclude that $\tilde \eta_u, \tilde \eta_v = \Theta(1)$. However, these learning rates are too small to cause alignment to begin with! At early times when the alignments are near zero, we have&lt;/p&gt;

\[\frac{d}{dt} \mathcal{A}_\text{in}
\sim \tilde{\eta}_u \cdot \mathcal{A}_\text{out} \sim \mathcal{A}_\text{out},\]

\[\frac{d}{dt} \mathcal{A}_\text{out}
\sim \tilde{\eta}_u \cdot \mathcal{A}_\text{in} \sim \mathcal{A}_\text{in}.\]

&lt;p&gt;Treating “$\sim$” as “$=$,” the solution to this coupled ODE from small initial value is&lt;/p&gt;

\[\begin{bmatrix} \mathcal{A}_\text{in} \\ \mathcal{A}_\text{out} \end{bmatrix}
\approx
e^t
\begin{bmatrix} \mathcal{A}_0 \\ \mathcal{A}_0 \end{bmatrix},\]

&lt;p&gt;where $\mathcal{A}_0 = \frac{1}{2} [\mathcal{A}_\text{in} + \mathcal{A}_\text{out} ]_{t=0}$ is the average of the initial alignments. We can now observe that, in order to grow to order unity from an initial size of $\Theta(N^{-1/2})$ requires a number of steps $T$ such that $e^T N^{-1/2} = \Theta(1)$, which implies that $T \sim \log N$! As $N$ grows, it takes longer and more and more steps to reach alignment.&lt;/p&gt;

&lt;h3 id=&quot;what-intuitively-is-going-on&quot;&gt;What, intuitively, is going on?&lt;/h3&gt;

&lt;p&gt;The crux of the problem here is that &lt;em&gt;each weight tensor’s gradient is mediated by the other weight tensor’s alignment.&lt;/em&gt; The more aligned one tensor is, the bigger an update the other one will see. The problem is that since both alignments start small, the dynamics are a classic case of two small variables suppressing each other’s gradients!&lt;sup id=&quot;fnref:c&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:c&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; We’re stuck with a Catch-22: we could jack up the learning rates to have huge updates at the start that overcome the tiny init, but then our dynamics at late times blow up! However, if the learning rate is small enough so the dynamics at late time don’t blow up, then the dynamics take a long time to get going.&lt;sup id=&quot;fnref:b&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:b&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-simulation&quot;&gt;A simulation&lt;/h3&gt;

&lt;p&gt;Below is a sweep of SGD trajectories for the loss $\mathcal{L} = (\mathbf{h}^\top \mathbf{u} \mathbf{v}^\top \mathbf{g} - 1)^2$. I have taken $|\!| \mathbf{h} |\!| = |\!| \mathbf{g} |\!| = 1$ and initialized with $u_i, v_i \sim N^{-1/2}$, because any larger init will not have aligned at convergence, and any smaller init will still suffer from the core problem but worse. I train both vectors with a learning rate $\eta$ and vary $N$. I train for a fixed number of steps $T = 10$.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/inf_width_autoencoders/autoencoder_sweep.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;One can clearly see that the region of optimizability is shrinking, albeit slowly, with $\eta_\text{min} \sim \log N$ and $\eta_\text{max} \sim 1$ bound to converge at sufficiently large $N.$ A Colab notebook reproducing this experiment may be found &lt;a href=&quot;https://colab.research.google.com/drive/1zt6qRlnDKgxzbc95\_bE8e3ptJq4A6LZs?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;so-what-requiring-log-n-time-isnt-so-bad&quot;&gt;So what? Requiring $\log N$ time isn’t so bad.&lt;/h3&gt;

&lt;p&gt;That’s true. The two reasons to care are that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;In our theoretical models, we want to work at truly infinite width, and the logarithmic factor still blows up.&lt;/li&gt;
  &lt;li&gt;Hyperparameter transfer ($\mu$Transfer) probably won’t work because the infinite-width process isn’t well-defined.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So there you have it. Infinite-width networks with finite-width bottlenecks are cursed and can’t be  trained in a traditional fashion so all layers align to their gradients. The above argument can be extended to bottlenecks with finite width $k &amp;gt; 1$, and even to those with any width $k = o(N)$, though that’d require some random matrix theory to see.&lt;/p&gt;

&lt;p&gt;This was actually an awkward point that came up during the writing of “&lt;a href=&quot;https://arxiv.org/abs/2310.17813&quot;&gt;A Spectral Condition for Feature Learning&lt;/a&gt;.” We never resolved it, we just didn’t discuss it! It’s fine for most architectures, but it does feel like a lingering problem with $\mu$P. What is to be done?&lt;/p&gt;

&lt;h2 id=&quot;two-possible-solutions-rank-regularization-and-cascading-init&quot;&gt;Two possible solutions: rank regularization and cascading init&lt;/h2&gt;

&lt;p&gt;Here I’ll pitch two solutions that I think can be used to width-scale autoencoders.&lt;/p&gt;

&lt;p&gt;The first is to ditch the variation in width between layers — just keep everything width-$N$ — and instead enforce the rank constraints implicitly with regularization. For example, if a layer is intended to have fan-out dimension $k$, one could gradually turn on an $\ell_1$ regularization on all but the top $k$ singular vectors of the weight matrix until the regularization is so high that it becomes sparse. I believe this is consistent even at infinite width, though it does unfortunately require computing the SVD lots of times.&lt;/p&gt;

&lt;p&gt;The second idea is to do a “cascading init” in the following fashion. First, initialize all weight tensors to zero. Next, choose a random batch of perhaps $P = 10^3$ inputs. Then, starting from the start of the network and working forwards, initialize each weight tensor so that its “input” singular subspace aligns with the top PCA directions of this input batch. I believe that this, too, makes sense even at infinite width, it doesn’t require computing lots of SVDs throughout training, and it gives you a nice network with aligned vectors right from the get-go. Having everything aligned like this makes the theory really nice, and $\mu$P can be very simply expressed in spectral language.&lt;/p&gt;

&lt;p&gt;A third possibility is that batchnorm or layernorm somehow fix this. My intuition’s that they won’t, though I don’t have a solid argument.&lt;/p&gt;

&lt;p&gt;A fourth solution is to use Adam or another optimizer where the update sizes are independent of the magnitude of the gradient. I think this actually just works (I suspect the LoRA analysis of &lt;a href=&quot;https://arxiv.org/abs/2402.12354&quot;&gt;this paper&lt;/a&gt; shows this!), but it still seems like things ought to be possible with SGD.&lt;/p&gt;

&lt;h2 id=&quot;discussion-what-now&quot;&gt;Discussion: what now?&lt;/h2&gt;

&lt;p&gt;Based on the above argument, I’m of the opinion that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mu$P for networks of greatly-varying width (like autoencoders) is broken: there’s no way to parameterize to get proper feature learning.&lt;/li&gt;
  &lt;li&gt;There should be a unifying solution, and it’s likely to simplify $\mu$P in the process.&lt;/li&gt;
  &lt;li&gt;“Cascading init” in particular seems like it might work.&lt;/li&gt;
  &lt;li&gt;A good metric of success would be achieving hyperparameter transfer when width-scaling an autoencoder.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seems like a good project for an ambitious grad student :)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;The ideas in this blogpost were born from research discussions with Greg Yang and Jeremy Bernstein. Thanks to Blake Bordelon for recent helpful discussion.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;To see this, note that $(\mathbf{u} \rightarrow \alpha \mathbf{u}, \ \eta_u \rightarrow \alpha \eta_u, \ \eta_v \rightarrow \alpha^{-1} \eta_v)$ is an exact symmetry of the dynamics, with a similar symmetry for \(\mathbf{v}\). &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:c&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For example, consider optimizing $\mathcal{L}(x, y) = (1 - xy)^2$ for scalars $x, y$. When $x, y$ are initialized very close to zero, these dynamics will take a logarithmically-long time to get going, because each parameter suppresses the other’s gradient. &lt;a href=&quot;#fnref:c&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:b&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is also the story with neural network dynamics in the ultra-rich regime as we describe &lt;a href=&quot;https://arxiv.org/abs/2410.04642&quot;&gt;here&lt;/a&gt;. &lt;a href=&quot;#fnref:b&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 08 Oct 2024 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/autoencoders-are-cursed/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/autoencoders-are-cursed/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>It's hard to turn a low-rank matrix into a high-rank matrix</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;em&gt;In this blogpost, I point out that applying a nonlinear function to a low-rank random matrix cannot make it into a high-rank matrix.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Large random matrices can often be profitably understood in terms of their singular value spectrum. One useful distinction is between &lt;em&gt;high (effective) rank&lt;/em&gt; and &lt;em&gt;low (effective) rank&lt;/em&gt; matrices. Low-rank matrix effects are important in deep learning, and if you listen to the whispers of deep learning theorists, they’re cropping up on the margins everywhere. My money’s on their assuming a central role in our understanding of deep learning in the coming years.&lt;/p&gt;

&lt;p&gt;This blogpost conveys a particular idea about low-rank matrices: it’s really hard to deterministically transform them into high-rank matrices. It’s written mostly for myself, but hopefully it’s useful to others.&lt;/p&gt;

&lt;h2 id=&quot;trying-and-failing-to-turn-a-low-rank-matrix-high-rank&quot;&gt;Trying and failing to turn a low-rank matrix high-rank&lt;/h2&gt;

&lt;p&gt;Suppose we independently sample two random vectors $\mathbf{x}, \mathbf{y} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_n)$ with dimension $n$ and standard Gaussian entries. From these vectors, we can construct the rank-one matrix $\mathbf{A} = \frac{1}{n} \mathbf{x}\mathbf{y}^\top$. The matrix $\mathbf{A}$ will have one nonzero singular value with value approaching $\sigma_1 = 1$ as $n$ grows.&lt;/p&gt;

&lt;p&gt;Suppose we want to turn this low-rank matrix into a high-rank matrix, with many singular values of the same order as the largest. Suppose we must accomplish this via some simple transformation, but we have no access to additional randomness, so it’s got to be a deterministic function. The most basic thing we might try is to choose some nonlinear function $\phi: \mathbb{R} \rightarrow \mathbb{R}$ and apply it elementwise to each matrix entry to get $\mathbf{A}_\phi = \frac{1}{n} \phi \circ (\mathbf{x}\mathbf{y}^\top)$.&lt;/p&gt;

&lt;p&gt;This seems promising! Having rank one is a very fragile condition, and so we expect that almost any such nonlinearity would break that condition and give us a matrix with a singular spectrum more like that of a full-rank matrix.&lt;/p&gt;

&lt;p&gt;Well, let’s try it. Here are some examples:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/low_rank_matrices/low_rank_exps_a.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In the top left plot, I’ve shown the singular spectrum of a rank-one matrix for three values of $n$. In the next four plots, I show the singular spectrum of matrices $\mathbf{A}_\phi$ for various $\phi$. In the final plot, I show the spectrum of a full-rank random matrix with i.i.d. entries, again for various $n$.&lt;/p&gt;

&lt;p&gt;What’s happened? None of the matrices $\mathbf{A}_\phi$ are still rank-one (well, except for $\phi: z \mapsto z^2$), but they seem to only have a handful of singular values of order unity. (They’re actually all nonzero, but they decay really fast.) Their spectra don’t look anything like the singular spectra of the full-rank matrix, which for large $n$ have many eigenvalues close to $\sigma_1$.&lt;/p&gt;

&lt;p&gt;One way to characterize this difference is through the &lt;em&gt;effective rank&lt;/em&gt; of the matrix, defined for a matrix $\mathbf{M}$ as $\text{erank}(\mathbf{M}) = \frac{(\sum_i \sigma_i(\mathbf{M}))^2}{\sum_i \sigma_i^2(\mathbf{M})}$. Here I will plot the effective rank of these same matrices as $n$ increases:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/low_rank_matrices/low_rank_exps_b.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We see clearly that the effective rank quickly stabilizes at an order-unity value in each case, while the effective rank of a dense random matrix grows proportionally to $n$. What’s going on?&lt;/p&gt;

&lt;h2 id=&quot;the-explanation-constructing-the-singular-spectrum&quot;&gt;The explanation: constructing the singular spectrum&lt;/h2&gt;

&lt;p&gt;We will now explain what’s going on. Remarkably, we can understand this by explicitly constructing the singular vectors for large $n$.&lt;/p&gt;

&lt;h3 id=&quot;warmup-the-square-function&quot;&gt;Warmup: the square function&lt;/h3&gt;

&lt;p&gt;We’ll start with the case of the square function, which seems like the easiest. For ease of explanation, let’s give it the notation $\xi: z \mapsto z^2$.&lt;/p&gt;

&lt;p&gt;The left- and right- singular vectors of $\mathbf{A} = \frac{1}{n} \mathbf{x}\mathbf{y}^\top$ are (proportional to) $\mathbf{x}$ and $\mathbf{y}$, respectively. What about the matrix  $\mathbf{A}_\xi = \frac{1}{n} \xi \circ \mathbf{x}\mathbf{y}^\top$? Well, this matrix has entries $A_{\xi;i,j} = \frac{1}{n} x_i^2 y_j^2$, from which we can see that&lt;/p&gt;

\[\mathbf{A}_\xi = \frac{1}{n} \left( \xi \circ \mathbf{x} \right)\left( \xi \circ \mathbf{y} \right)^\top.\]

&lt;p&gt;That is, because $\xi$ has the special property that $\xi(ab) = \xi(a) \cdot \xi(b)$, the resulting matrix $\mathbf{A}_\xi$ is actually still rank-one, and its singular vectors are pointwise-nonlinearly transformed versions of the originals. This is also true for many other common functions, including $\phi \in {\mathrm{sign}, \mathrm{abs}}$ and indeed any monomial. It’s worth noting that the condition for preserving rank-one-ness is actually just that $\xi(ab) = f(a) \cdot g(b)$ for some functions $f, g$; these functions need not equal $\xi$.&lt;/p&gt;

&lt;h3 id=&quot;the-general-case&quot;&gt;The general case&lt;/h3&gt;

&lt;p&gt;To get the case of general $\phi$, we’ll actually consider an even &lt;em&gt;larger&lt;/em&gt; set of matrices: matrices $\mathbf{H}$ such that $H_{ij} = \frac{1}{n} h(x_i, y_j)$ for a bivariate function $h : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$. The difference is that we’re not requiring this function to have the form $h(xy)$; it can be an arbitrary bivariate function.&lt;/p&gt;

&lt;p&gt;We now have a function $h(x, y)$ and probability measures $\mu_x, \mu_y$ from which $x, y$ can each be sampled (namely, both $\mu_x, \mu_y$ are unit Gaussian). This means that we can define what I think of as the “operator SVD” of the function $h$, which means writing it in the form&lt;/p&gt;

\[h(x, y) = \sum_i \sigma_i f_i(x) g_i(y),\]

&lt;p&gt;where ${ \sigma_i }$ are nonnegative singular values indexed in nonincreasing order and the functions ${f_i}, {g_i}$ are orthonormal bases in the sense that $\mathbb{E}_{x \sim \mu_x} [f_i(x) f_j(x)] = \delta_{ij}$ and $\mathbb{E}_{y \sim \mu_y} [g_i(x) g_j(x)] = \delta_{ij}$.&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The upshot is that the first singular value of $\mathbf{A}_\phi$ will be approximately the value $\sigma_1$ obtained from the operator SVD of $h(x, y) = \phi(xy)$, and the first singular vectors are going to be approximately $f_1(\mathbf{x})$ and $g_1(\mathbf{y})$. Ditto with the second and third and so on. These generally decay fairly fast (faster than a powerlaw if $h$ is analytic, I believe), so in the examples above, you usually end up with only a handful of singular values appreciably above zero. Because this spectrum is asympotically independent of the matrix size $n$ (which merely determines the error to which one approximates it), we have order-unity effective rank. Deterministic nonlinear functions applied elementwise can’t bring a matrix from rank one to full rank!&lt;/p&gt;

&lt;p&gt;As an aside, I don’t actually know how to compute the operator SVD of a generic $h$ analytically. I’d guess there’s no general form, even for Gaussian $\mu_x, \mu_y$. However, we could approximate it numerically by taking some finite-size sample of $x, y$ and working from that, and in fact, computing the singular values of $\mathbf{A}_\phi$ is precisely computing a Monte Carlo estimate of the operator singular spectrum of $h$, and ditto for the singular vectors.&lt;/p&gt;

&lt;h3 id=&quot;an-even-more-general-case-rank-k-inputs&quot;&gt;An even more general case: rank-$k$ inputs&lt;/h3&gt;

&lt;p&gt;Well, what if instead of our original matrix isn’t the rank-one matrix $\frac{1}{n} \mathbf{x} \mathbf{y}^\top$ but instead the rank-$k$ matrix $\frac{1}{n} \mathbf{X} \mathbf{Y}^\top$, with $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times k}$ having standard Gaussian entries and $k = O_n(1)$? Well, turns out that’s fine — you can do the same argument as above, but instead of a scalar bivariate function $h(x, y)$, you end up having to consider a function of two $k$-dimensional vectors $h(\mathbf{\bar{x}}, \mathbf{\bar{y}})$, where $\mathbf{\bar{x}}, \mathbf{\bar{y}}$ represent rows of $\mathbf{X}, \mathbf{Y}$, respectively. (To see this, note that the matrix entries look like $\phi(\mathbf{\bar{x}}^\top \mathbf{\bar{y}})$, and then we’re taking a generalization like we did before.) You can then compute the operator SVD in the same way, where now e.g. $f_i : \mathbb{R}^{k} \rightarrow \mathbb{R}$ are functions of vectors, not scalars. The rest of the argument goes through the same way. This can also be extended to different types of nonlinear functions: for example, if instead of acting elementwise, $\phi$ acts jointly on $k \times k$ submatrices, this still carries through. I’d guess that basically any matrix transformation which acts locally, is blind to the global index of a local region, and is smooth will admit basically the same argument.&lt;/p&gt;

&lt;h2 id=&quot;relevance-to-deep-learning&quot;&gt;Relevance to deep learning&lt;/h2&gt;

&lt;p&gt;One reason this stuff matters is that gradient updates to matrices are low-rank, but modern optimizers (Adam, SignSGD, etc.) often don’t update in the true direction of the gradient, instead applying some kind of elementwise preconditioning. This story about the robustness of low-rankness means that even with these modern optimizers, we expect this low-rank behavior to remain pretty strongly in effect regardless of the optimizer. This is basically why conclusions about the scaling behavior for SGD at infinite width continue to apply (with appropriate modification) for Adam and other optimizers even though their updates aren’t gradient-aligned.&lt;/p&gt;

&lt;h3 id=&quot;some-questions-i-still-have&quot;&gt;Some questions I still have&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Can this be understood via some kind of “entropy” argument? It’d be very cool if something like the following could be made rigorous: &lt;em&gt;we only had a $\Theta(n)$ “amount of randomness” to begin with, but a full-rank random matrix has an “amount of randomness” scaling as $\Theta(n^2)$, so we couldn’t possibly close that gap with a deterministic function, and we need another source of randomness to do so.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;What if the rank $k$ isn’t order-unity, but it’s also not order-$n$, but instead scales like say $k \sim n^{1/2}$? Does the effective rank remain $O(k)$? Seems likely.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Thanks to Greg Yang for pointing this out to me during the development of &lt;a href=&quot;https://arxiv.org/pdf/2310.17813&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is a super powerful and underutilized decomposition of bivariate functions! My coauthors and I used it as the basis of understanding Gaussian universality for random feature models in our paper &lt;a href=&quot;http://tinyurl.com/more-is-better&quot;&gt;More Is Better&lt;/a&gt;. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 04 Oct 2024 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/transforming-low-rank-matrices/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/transforming-low-rank-matrices/</guid>
        
        
        <category>research</category>
        
      </item>
    
      <item>
        <title>Insights into GPT-2's positional encodings</title>
        <description>&lt;p&gt;&lt;em&gt;In this blogpost, I show that GPT-2’s positional encodings lie in a roughly orthogonal subspace to its token embeddings. I then show that the subsequent attention layer and MLP are sensitized to these subspaces in intuitive fashions.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I’m finally learning about transformers, and I intend to both learn the factual basics and also build up some intuition for how they work and how to think about them.
In doing so, I found I got a bit stuck when reading about &lt;a href=&quot;https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/&quot;&gt;positional encoding&lt;/a&gt;, the original scheme used to provide transformers with token position information.
In particular, I was confused as to how the positional encodings wouldn’t interfere with the token embeddings.
Messing around with GPT-2, I found the answer’s basically that they’re learned to be orthogonal, and the consequences of this can be seen in downstream weight tensors, too!
I’ll begin by framing the puzzle here, then show numerical evidence that these two types of embedding are approximately orthogonal, and finally show how this is reflected in some model weight tensors.&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction-why-dont-positional-encodings-interfere-with-token-embeddings&quot;&gt;Introduction: why don’t positional encodings interfere with token embeddings?&lt;/h2&gt;

&lt;p&gt;The attention operation at the core of modern transformer models is blind to the order of the embedding sequence it takes as input.
The operation cares only about the similarity (as computed via the attention mechanism) between two token embeddings in the sequence and is blind to their absolute and relative positions in the sequence (except for the causal mask, which we’ll ignore).
This positional information is important, though, so it’s typically given to the model via &lt;em&gt;positional encodings.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Under this scheme, after the input sequence is tokenized and mapped to initial token embeddings, a unique vector representing a token’s index in the sequence is added to each embedding.
This provides token position information to the downstream model.
Mathematically, if our input is a sequence of tokens ${x_0, x_1, x_2, \ldots }$, we first apply a token embedding map $E$ to get a sequence of vectors ${E(x_0), E(x_1), E(x_2), \ldots}$ and then add positional encoding vectors ${p_i}$ to each embedding vector to get
${E(x_0) + p_0, E(x_1) + p_1, E(x_2) + p_2, \ldots}$.
The result is a sequence of embedding vectors such that each vector $e_i = E(x_i) + p_i$ is aware of both its input token $x_i$ and its index $i$.
This sequence is then passed on to the rest of the transformer.&lt;/p&gt;

&lt;p&gt;Thinking through the vector math here, the choice to simply add positional encodings to the token embeddings seems surprising.
I’d naively expect these vectors to interfere with each other!
We’d ideally like to be able to easily resolve both the token embedding and its position, but by adding the corresponding vectors, we get some superposition that sort of muddies them together.&lt;sup id=&quot;fnref:b&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:b&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
This is weird because you’d imagine the downstream circuitry will want to be able to resolve the two.&lt;/p&gt;

&lt;p&gt;This wouldn’t be a problem if the operation were vector concatenation instead of addition.
Equivalently, this wouldn’t be a problem if we were knew that the token embeddings and positional encodings lie in roughly orthogonal subspaces, so they won’t interfere.
This seemed like a reasonable enough hypothesis that I looked at GPT-2’s encoding to check, and it turns out that this is basically what’s happened — the embeddings and encodings learn to lie in roughly orthogonal subspaces!&lt;/p&gt;

&lt;h2 id=&quot;claim-positional-encodings-perp-token-embeddings&quot;&gt;Claim: [positional encodings] $\perp$ [token embeddings]&lt;/h2&gt;

&lt;p&gt;Here I will present numerical evidence that the positional encodings and token embeddings lie in roughly orthogonal subspaces.
I’ll first plot the singular values of both embedding matrices, which will show that the positional encodings concentrate in a pretty low-dim subspace, then show that this subspace is roughly orthogonal to the space of the token embeddings.&lt;/p&gt;

&lt;p&gt;Let’s fix some notation.
GPT-2 has an embedding dimension of $d_\text{model} = 768$, a vocabulary size of $d_\text{vocab} \approx 50 \times 10^3$, and a max context window of size $d_\text{context} = 1024$.
We’ll denote the token embedding matrix by $\mathbf{E} \in \mathbb{R}^{d_\text{vocab} \times d_\text{model}}$ and the positional encoding matrix by $\mathbf{P} \in \mathbb{R}^{d_\text{context} \times d_\text{model}}$.
Here are the singular values of these two matrices:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/gpt2_pos_encs/P_and_E_svals.png&quot; width=&quot;75%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The most important observation here is that $\mathbf{P}$ has fairly low rank: it’s very well captured by its top, say, 30 singular directions.
The embeddings, by contrast, have fairly high rank: the mass of $\mathbf{E}$ is spread over many singular directions.&lt;/p&gt;

&lt;p&gt;We will now show that the top directions of the right singular subspace of $\mathbf{P}$ (which capture most of its mass) align well with the &lt;em&gt;bottom&lt;/em&gt; directions of the right singular subspace of $\mathbf{E}$, which permits them to not interfere with each other.
Let the right singular vectors of $\mathbf{E}$ be $v_j$ for $j = 1, \ldots, d_\text{model}$, and let \(\Pi^{(30)}_\mathbf{P}\) be the projector onto the top 30 right singular directions of $\mathbf{P}$. The plot below shows \(v_j^\top \Pi^{(30)}_\mathbf{P} v_j\).
This quantity lies in $[0,1]$ for each index $j$ and tells us the degree to which each &lt;em&gt;embedding&lt;/em&gt; singular direction is captured by the top singular directions of the &lt;em&gt;positional encoding&lt;/em&gt; matrix.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/gpt2_pos_encs/tokemb_posenc_capture_plot.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The large mass towards the right of the above plot shows that the top subspace of the positional encodings is well-aligned with the bottom subspace of the token embeddings.
Phrased differently, the positional encodings and token embeddings lie in roughly orthogonal subspaces, and so they won’t interfere!&lt;/p&gt;

&lt;p&gt;Curiously, the above plot also shows a small mass at the low indices.
This is strange to me, and I don’t know what to make of it.
Feel free to send me hypotheses!&lt;/p&gt;

&lt;p&gt;Here’s another, more detailed viz of the same phenomenon.
Here I simply plot the squared overlaps of the right singular vectors of $\mathbf{P}$ and $\mathbf{E}$:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/gpt2_pos_encs/tokemb_posenc_overlap_heatmap.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;I’ve applied a small Gaussian blur to the image in order to denoise a bit and bring out the essential feature: the large masses in the upper-right and bottom-right corners, which demonstrates that &lt;strong&gt;the&lt;/strong&gt; &lt;strong&gt;top positional encoding subspace is aligned to the bottom token embedding subspace and vice versa.&lt;/strong&gt;&lt;sup id=&quot;fnref:q&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:q&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;claim-query-key-value-and-post-value-projection-weights-are-attuned-to-the-top-subspaces-of-both-mathbfp-and-mathbfe&quot;&gt;Claim: query, key, value, and “post-value projection” weights are attuned to the top subspaces of both \(\mathbf{P}\) and \(\mathbf{E}\)&lt;/h2&gt;

&lt;p&gt;We’ve shown that GPT-2’s positional encodings and token embeddings lie in roughly orthogonal subspaces.
This raises a natural question: can we see alignment to these subspaces in the weight matrices of the transformer?&lt;/p&gt;

&lt;p&gt;It turns out we can: the first-layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;value&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proj&lt;/code&gt;&lt;sup id=&quot;fnref:c&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:c&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; weights are far more aligned to the top subspaces of both \(\mathbf{P}\) and \(\mathbf{E}\) than one would expect from chance.
The following big eight-panel plot which shows the squared singular value overlap between each of these four weight matrices (taken from the first attention layer) and both of these two embedding matrices.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/gpt2_pos_encs/eight_panel_plot.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This plot shows that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt; matrices are well-aligned to the very top subspace of \(\mathbf{P}\) (note the dark dot in the very top left of the top left two plots – you might have to zoom in).&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;value&lt;/code&gt; matrices are well-aligned to the top subspaces of \(\mathbf{E}\). The corresponding plots have large mass in the top left.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proj&lt;/code&gt; matrix is well-aligned with \(\mathbf{P}\) and apparently anti-aligned to \(\mathbf{P}\)!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One could come up with various just-so stories here — for example, that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proj&lt;/code&gt; matrix is spreading around positional information — but I’ll refrain. The important takeaways are that these matrices are clearly aware of these two important subspaces and that the top few positional encoding directions are used quite a lot in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;query&lt;/code&gt;-&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key&lt;/code&gt; similary computation, which makes sense. These seem like useful things to keep in mind when, for example, trying to reverse-engineer transformer circuitry.&lt;/p&gt;

&lt;h2 id=&quot;claim-mlp-weights-perp-positional-encodings&quot;&gt;Claim: [MLP weights] $\perp$ [positional encodings]&lt;/h2&gt;

&lt;p&gt;Here I will present evidence that the fan-out weights of the first-hidden-layer MLP of GPT-2 have learned to be &lt;em&gt;sensitive&lt;/em&gt; to the top directions of the token embeddings and &lt;em&gt;insensitive&lt;/em&gt; to the top directions of the positional encodings.&lt;/p&gt;

&lt;p&gt;The plot below shows the squared overlaps between the right singular vectors of the MLP weights \(\mathbf{W}_1 \in \mathbb{R}^{d_\text{hid} \times d_\text{model}}\) (with \(d_\text{hid} = 4 d_\text{model}\)) and the positional encoding and token embedding matrices \(\mathbf{P}\) and \(\mathbf{E}\).
We see that the MLP is strongly attuned to the top token embedding directions (the first heatmap has most of its mass along the diagonal) and strongly insensitive to the positional encoding directions (the second heatmap has most of its mass along the antidiagonal).
This makes sense: the MLP basically acts the same on each token embedding, independently of its position in the sequence.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/gpt2_pos_encs/mlp_overlap_heatmaps.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;claim-the-positional-encoding-subspace-is-sparse&quot;&gt;Claim: the positional encoding subspace is sparse!&lt;/h2&gt;

&lt;p&gt;Since the top singular subspace of \(\mathbf{P}\) seems to be important, I decided to visualize it.
To my surprise, it’s sparse in the embedding space!&lt;/p&gt;

&lt;p&gt;Recall that \(\mathbf{P}\) has shape \([d_\text{context} \times d_\text{model}]\), with \(d_\text{context} = 1024\) and \(d_\text{model} = 784\). Let us visualize its top five singular vectors of \(\mathbf{P}\) in these two spaces.
The left singular vectors live in the context space, and the right singular vectors live in the embedding space.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/gpt2_pos_encs/pos_enc_top_vecs.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As we might guess, the top singular vectors in the context (i.e. token) space are basically sinusoids of different frequencies… but the top singular vectors in the embedding space are sparse, concentrating strongly on maybe 40 indices.
Returning to our original motivation of understanding how the positional encodings don’t interfere with the token embeddings, it seems that not only do they lie in orthogonal subspaces, they actually make use of (roughly) &lt;em&gt;disjoint sets of embedding indices!&lt;/em&gt; This is just a permutation away from the positional encodings being &lt;em&gt;appended&lt;/em&gt; to the token embeddings rather than added, which makes a lot of sense.&lt;/p&gt;

&lt;p&gt;That said, I have no idea why we get sparsification here.
I don’t know what induces it; I don’t know why the model prefers this over merely orthogonal subspaces.
Odd!
It’s a strong enough effect that I’d bet there’s a simple explanation.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;This blogpost describes some purposeful poking around the singular value structure of GPT-2’s positional encoding and token embedding matrices.
The main observation – that the these two types of information lie in basically orthogonal subspaces, and this is reflected in intuitive ways in downstream weight matrices – seems pretty solid!
There are a bunch of auxiliary observations, including the sparsity of the positional encodings, that seem pretty clear but which I don’t have a good explanation for.
In any case, it seems useful to compile heuristic, broad-strokes observations like this when trying to build intuition for how transformers work.
Observations like this feel to me like puzzle pieces, and if we get enough on the table, maybe we can start fitting them together.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Thanks to Chandan Singh for proofreading this post and for the discussion that led to it. A Colab notebook reproducing all experiments is &lt;a href=&quot;https://colab.research.google.com/drive/1CuNdE2BOdHMZQAoYXiAC0C21j-yRA8BR?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Two disclaimers here: first, this was written rather fast and some parts might be unclear, and second, I imagine someone out there already knows this! In either case, feel free to drop me a line. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:b&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For example, if you don’t choose the vectors carefully, you might find that the word &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CAT&lt;/code&gt; at index &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3&lt;/code&gt; is embedded similarly to the word &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;APPLE&lt;/code&gt; at index &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;6&lt;/code&gt; — or, mathematically, that $E(\text{CAT}) + p_3 \approx E(\text{APPLE}) + p_6$ — making them hard to distinguish. &lt;a href=&quot;#fnref:b&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:q&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I often bump into the problem of how to best measure or visualize the alignment of two matrices. This very info-dense SV-overlap plot is something I hadn’t tried before. I find it somewhat useful and will probably use it in the future. &lt;a href=&quot;#fnref:q&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:c&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;By this I refer to the “projection matrix” from the output of the attention operation back into the residual stream. This isn’t really a projection in any conventional sense, since it’s square – seems better thought of as simply a linear transformation – but that’s what people call it, so we’ll do so here. &lt;a href=&quot;#fnref:c&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 20 Aug 2024 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/gpt2-positional-encs/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/gpt2-positional-encs/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>Understanding fractals from iterated maps</title>
        <description>&lt;p&gt;&lt;em&gt;In this post, I give some mechanistic intuition for why and how the basins of iterated maps form fractals which I feel is usually missing from academic treatments of the subject.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It is a remarkable fact of mathematics that simple dynamical systems can display immensely complex behavior.
The poster children of this notion are fractals generated from iterated maps.
You have likely seen the &lt;a href=&quot;https://math.hws.edu/eck/js/mandelbrot/MB.html&quot;&gt;Mandelbrot set&lt;/a&gt;, the most famous such fractal.
Here is a related &lt;em&gt;Julia set&lt;/em&gt; which is slightly simpler to define:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-6&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/julia_set.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The rule by which this stunning image is generated is remarkably simple.
This plot represents the complex plane, and each point \((x, y)\) in the image represents the complex number $z_0 = x + y i$.
For each such point $z_0$, we compute $z_1$, $z_2$, and so on using the iterated map&lt;/p&gt;

\[z_{t + 1} = f(z_{t+1}) = z_t^2 + c,\]

&lt;p&gt;where \(c \approx 0.2883 + 0.5383 i\) is a parameter I have tuned.
This either eventually blows up (with \(|z|\) getting very big) or doesn’t.
If $|z|$ blows up, the pixel at $z_0$ is colored white, and if it remains bounded, it is colored black.
The result is the stunning fractal above.
You can explore different values of $c$ — which generate surprisingly varied and wondrous Julia sets — &lt;a href=&quot;https://www.marksmath.org/visualization/julia_sets/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;where-does-all-this-detail-come-from&quot;&gt;Where does all this detail come from?&lt;/h2&gt;

&lt;p&gt;Part of the amazement of images like the above is that our dynamical process was extremely simple to define, but the resulting visualization is quite complicated — infinitely complicated, in fact, or at least infinitely detailed!
It gives me (and others, I suspect) the feeling of having somehow “gotten more out than we put in”.
It simply does not feel like this dynamical map should be complex enough to generate this fractal!&lt;/p&gt;

&lt;p&gt;The answer to this seeming paradox is that we are iterating the dynamical map many times: the level of detail results not from the complexity of the map but rather from the amount of computation we expend repeatedly applying it.
This can be beautifully illustrated by visualizing the result of applying only finitely many iterations, using shades of grey to indicate how may iterations a point takes to blow up:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-6&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/julia_set.gif&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The complexity of these fractals is built up over many iterations.&lt;/p&gt;

&lt;h2 id=&quot;why-do-you-get-a-fractal&quot;&gt;Why do you get a fractal?&lt;/h2&gt;

&lt;p&gt;There is a very basic question one can ask here that is virtually never addressed in introductory treatments of chaotic maps:&lt;sup id=&quot;fnref:c&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:c&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; why do we get a fractal?
This question is so obvious that, if you’ve already seen this stuff, it’s kind of hard to even see as legitimate: why does the “escape region” of \(f\) take the shape of a fractal?
Like, why not some other, non-self-similar shape?
Where does the fractal come from?
What property of the map $f$ is responsible for the self-similarity?&lt;/p&gt;

&lt;p&gt;The usual fact that &lt;em&gt;is&lt;/em&gt; given is that these dynamical maps are chaotic.
For example, students in a college course might compute the system’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Lyapunov_exponent&quot;&gt;Lyapunov exponent&lt;/a&gt; and see that it is positive around the boundary of the fractal, implying the system displays the sensitivity to initial conditions characteristic of chaotic dynamics.
Okay, great, it’s believable that these maps are chaotic, but where does this &lt;em&gt;fractal&lt;/em&gt; come from?
The two concepts are certainly related, but how exactly?&lt;/p&gt;

&lt;p&gt;I gnawed on this question sporadically for many years, and I finally have what feels like an intuitive understanding.
The main purpose of this blogpost is to convey that (high-level, nonrigorous) intuition.
The reason is basically that, if you run the dynamical map forwards, you will find that…&lt;/p&gt;

&lt;h3 id=&quot;distinctive-regions-of-the-fractal-get-mapped-to-larger-versions-of-themselves&quot;&gt;Distinctive regions of the fractal get mapped to larger versions of themselves&lt;/h3&gt;

&lt;p&gt;This property is easier to show than to tell.
Pick a small lobe in the above fractal — say, the little red lobe in the graphic below.
If you look around the fractal, you will be able to find lots and lots of little lobes, both bigger and smaller, that have the same shape.
Now pick your favorite point in the red lobe.
Upon the action of the map $f$, this point will generally be mapped to another point at the same position in a larger version of the same lobe!
In the diagram below, an initial point in the red lobe gets mapped to a point in the orange lobe, then the yellow lobe, then the green, and so on.&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-6&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/lobe_map.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Here’s the same thing with a different starting lobe:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-6&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/lobe_map_2.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;To underscore the first example, here’s an actual computer-generated plot in which the region enclosed by each colored circle is mapped to the region enclosed by the subsequent one:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-6&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/circle_map.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-3&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Note how each successive circle here is larger than the one before it, showing that the map is expansive, and the Lyapunov exponents here are positive.&lt;/p&gt;

&lt;p&gt;This behavior is enough to explain why the final Julia set contains so many versions of the same lobe.
This follows from basically an inductive argument.
I will give the argument first and then follow it up with some graphical illustration.&lt;/p&gt;

&lt;p&gt;Note first that &lt;em&gt;any point that is mapped to a black point will also be a black point&lt;/em&gt; – the two points lie on the same trajectory, so they have the same fate (to diverge or not to diverge).
As the base case, assume there is some black region with an interesting shape.
As the inductive step, now, suppose that some other, smaller volume of space gets mapped to a volume of space including this black region.
There will be a shrunken copy of the black region in this other volume — the black region has been duplicated!
Now we can run this process again and again, positing a &lt;em&gt;third,&lt;/em&gt; even smaller volume of space that maps to the second, and so on and so on.
(Fairly simple mapping functions $f(z)$ can give you an infinite sequence of volumes mapping to each other like this; note that the volumes can overlap.
We will give a toy example illustrating this soon.)
We now have an infinite number of copies of the original black region, getting ever smaller and more delicate — a fractal!&lt;/p&gt;

&lt;p&gt;This really becomes much more tangible when you play around with yourself with an eye to this behavior.
You can explore this fractal in-browser at (this nice website)[https://www.marksmath.org/visualization/julia_sets/].&lt;/p&gt;

&lt;p&gt;As a final speculation before we use this notion to make some fractals, I wonder if there’s some sense in which getting a fractal is inevitable in the Julia set of a chaotic iterated map.
Surely the set will be some chaotic, extremely complicated shape… and it vaguely seems to me like, because the generating function is some fixed function with some finite amount of information required to specify it, it couldn’t possibly result in a chaotic shape which &lt;em&gt;isn’t&lt;/em&gt; a fractal — that is, for which each lower level of detail is unlike the higher levels — because that would entail an infinite amount of information to specify.
Perhaps there are connections to computational complexity in here, where the function is fixed and has $O(1)$ complexity, but the fractal naively has “geometric complexity” $O(T)$, where $T$ is the number of iterations the map is run for, and somehow the self-similarity of the fractal lets one come up with a new notion of geometric complexity for which these Julia sets only have $O(1)$ complexity.
This would feel like a satisfying resolution to the motivating “paradox” that it felt like we were getting more out of these systems than we put in.&lt;/p&gt;

&lt;h2 id=&quot;hand-designing-some-fractals&quot;&gt;Hand-designing some fractals&lt;/h2&gt;

&lt;p&gt;If you really understand how something works, you should be able to make one yourself.
I’m claiming that the ingredients for self-similar basins of attraction from an iterated map are basically (a) expansive behavior and (b) an infinite sequence of volumes that map into each other, leading ultimately to some interesting basin boundary.
The easiest way to satisfy both (a) and (b) is perhaps to have some region $\mathcal{R}$ that maps to a larger region $\mathcal{R}’$ that contains both some basin boundary &lt;em&gt;and the original region $\mathcal{R}$ itself.&lt;/em&gt;
This pretty easily gives an infinite sequence of volumes (and a resulting fractal) because, well, the basin structure within $\mathcal{R}’$ has to contain a complete copy of itself, so self-similar structure is inevitable!&lt;/p&gt;

&lt;h3 id=&quot;first-example-circles-in-2d&quot;&gt;First example: circles in 2D&lt;/h3&gt;

&lt;p&gt;Here’s an example.
We will continue to work in the complex plane and use the map&lt;/p&gt;

\[z_{t+1} = c z_t,\]

&lt;p&gt;where $c = 2$ for now.
I will then draw a circle somewhere in the plane.
If a point $z_0$ ever lands inside the circle after some number of iterations, we decree that it may never leave, and we color the pixel at $z_0$ black.
If it never lands in the circle, the pixel at $z_0$ is white.&lt;/p&gt;

&lt;p&gt;Here’s what you get:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/circles_1.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Here I have chosen the “sticky circle” (the largest circle) to have radius $0.4$, centered at $z_c = 1 + i$.
The red crosshair shows the origin.
Note that we get a fractal!&lt;/p&gt;

&lt;p&gt;We can make it more interesting if we instead add some rotation with, say, $c = 1 + 0.1 i$:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/circles_2.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;second-example-chaotic-map-in-1d&quot;&gt;Second example: chaotic map in 1D&lt;/h3&gt;

&lt;p&gt;Let’s design another one.
This one will be only 1D.
Consider the following map $z_t \mapsto z_{t+1}$:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/1d_map_one_iteration.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Try to guess what this map will do when iterated many times.
It’s not too hard to see that the points on the far left and far right will continue to flow all the way to the edges, so we have $0$ and $1$ as basins of attraction…
and I have designed the middle region so that parts of it map into these two basins (the peak and trough), and other parts &lt;em&gt;map back into the whole middle region,&lt;/em&gt; guaranteeing that any basin structure is repeated and making the basins structure self-similar.
Here’s what this map looks like after being applied seven times:&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
			&lt;p style=&quot;text-align:center;&quot;&gt;
				&lt;img src=&quot;/img/fractals/1d_map_iterated.png&quot; style=&quot;width: 100%&quot; /&gt;
			&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This structure is self-similar (which I verified by zooming in).&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In this post, I’ve aimed to give some mechanistic insight into why and how the basins of attraction of iterated maps give fractals.
There’s a huge raft of technical content I could have brought in here but didn’t — if you’re interested, I’d encourage you to check out Lyapunov exponents, the logstic map, the Mandelbrot set, and perhaps Steven Strogatz’ &lt;em&gt;Nonlinear Dynamics and Chaos.&lt;/em&gt;
I had never really felt I could see the genesis of this class of fractals before, but now I can, so I hope this sheds some light for some other folks, too!&lt;/p&gt;

&lt;p&gt;A major takeaway I have here, which I didn’t fully appreciate before, is that in systems like these, chaos is usually studied as a &lt;em&gt;local&lt;/em&gt; property — e.g., the Lyapunov exponent is positive, and it’s locally defined — but these fractals are &lt;em&gt;global.&lt;/em&gt;
I think this probably explains why I hadn’t encountered a good explanation like this before: it seems hard to reduce the global property of fractal-formation to a property of the return map that’s simple enough to prove a theorem about.
I wonder if there’s some analytical condition on the return map that one could define that’d be necessary and sufficient for it to give you fractals.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:c&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This has never been addressed in the maybe five times I’ve seen this material, that is – but tell me if you know a good treatment somewhere! &lt;a href=&quot;#fnref:c&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 31 Jul 2024 12:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/fractals-in-iterated-maps/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/fractals-in-iterated-maps/</guid>
        
        
        <category>fun-science</category>
        
      </item>
    
  </channel>
</rss>
