<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JS</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 14 Apr 2024 11:11:48 -0700</pubDate>
    <lastBuildDate>Sun, 14 Apr 2024 11:11:48 -0700</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Roadtripping to North Dakota</title>
        <description>&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/hover_effects.css&quot; /&gt;

&lt;p&gt;A few weeks ago, I did a solo roadtrip, spending two weeks driving a curving path from Nashville, TN to Bismarck, ND to go to every state I’d never been to. This was a deeply satisfying and rewarding experience – I got to drink deeply of solitude and nature and freedom, got to practice living on the road, and hit 50 states just before my father and brother, who were at 48 and 49 respectively and didn’t know I was doing this. Here I’ll sketch my journey and conclude with a few things I’m taking away from this journey.&lt;/p&gt;

&lt;h2 id=&quot;the-route&quot;&gt;The route&lt;/h2&gt;

&lt;p&gt;I had eight states to knock off: Georgia, Kentucky, Arkansas, Oklahoma, Kansas, Missouri, Nebraska, and North Dakota. I’d been picking off stragglers for years, leaving myself a big mostly-contiguous chunk in the midwest ripe for a high-value roadtrip. I viewed this as a more extreme version of how, when playing Tetris, one tends to carefully preserve a vertical gap for a long piece and then knocks out four rows at once.&lt;/p&gt;

&lt;p&gt;The one fixed pin of this trip – and the reason I did it when I did – was an engagement party one weekend in Nashville, so I decided to plan a long layover in Atlanta on my way there and then drive everywhere else. The final route looks like a path you’d only travel if you were trying to drive through a very specific sequence of states:&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/states_roadtrip/route_map.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;some-logistics&quot;&gt;Some logistics&lt;/h2&gt;

&lt;p&gt;I lived out of a 2024 Hyundai Elantra for these two weeks. I’d actually rented the cheapest car available, but when I got to the lot, the rental agent told me he “wouldn’t do me dirty like that” and instead gave me a stupidly nice brand new car that, thanks to automated lanekeeping, basically drives itself on the highway.&lt;/p&gt;

&lt;p&gt;I stayed in a few Airbnbs and a motel, but I greatly preferred my nights in the woods. As it turns out, dispersed camping is allowed in national forests (which are nicely visualized on &lt;a href=&quot;https://www.fs.usda.gov/ivm/&quot;&gt;this wonderful map&lt;/a&gt;), and it’s somehow quite freeing to drive into a big wild forest and get to camp basically anywhere, picking an arbitrary dirt road and pitching camp too deep in the woods to be likely to see another soul.&lt;/p&gt;

&lt;p&gt;I set out from Nashville on the morning of March 17th. I’ll flash some scenes from the trip. Mouse over the photos for vignettes!&lt;/p&gt;

&lt;style&gt;
.hover-container {
  position: relative;
  width: 100%;
}

.image {
  display: block;
  width: 100%;
  height: auto;
}

.overlay {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  right: 0;
  height: 100%;
  width: 100%;
  opacity: 0;
  transition: .1s ease;
  background-color: #000000;
}

.hover-container:hover .overlay {
  opacity: .8;
}

.text {
  color: white;
  font-size: 12px;
  position: absolute;
  top: 10%; /* Closer to the top edge */
  left: 10%; /* Closer to the left edge */
  transform: translate(-5%, -5%); /* Adjust these translate values to fine-tune the position */
  text-align: center;
}
&lt;/style&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-2&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/atlanta.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;In Atlanta, I took public transit into the city and rested in Centennial Park. The air was warm. I chatted briefly with a few homeless folks but mostly kept to myself.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/mammoth_caves.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;To check off Kentucky, I went to Mammoth Caves with a friend of friends from the engagement party. He turned out to be a mathematician, and we knocked some math puzzles back and forth as we walked around the largest cave in the world.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/memphis_pyramid.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;This ridiculous Bass Pro Shops pyramid stands in Memphis. Like the greater pyramids of Egypt, this structure sports four triangular sides, various anti-robbery mechanisms, and an assortment of dead animals, and is fascinating to anthropologists. Instead of mummies, however, this modern pyramid only contains old people.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/arkansas_2.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Ironically, I found myself unintentionally on the path of totality of the 2024 solar eclipse just a few weeks too early, and I’d drive far away and then do a whole second roadtrip back to the path from California.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/kansas_3.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Kansas was expansive, calm, and flat. As I drove north from Arkanasas, I could see the land change, quickly turning from rocky forested hills to flatter Southern forest to plains, all in the span of an hour or two.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/kansas_1.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;This Kansas town had a classic middle-America look that'd seem at home in the 1950's to me. I slept in Kansas City, Missouri that evening.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/sioux_falls.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I stayed a few days with Matthew Schallenkamp, a friend and coworker, in Sioux Falls, SD, working on a coding project and seeing the town. Remarkably for a city of a quarter million people, Sioux Falls has a huge set of rapids rolling through the middle of town.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/devils_tower.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Matthew joined me on a trip west to Devils Tower. What a weird formation.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/nashville.gif&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;In Nashville, I athletically somersaulted onto a mechanical bull.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/arkansas_1.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I spent two nights in the Arkansas Ozarks. It’s beautiful hill country.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/oklahoma.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;To me, Oklahoma consists of Tiger King, some associations with panhandles, and this hybrid gas station/casino.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/kansas_2.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I stopped for lunch by a pond.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/nebraska.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Nebraska allegedly has some cool buttes and bluffs off on its western side (actually, all the plains states seem to get cooler as you go west), but I skirted its eastern border, following the Missouri River, camping on its banks.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/badlands.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I love the Badlands. They’re so cool. I hadn’t been since I was a child, and I hadn’t remembered that these wonderful formations are just dirt, not rock. They’re so fragile — one determined person with a shovel could probably really change the view! In line with this, it’s a pretty geologically young place, with erosion starting only some 500k years ago.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/north_dakota.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I sent this picture to my brother and father after I told them I had some news.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;div class=&quot;col-2&quot;&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;some-lessons&quot;&gt;Some lessons&lt;/h2&gt;

&lt;p&gt;This trip afforded me ample time for reflection and coincided with a more extended personal journey I’ve been on for the past year in which I’ve been figuring out how to live as an adult in ways feel good. I learned a number of things about roadtripping, including that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it’s very useful to run a tight ship logistically, taking care of problems as soon as they arise and keeping gear quite organized. I never had any major logistical problems, which is unusual for me, and it’s because I gave problems my attention when they were still small.&lt;/li&gt;
  &lt;li&gt;Talking to strangers is a great joy of travel, and a good way to quickly get the feel of a place. Unplanned conversations were highlights of the trip. (See &lt;a href=&quot;&quot;&gt;Why I Talk To Strangers&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Driving on snow is way harder than I expected.&lt;/li&gt;
  &lt;li&gt;The land is big, but it’s fathomably big. Taking a plane from coast to coast gives little sense of how much land there is since you really have no physical feeling of how fast you’re traveling. Driving across the heartland gave me a real sense of how big the land is: if I drive for an hour or two or three, the character of the land will differ – hills turn to forest, forest turns to grassland, grassland to plains, to prairie, to badlands – and if I go for ten or twenty of these changes I’ll go from one side of the country to the other. America feels like a much more tangible thing to me now.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And, even more valuable, some broader life lessons and realizations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A career has to be fun to be fulfilling. No matter how important I feel science is, and how convinced I am that I could make a valuable contribution, it’s not worth doing if it’s not fun.&lt;/li&gt;
  &lt;li&gt;Various realizations about software engineering thanks to Matthew, including that it’s a skill I simply don’t have but could develop.&lt;/li&gt;
  &lt;li&gt;I’m kinda lonely! This particularly solitary endeavor clarified for me that &lt;em&gt;most&lt;/em&gt; of my endeavors are actually solitary, and I want more collaboration and closeness with the people around me.&lt;/li&gt;
  &lt;li&gt;Nature is good medicine, and I need more of it in my life.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- divider above footnotes --&gt;
&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is the route I actually followed, not the route I’d planned, which was more stripped down, going north from Sioux Falls to Fargo to finish. I added on the big excursion to go see nature in the western Dakotas halfway through, which turned out to be a great idea. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 14 Apr 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/fifty-states-roadtrip/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/fifty-states-roadtrip/</guid>
        
        
        <category>random, personal</category>
        
      </item>
    
      <item>
        <title>Why I talk to strangers</title>
        <description>&lt;p&gt;i met someone today&lt;br /&gt;
and in the course of conversation&lt;br /&gt;
told her about you&lt;br /&gt;
and your love of people&lt;br /&gt;
and what I learned&lt;br /&gt;
and how it changed me&lt;/p&gt;

&lt;p&gt;and she said:&lt;br /&gt;
that sounds good and all&lt;br /&gt;
but i don’t see the point&lt;br /&gt;
in connecting with someone&lt;br /&gt;
i will never see again.&lt;/p&gt;

&lt;p&gt;and so i thought for a moment&lt;br /&gt;
and i said&lt;br /&gt;
when you look at a flower&lt;br /&gt;
do you fret&lt;br /&gt;
that your examination is a waste of time&lt;br /&gt;
because you will never see that flower again?&lt;/p&gt;

&lt;p&gt;and in any case&lt;br /&gt;
the things you see in that flower&lt;br /&gt;
the details you notice in its petals, its leaves, its shape, its hue&lt;br /&gt;
you will surely see in other flowers&lt;br /&gt;
and to look at one is in some sense to look at them all&lt;br /&gt;
and teaches you to better see&lt;br /&gt;
each future flower.&lt;/p&gt;

&lt;p&gt;returning to people&lt;br /&gt;
all you see in a stranger&lt;br /&gt;
you will surely see again in countless others –&lt;br /&gt;
strangers, friends, and loved ones –&lt;br /&gt;
and learning about a stranger&lt;br /&gt;
is in fact learning about humanity,&lt;br /&gt;
each individual a view of the whole&lt;br /&gt;
and so infinitely more resplendent&lt;br /&gt;
if you know that secret.&lt;/p&gt;

&lt;p&gt;but it goes deeper&lt;br /&gt;
because unlike with flowers,&lt;br /&gt;
here you are that type of thing&lt;br /&gt;
and you can learn about yourself&lt;br /&gt;
in a subway chat with a stranger&lt;br /&gt;
whose name you do not know&lt;br /&gt;
and whose path you do not share.&lt;br /&gt;
you may see yourself reflected in their eyes,&lt;br /&gt;
see a life you might have lived,&lt;br /&gt;
or a joy you have forgotten,&lt;br /&gt;
or a struggle you have not known but will soon.&lt;br /&gt;
humans are not so different&lt;br /&gt;
and i see now that each gives a view of me&lt;br /&gt;
and that, i said,&lt;br /&gt;
is why i talk to strangers.&lt;/p&gt;

&lt;p&gt;our conversation ran its course&lt;br /&gt;
and we went our separate ways&lt;br /&gt;
drifting back to our separate lives&lt;br /&gt;
and i may well never see her again&lt;br /&gt;
but that, of course, isn’t the point&lt;br /&gt;
and i am glowing regardless&lt;br /&gt;
with the feeling&lt;br /&gt;
of having caught&lt;br /&gt;
a glimpse of humanity&lt;/p&gt;
</description>
        <pubDate>Fri, 15 Mar 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/why-i-talk-to-strangers/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/why-i-talk-to-strangers/</guid>
        
        
        <category>poetry, personal</category>
        
      </item>
    
      <item>
        <title>Creating and erasing AI watermarks</title>
        <description>&lt;p&gt;I’ve been sitting in on many of the guest lectures for &lt;a href=&quot;https://rdi.berkeley.edu/understanding_llms/s24&quot;&gt;CS 294&lt;/a&gt;, “Understanding LLMs.” To my excitement, yesterday’s speaker was &lt;a href=&quot;https://www.boazbarak.org/&quot;&gt;Boaz Barak&lt;/a&gt;, a Harvard professor who studies theoretical CS and machine learning foundations and whose work and lectures I often enjoy. He spoke about LLM watermarking schemes, and in particular a scheme for breaking more or less arbitrary watermarks. In this post, I’ll give a brief overview of the problem and their solution and offer a synthesis that, to me, gives a succinct way to think about what’s going on here.&lt;/p&gt;

&lt;h2 id=&quot;ai-watermarking&quot;&gt;AI “watermarking”&lt;/h2&gt;

&lt;p&gt;There are many settings in which we might like to ensure that some artifact is &lt;em&gt;not&lt;/em&gt; generated by an AI model — for example, ensuring that photos came from cameras and not diffusion models or that a homework assignment was written by a student and not an LLM. As generative models get better, it’s going to continue to get harder to make this distinction. The problem statement here is basically: “how can we modify generative AI systems so that essentially all generated content can later be identified as AI-generated, in a way that (a) doesn’t make the model’s outputs significantly worse and (b) can’t be removed by a simple transformation of the output?”&lt;/p&gt;

&lt;p&gt;To get a sense for what the landscape looks like here, it’s helpful to imagine some solutions. One naive solution for watermarking LLMs in particular might be: have the LLM only output paragraphs whose characters hash to strings ending in, say, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;. This could be achieved by just regenerating each paragraph until it satisfies this condition. A page of text generated by this model is going to be easily identifiable as such — you just compute the hash of each paragraph and see only ending &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;s, which would be unlikely for any text that wasn’t generated under that constraint — but it’s also very fragile: to break this condition, it’s enough to change one word in each paragraph.&lt;/p&gt;

&lt;p&gt;Fortunately, there are better ideas: a handful of papers, including one by &lt;a href=&quot;https://arxiv.org/abs/2301.10226&quot;&gt;a team from UMD&lt;/a&gt;, propose that an LLM might instead write with a statistical bias that applies at every word, but which is randomized enough to be undetectable to a human — for example, using the most recent token to seed a RNG that splits the dictionary into a “good” and a “bad” half, and weakly biasing the model to sample the &lt;em&gt;next&lt;/em&gt; token from the “good” half. This results in human-readable text which can nonetheless be identified as AI-generated with high statistical power (by just running back the “good-list” algorithm), and this “watermark” can’t be removed without changing a &lt;em&gt;lot&lt;/em&gt; of the tokens.&lt;/p&gt;

&lt;h2 id=&quot;a-generic-black-box-attack-on-ai-watermarks&quot;&gt;A generic black-box attack on AI watermarks&lt;/h2&gt;

&lt;p&gt;A watermark on AI text is only good if it is hard to remove. In particular, there shouldn’t exist an “attack” process which (1) is easy to compute and (2) transforms “watermarked” text $T$ into “unwatermarked” text $T’$, where $T$ and $T’$ are equally good answers to the original prompt.&lt;/p&gt;

&lt;p&gt;Remarkably, Boaz + coauthors &lt;a href=&quot;https://arxiv.org/abs/2311.04378&quot;&gt;propose a generic attack satisfying (1,2)&lt;/a&gt;. I’ll quickly describe it before offering a synthesis.&lt;/p&gt;

&lt;p&gt;Their strategy is pretty simple: &lt;em&gt;just make random small changes to the text, and eventually you’ll have changed enough that any watermark will be destroyed.&lt;/em&gt; For example, if you replace every other word with a random synonym, that would be enough to scramble a watermark planted by the UMD team.&lt;/p&gt;

&lt;p&gt;…of course, swapping in &lt;em&gt;random synonyms&lt;/em&gt; would lead to terrible text, so the authors assume the attacker also has an “quality oracle” that can tell you whether you’ve made a swap that unintentionally made things worse. And of course swapping out single words is a bit limited, so sometimes you might want to rephrase bigger chunks of text, so the authors assume that instead of thesaurus, you’re using a “perturbation oracle” that gives random small modifications. In practice, both of these are language models: a powerful model to tell you if your text is worse, and a weak model to propose local modifications.&lt;/p&gt;

&lt;p&gt;Their main result is that, given both a quality oracle and a perturbation oracle, you can remove the watermark by &lt;em&gt;rejection sampling:&lt;/em&gt; at every step, use the perturbation oracle to generate a random modification, and then accept the change only if the quality oracle confirms that the change hasn’t made the overall text worse. This provably results in a &lt;em&gt;random walk around the space of high-quality responses,&lt;/em&gt; and since only a small fraction of these high-quality responses are “watermarked,” after you follow this random walk for a while, you’re going to reach the promised land of watermark-free essays for your English class. Here’s a beautiful figure from their paper illustrating this scheme:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/llm_watermarks/harvard_random_walk_attack.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;!-- &lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;  &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt; --&gt;

&lt;p&gt;The authors test their attack against several proposed watermarking schemes and find that it consistently works with little degradation in text quality.&lt;/p&gt;

&lt;p&gt;I really like this attack. The fact that such a naive search algorithm — a random walk with rejection sampling — works for this is beautiful (and appeals to me as a trained physicist!). The scheme does a great job of reminding you that watermarked samples are only a small fraction of all samples — indeed, they have to be to be a good watermark — and this very fact makes them fragile! Rarity and nonrobustness are two edges of the same sword.&lt;/p&gt;

&lt;p&gt;The one weakness is that it requires a lot of calls to a quality oracle, which in practice will be a fairly powerful language model, so this isn’t a very cheap attack. But then, if you imagine a near-future world in which AI calls are easily available but all AIs are watermarked, this attack works perfectly well!&lt;/p&gt;

&lt;p&gt;In all honesty, it doesn’t seem to me like breaking watermarking schemes would be particularly hard. For example, it seems like it’d fool the UMD watermarking scheme to just, say, ask the model to write the essay in Spanish and Google Translate it into English, or to generate the essay with a filler word between every content word. I really like this attack, though, not because it does something I thought was impossible, but because it works very generally and shines light on the landscape of attack and defense here.&lt;/p&gt;

&lt;h2 id=&quot;synthesis-bootstrapping-a-discriminator-into-a-generator&quot;&gt;Synthesis: bootstrapping a discriminator into a generator&lt;/h2&gt;

&lt;p&gt;To me, the deep idea behind this attack is that a “quality oracle” can be hooked up to a random process and used to generate high-quality samples — or, in the language of GANs, a “discriminator” can be bootstrapped into a “generator.” A system that merely &lt;em&gt;scores&lt;/em&gt; samples is in some sense almost as powerful as a system that generates them! An AI organization might &lt;em&gt;think&lt;/em&gt; it’s safe giving you only watermarked text and a powerful discriminator, but in fact that discriminator is enough to effectively regenerate the text.&lt;/p&gt;

&lt;p&gt;When can a discriminator be cheaply bootstrapped into a generator? &lt;strong&gt;In some sense, this is what every local optimization algorithm is doing: iterated queries to a loss function (the oracle) are converted into a &lt;em&gt;point&lt;/em&gt; with a low loss value (the generated sample)!&lt;/strong&gt; This “iterated bootstrapping of discriminators” is a motif we should be on the lookout for in optimization and learning generally.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;thought-experiment-how-far-can-this-bootstrapping-idea-go&quot;&gt;Thought experiment: how far can this bootstrapping idea go?&lt;/h2&gt;

&lt;p&gt;This de-watermarking attack effectively works on the principle that &lt;em&gt;a discriminator plus a source of random perturbations can be used as a generator.&lt;/em&gt; I wonder how far we can take this idea!&lt;/p&gt;

&lt;p&gt;Here’s a thought experiment which this principle suggests, and which is sort of the de-watermarking attack taken to the extreme:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Choose a prompt from which we’d like to generate some high-quality text.&lt;/li&gt;
  &lt;li&gt;Start with &lt;strong&gt;&lt;em&gt;totally random text&lt;/em&gt;&lt;/strong&gt; — just random tokens.&lt;/li&gt;
  &lt;li&gt;Make random perturbations and run rejection sampling as in this attack for a long time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Question: do you reach high-quality text after many iterations?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In some idealized mathematical setting, the answer &lt;em&gt;has&lt;/em&gt; to be yes: quality is only increasing as per the oracle, so eventually it will either reach some local maximum or cross an arbitrary threshold. However, this is a real interesting question in practice thanks to the fact that every component is flawed! &lt;em&gt;Is the oracle reliable enough? Are the perturbations accepted at a good enough rate? Do we reach the promised land of high-quality text in a &lt;strong&gt;reasonable amount of time?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I don’t know the answer here, but it seems like it’d be a really cool result if it worked! In particular, I’m curious how dumb the source of perturbations can be in order for it to work. It’d also be really cool to see how the text changes as the process runs — what does it look like, say, 10% of the way through? 25%? 75%? Does it first form good English sentences, then gradually get on topic, then finally become high quality? If it gets stuck, at what point in this progression?&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I’m pretty curious about this! It actually seems like an experiment that could be run by some entity with a lot of compute.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In a meta sense, this is also fundamentally how reinforcement learning works: the learning algorithm converts task feedback scores (the oracle) into a system that can generate new good actions (generated samples). (This process is more expensive than schemes that generate only one sample, but once you’re done you have a system for cheaply generating lots of samples.) It’s not totally clear to me how to relate this meta-bootstrapping — aka learning — to the base-level bootstrapping that generates only one sample, as in this attack and optimization algorithms, but they sure feel suspiciously related to me. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The main reason to doubt that this thought experiment would work is the lack of a gradient here — we’re taking random steps, most of which will be neutral or a little bit harmful. I’m guessing this is fine for the original attack scheme because we already start at a good point, and because the perturbation oracle is designed to generate samples that tend to neither help nor hurt quality. (Incidentally, this is similar to some physics-inspired sampling schemes, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo&quot;&gt;Hamiltonian Monte Carlo&lt;/a&gt;, which use sampling strategies biased towards regions of similar loss to get faster exploration.) Can you improve text in this way instead of just paraphrasing it? I don’t know, but it’d sure be interesting if you could! &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 06 Mar 2024 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/llm-watermarking/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/llm-watermarking/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>Reflections on introductory neuroscience reading</title>
        <description>&lt;p&gt;Have you ever lived in a neighborhood for years and realized, as you prepare to move out, that you never got to know the guy who lives next door? For me, that unknown neighbor is the field of neuroscience. I am nearing the end of a PhD in &lt;a href=&quot;https://deweeselab.com&quot;&gt;a lab that does largely neuroscience&lt;/a&gt; and live under the umbrella of the &lt;a href=&quot;https://redwood.berkeley.edu/&quot;&gt;Redwood Center for Theoretical Neuroscience&lt;/a&gt;, but I confess I’ve never engaged with the field in any serious way.&lt;/p&gt;

&lt;p&gt;As with that neighbor you don’t really know, I’ve often &lt;em&gt;seen&lt;/em&gt; neuroscience — I regularly pass by it on the sidewalk, so to speak, exchanging cordial pleasantries but never really engaging, always with something just a little more important to do. Look, I’m sure neuroscience is a &lt;em&gt;nice guy&lt;/em&gt; and all that, but we just don’t have that much in common! A lot of people seem to like him, but we’ve just never really clicked – and besides, isn’t he the one with all those rats? It’s hard for me to understand what he’s saying or why, so when we pass each other on the street, I usually just smile and nod and continue on my way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No more. I’ve decided to learn some neuroscience. These past weeks, I’ve been doing basic reading in an effort to absorb some of the big ideas. This post will summarize some of what I’ve learned.&lt;/strong&gt; If you come from a similar academic background and have a similar curiosity, perhaps you’ll find some of this interesting.&lt;/p&gt;

&lt;h3 id=&quot;why-would-i-learn-neuroscience&quot;&gt;Why would I learn neuroscience?&lt;/h3&gt;

&lt;p&gt;I’m doing this for two main reasons. The first is self-knowledge: I’m in a reflective period in which I aim to better understand myself, and I suspected that some basic neuro- and cognitive science might help me better understand my own experience — and indeed it has! Seeing the important ways in which our brains are hacky really gives me a sense of humility and a feeling that human experience is often different and much stranger than we conceive it to be, so we ought to really look at it. The second is more general: if you’ve never touched a field, there are often big foundational ideas sitting on the surface for you to learn, and so a fairly short period of learning can give outsize returns because you’re on the steep part of the learning curve. That also turned out to be true — I’d underestimated how much we know about the brain and how easily some of my naïve notions could be improved.&lt;/p&gt;

&lt;h3 id=&quot;plan-of-attack&quot;&gt;Plan of attack&lt;/h3&gt;

&lt;p&gt;I’m a physicist by training, and I like to think about general principles and big ideas, so over the past few weeks I read Sterling and Laughlin’s &lt;em&gt;Principles of Neural Design.&lt;/em&gt; This is an introductory book focused less on the specifics of anatomy and much more on broad principles which govern neural circuitry across brain regions and across species. This was a good match for the level of detail at which I wanted to learn things — I don’t need to know about, say, the superior temporal gyrus or the difference between norepinephrine and epinephrine, but I do want to know that neural circuitry aggressively tries to minimize wire length and energy consumption.  It gives a nice overview of how someone familiar with physical or systems thinking could begin to start thinking about the brain. I also read some of Kandel et al.’s &lt;em&gt;Principles of Neural Science&lt;/em&gt;,&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; a classic introductory text.&lt;/p&gt;

&lt;h1 id=&quot;some-learnings&quot;&gt;Some learnings&lt;/h1&gt;

&lt;p&gt;Without further ado, here’s a bunch of stuff I learned.&lt;/p&gt;

&lt;h3 id=&quot;the-brain-is-very-structurally-complicated&quot;&gt;The brain is very structurally complicated&lt;/h3&gt;

&lt;p&gt;Coming from machine learning, my naive picture of the brain was basically that it’s a big homogeneous mass of neurons initially connected in a mostly random fashion, with inputs to some regions and outputs from others, and that learning from experience leads to gradual strengthening and weakening of neural connections so that this mass of neurons eventually knows and learns.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/neuro/self_conception.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In reality, the brain is highly structured. This is actually pretty apparent from anatomy, even at a coarse level: the inside of the brain is whitish (containing mostly long insulated communication channels, or &lt;em&gt;white matter&lt;/em&gt;), while the outer few millimeters is greyish (containing lots of neurons with dense short-range connections, or &lt;em&gt;grey matter&lt;/em&gt;). Different regions of the brain have different textures, with peculiarly-shaped masses on the inside and in the hindbrain.&lt;/p&gt;

&lt;p&gt;In fact, even the cerebral cortex — the big wrinkly part that covers most of the outside of the brain — is in reality made up of lots of distinct regions which differ in their cellular structure! Over a century ago, Korbinian Brodmann made close examination of the cytostructure of the cortex and identified some &lt;a href=&quot;https://en.wikipedia.org/wiki/Brodmann_area&quot;&gt;52 distinct regions&lt;/a&gt; with different cellular composition and patterning. Some of these regions have since been found to be robustly responsible for distinct brain functions, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Auditory_cortex&quot;&gt;processing sound&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Postcentral_gyrus&quot;&gt;processing touch&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Broca%27s_area&quot;&gt;language production&lt;/a&gt;. The fact that these different regions of the homogenous-looking cortex are physiologically distinct and consistently perform different roles is a surprise to me!&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/neuro/brodmann_areas.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; The homogeneous-looking cortex is actually comprised of many regions of differing functions and cellular properties. Surprisingly to me, these regions do not follow the folds of the cortex! &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This complexity extends down to the level of individual neurons. There isn’t just one type of “neuron” — rather, the brain has tens to hundreds of different types of firing and support cells with very different geometries adapted to different roles and parts of the brain. On the small end, cerebellar &lt;a href=&quot;https://en.wikipedia.org/wiki/Granule_cell&quot;&gt;granule cells&lt;/a&gt; are only ~5 µm in size with axons of width on the order of only 200 nm (!). These connect directly with &lt;a href=&quot;https://en.wikipedia.org/wiki/Purkinje_cell&quot;&gt;Purkinje cells&lt;/a&gt;, which have cell bodies about 10x wider and dendritic arbors that spread out in a striking planar shape as far as several millimeters. The retina, which translates incoming light into neural signals, contains multiple layers of many distinct cell types, starting with specialized rod and cone cells and leading up to the optic nerve, which is essentially a cable-like bundle of about $10^6$ axons which extend for several &lt;em&gt;centimeters&lt;/em&gt; (!) from the eye to the visual cortex. The largest neurons in the body stretch from the spinal cord to the ends of the limbs and can be over a meter in length. Thinking of these all as “neurons” seems as reductive as referring to all components of a mechanical system or electronic circuit as just “components” — it’s not wrong, but it’s almost always more useful to work at a finer level of abstraction.&lt;/p&gt;

&lt;h4 id=&quot;why-are-things-so-complicated&quot;&gt;Why are things so complicated?&lt;/h4&gt;

&lt;p&gt;Here’s another amazing fact which will turn out to be related: in the nematode C. elegans, every individual has exactly 302 neurons, and they’re always in the same place and connected the same way.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This might seem shocking — after all, aren’t central nervous systems these flexible, adaptive systems that differ between individuals? It certainly surprises me until I remember that, well, the adult human body has exactly 206 bones and some 600 muscles, and they’re always in the same place and connected the same way. Biology is certainly capable of specifying this level of detail. Since the nematode doesn’t really need to learn, why &lt;em&gt;not&lt;/em&gt; just hard-code a rigid rule-based control system for its simple body?&lt;/p&gt;

&lt;p&gt;The important fact here is that the main role of a central nervous system is not to &lt;em&gt;learn,&lt;/em&gt; it is to dictate global actions and share information between different parts of an organism’s body necessary for doing so. Sterling and Laughlin illustrate this beautifully by pointing to the rudimentary chemical and electrical signaling of the paramecium — which is hard-coded and includes commands like “back up” and “turn” — as the ancestor of our own nervous systems. Learning in its various forms is sort of a remarkable recent development which allows a nervous system to adapt to its environment. In this light, it makes a lot of sense that the brain would be so complicated — it’s evolved from nervous systems like those of C. elegans, except instead of 302 hard-coded neurons, our brains have a few hundred hard-coded parts, and the fact that some learning can occur within each part is somewhat secondary to the overall control flow.&lt;/p&gt;

&lt;h3 id=&quot;the-brain-generally-obeys-a-handful-of-low-level-design-principles&quot;&gt;The brain generally obeys a handful of low-level design principles&lt;/h3&gt;

&lt;p&gt;What does it mean to “understand the brain”? Coming from machine learning theory, I’d held this notion to a high bar: surely &lt;em&gt;understanding&lt;/em&gt; in neuroscience would mean we know the precise encoding scheme for memories, or can give a mathematical model of human reasoning played out in neural firings, or concretely explain what happens neurologically when you imagine something. I hadn’t appreciated the degree to which these cognitive, experiential things are really just one level of the layer cake of neuroscience. There are in fact many other levels of abstraction which we understand fairly well!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Principles of Neural Design&lt;/em&gt; sets out to collect some low-level principles, and I found many of these quite compelling! Here are a few:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Compute with chemistry whenever possible.&lt;/strong&gt; Neural firing is energetically expensive — it’s much cheaper to transmit signals via releasing chemicals which travel diffusively! However, diffusive signaling spreads only as [distance] ~ [time]^1/2^, which ends up meaning it’s usually not a good choice when you need to go, say, more than a millimeter in one direction. Chemical computing tends also to require fewer parts and take up less space. The animal nervous system reliably uses chemical signal for either short-distance processing (e.g., across synapses or within cells) or slow global signal transmission (e.g., hormones in the bloodstream), and makes surprising use of proteins which change conformation in the presence of a ligand in order to perform basic computational operations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Send only what is needed.&lt;/strong&gt; Signal transmission is expensive — the brain uses almost 20% of the body’s energy, over half of which goes to neurons’ ion pumps! It’s therefore important to be economical in what’s sent — sending half as much information roughly cuts the energy cost in half. Across areas, the brain has robustly developed to only transmit necessary information and compress it as much as possible — often in ways that seem surprising given our experience of the world! The classic example of this is the visual system — we only have detailed vision in the central region of our field of view, and it’s shockingly low-res in the periphery — &lt;a href=&quot;https://www.scientificamerican.com/article/put-your-peripheral-vision-to-the-test/&quot;&gt;people regularly underestimate how bad their peripheral vision is&lt;/a&gt;. I honestly find it a bit annoying when I notice it, but it makes a lot of sense evolutionarily — the point of the visual system isn’t to provide our brain with a beautiful detailed image of the outside world, it’s to give us enough information to do survival and social tasks, and we can do these tasks quite well with only a small area of high-res vision that we scan around! This principle also applies to the postprocessing that occurs in the retina, which famously &lt;a href=&quot;http://www.scholarpedia.org/article/Sparse_coding&quot;&gt;compresses visual information by translating to a sparse basis&lt;/a&gt; before transmission through the optic nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Send at the lowest acceptable rate.&lt;/strong&gt; A cool fact I hadn’t known: sending a neural signal faster requires a thicker axon, which for biophysical reasons turns out to require superlinearly more resources and energy! The brain’s thus incentivized to send information as slowly as possible. This is basically true everywhere in the brain — I suppose it’s evolutionarily “easy” to tweak axons to be thinner and slower, so they’ll always tend to settle down to the slowest rate that works well enough. An amazing example of this is the speeds of different sensory modalities: olfaction (smell) has no need to be fast, so it uses a cable of $10^7$ very thin axons to send information quite slowly, while on the other end, the vestibular sense (balance) needs to send little information but needs to send it fast to keep us upright, so it uses far fewer axons which are about 100x cross-sectionally larger.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are, of course, plenty of exceptions here: the design of the vertebrate retina is famously silly, with processing circuitry partially blocking the photosensitive cells and the optic nerve creating a blind spot in each eye. I’m not sure how to square these cases with the observation that the brain often does a good job finding efficient design. Maybe the satisfaction of these principles is achieved via a kind of local gradient descent – e.g., by making small adjustments to axon thickness, neuron count, connectivity, and so on – and these weird cases reflect evolutionary “local minima” that are hard to escape?&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/neuro/axon_sizes.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Minimize wire.&lt;/strong&gt; Rather intuitively, longer axons take up more space, cost more energy, and slow signal transmission, so the brain tends to shorten wires as much as possible. This is achieved partially at the level of individual neurons, which tend to take shapes and choose branching points that reach all their connections in a local minimum of total distance.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; It is also achieved via organization of neural regions into maps — for example, the early visual cortex is arranged spatially in a 2D way that mimics the retina, which minimizes diagonal cross-wiring. Some brain areas have peculiar neural organization, like the aforementioned &lt;a href=&quot;https://www.snexplores.org/wp-content/uploads/2020/04/1030_LL_trees-1028x579.png&quot;&gt;fan-shaped Purkinje cells stacked in the cerebellum&lt;/a&gt;, and this organization allows for lots of dense connections in a small volume.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Complicate and specialize.&lt;/strong&gt; The cost of a neural design is &lt;em&gt;not&lt;/em&gt; in its complexity, it’s in the amount of resources it uses — that is, space, energy, time, and physical materials. If a neural circuit can be complicated in exchange for using less of one of these, it often will! As a result, there are a huge number of different neurons with different geometries, firing rates, sensitivities, and so on, which economize some resource. Every part of a signaling pathway in the brain is thus adapted to its place in the chain, and will adapt so it fulfills its role — transmitting with a particular fidelity, across a particular distance, and in a particular time — as cheaply as possible. An example I like: opsin molecules in rod cells in the eye are occasionally activated by random thermal noise… but they’re designed to be just robust enough that this noise level is just a few times below the activity rate when looking around in starlight.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;there-are-many-levels-at-which-we-might-want-to-understand-the-brain&quot;&gt;There are many levels at which we might want to “understand” the brain&lt;/h3&gt;

&lt;p&gt;As aforementioned, I hail from physics, where the bar for understanding is quite high: one expects a tight, testable, ideally-mathematical theory before one believes one understands a complex system. Applied to the brain, a physicist might want, say, a clean, elegant, mathematical theory for how high-level concepts are represented and manipulated in the brain before saying we understand what it’s doing, perhaps using notions of sparsity, information theory, high-dimensional geometry, and so on. This still seems like a reasonable-albeit-distant dream to me,&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; but there are many more levels at which we could understand the brain. Here are two:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low-level component design.&lt;/strong&gt; As discussed in the previous section, we can very much ask questions like: “given that this component performs this signaling task, why is it designed like this?” Questions like this are among the most answerable in neuroscience — appealing to efficiency principles seems to work pretty often! Of course, we often don’t know what task a component performs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Broad stories about high-level information processing.&lt;/strong&gt; I hadn’t appreciated the degree to which you don’t &lt;em&gt;need&lt;/em&gt; a mathematical description of learning in order to find pretty compelling stories about what different parts of the brain are doing. For example, we can tell loose stories like “when engaged in conversation, the auditory cortex preprocesses incoming sound, Wernicke’s area processes the sound as speech, and Broca’s area is responsible for speech production.” From the perspective of a physicist, this is an &lt;em&gt;incredibly vague&lt;/em&gt; story: what do you &lt;em&gt;mean&lt;/em&gt; by “processing the sound as speech”? What is &lt;em&gt;involved&lt;/em&gt; in speech production? And how on earth is this all encoded in a bunch of noisy neurons? These are very real questions, but the important realization for me is, well, you don’t need to know that for a lot of stuff. For example, if a patient’s Broca’s area is lesioned, they’ll be unable to produce speech correctly. If a patient’s Wernicke’s area is lesioned, they’ll be unable to understand speech but, remarkably, able to produce it. These areas light up under brain scanning when performing relevant tasks. This incredibly vague story seems to work, actually — well enough to inform medical interventions! We didn’t actually &lt;em&gt;need&lt;/em&gt; to know how the neural circuits work: vague, high-level stories are useful enough for some real understanding.&lt;/p&gt;

&lt;p&gt;It seems to me like most of our high-level brain knowledge is of this form: we have stories like “Part A is responsible for task X. Task X also requires input from Part B, so Parts A and B are wired together, and that wiring strengthens as one performs more of Task X.” We pretty rarely have a low-level understanding of how neural circuitry is computing, but we have stories like this for lots and lots of parts and tasks. This view is reminscent of &lt;a href=&quot;https://notes.andymatuschak.org/zQnTUMfm4YPLyi8GFA1Ym1e&quot;&gt;Marr’s three levels of abstraction&lt;/a&gt;, which proposes that computing systems can be studied at the level of the task ultimately performed (the highest level), the algorithm used to perform that task (the middle level), or the hardware used to implement that algorithm (the lowest level). Perhaps a good story here is that neuroscience has made major progress on the lowest and highest levels, but it’s still unclear how to connect them: we cannot extract the algorithms &lt;em&gt;performed by&lt;/em&gt; neural hardware which &lt;em&gt;implement&lt;/em&gt; the high-level functions which we know brain regions to perform.&lt;/p&gt;

&lt;p&gt;One interesting takeaway I glean from all this is that it now seems like the thing I purport to want — a simple mathematical description of learning — actually lies not at the highest level of abstraction but at an intermediate level, above the level of small circuits but below the level of brain regions. I’m also less confident that it’s really a good goal!&lt;/p&gt;

&lt;h3 id=&quot;how-do-we-learn&quot;&gt;How do we learn?&lt;/h3&gt;

&lt;p&gt;The nature of learning is still pretty unclear to me from my reading. It seems like there are a bunch of different mechanisms — synapses that have fired recently more readily fire again in the following seconds and minutes, synapses that have fired many times tend to increase their sensitivity, dopamine release (which is globally mediated) tends to reinforce neurons to do whatever they were just doing. I’m confused as to how to think about these mechanisms — is there a sharp difference between short-term and long-term memory? Is there a difference between short-term memory and “what you’re thinking about right now”? Is most learning distributed and reward-signal free, or is it top-down modulated as in machine learning? What tasks even count as “learning” — I could believe there are many more than the typical testing suite I envision! I feel I’ve gotten a bit of flavor for some of these learning mechanisms, but not enough to have any real picture of learning in the brain.&lt;/p&gt;

&lt;p&gt;This set of questions seems particularly interesting where it intersects with our everyday human experience. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;What’s going on neurologically when we forget things?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When you hear or use an usual word, you’re more likely to notice or use it again soon after. Is this explicable through some known neuro learning mechanism?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What’s the difference between factual learning and wisdom? Why does some learning feel like it affects our worldview, while other learning feels like just memorizing facts?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fun-facts&quot;&gt;Fun facts&lt;/h3&gt;

&lt;p&gt;Some disconnected fun facts from my reading:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The brain has a region called the &lt;em&gt;suprachiasmatic nucleus&lt;/em&gt; which takes in input from the retina regarding how generally light a scene is and serves as the body’s 24-hour clock.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neurons in the brain are outnumbered by glial cells (i.e. everything else) by a factor between 10 and 50.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The skull is physically full of brain, and synapses expand when they get stronger, so it’s often the case that practicing one skill causes part of the brain to physically grow, which causes other parts to shrink! This can have expected-but-still-alarming consequences in which learning one skill directly degrades performance at another — for example, learning to read decreases one’s ability to recognize faces! (I’m confused as to what to take away from this in light of &lt;a href=&quot;https://www.scientificamerican.com/article/what-is-the-memory-capacity/&quot;&gt;the common claim that the brain’s storage capacity is so large it’s virtually unlimited&lt;/a&gt;.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h1&gt;

&lt;p&gt;I set out to see if I could glean a high-level view of the field of neuroscience from some foundational background reading. I actually feel I’ve managed to do that to a modest degree! It was an endeavor well worth the time investment; I’d recommend it to others who are interested, and will probably do it again myself with different fields.&lt;/p&gt;

&lt;p&gt;In general, I’d say we actually understand more about neuroscience than I’d thought! While the things I thought were mysteries generally do seem to be unknown, I was blind to the many levels at which modern neuroscience examines the brain and thus to the very real progress we’ve made towards characterizing the brain’s functioning and relating it to both evolutionary pressures and our own human experience. I’m left quite impressed by the sheer amount of work required to get to this point — as an acquaintance recently told me, it takes a PhD’s worth of work to get one line in a textbook.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The most profound conceptual discoveries in the sciences — perhaps simply in general — often build bridges between two things that previously seemed to live in different worlds. My interest in neuroscience basically stems from a hope of this nature. Our lives as human beings take place largely in the world of our own internal processing: our cognition, sensation, and action; our feelings, reactions, and dreams. However, we have far more concrete, reliable understanding of the physical world of atoms and molecules, proteins and neurons, circuits and brains. I’m excited (both selfishly and altruistically) by the prospects for building bridges between these realms, starting to understand our own human experience in terms of basic science.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&quot;https://csinva.io/&quot;&gt;Chandan Singh&lt;/a&gt; and Mike DeWeese for feedback on this post!&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yes, these two books have confusingly similar names. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It took a protracted debate to arrive at the modern view of the cortex as composed of distinct regions which perform different elementary processing operations. Interestingly, this idea originated with &lt;a href=&quot;https://en.wikipedia.org/wiki/Phrenology&quot;&gt;phrenology&lt;/a&gt;, which in the early 1800s posited that the brain is comprised of some 30+ regions responsible for personality traits like “wit,” “religiosity,” “benevolence,” and so on. This framework is now understood to be totally wrong and based in little to no evidence, but the idea of a few dozen distinct brain regions was right - by accident as far as I can tell! After that came an era dominated by the “aggregate field view” that essentially held that the whole brain does everything, after which the modern “cellular connectionist” view took hold. Most of the early evidence for the modern view came from studying patients with certain cognitive or motor impairments and consistently finding damage in the same part of their brains. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Fun fact: C. elegans neurons also use analog signaling, not pulsatile signaling (aka “firing”). &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Tree branches and root systems do something like this too! &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In all honesty, I personally find this question so appealing and seductive that it’s distracting from more concrete problems. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;That said, it’s likely I’m too optimistic about understanding here given that I’ve just read a compendium of lots of things we understand. It seems likely that there are lots of things we don’t understand, even at the lowest and highest levels of abstraction: plenty of microscopic neural processes remain difficult or impossible to characterize with current tools, and there are surely lots of macroscopic brain areas about which we don’t have really compelling stories as to their function. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 02 Mar 2024 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/neuro-learnings/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/neuro-learnings/</guid>
        
        
        <category>neuroscience, research</category>
        
      </item>
    
      <item>
        <title>Can a chemical reaction measure the size of its container?</title>
        <description>&lt;p&gt;All modern society rests on a suite of basic chemical processes, developed over centuries, that let us feed ourselves, power our cities, and make virtually every artifact of technology we manufacture.
We know chemical processes that let us &lt;a href=&quot;https://en.wikipedia.org/wiki/Haber_process&quot;&gt;fix nitrogen&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Polymerization&quot;&gt;string together polymers&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Electroplating&quot;&gt;coat surfaces with metal&lt;/a&gt;, and perform a thousand other wonders.
This is a post about a chemical capability which we do not yet have.
It’s not a particularly useful one, but we’ll find there are things to be learned by considering it nonetheless.&lt;/p&gt;

&lt;p&gt;My favorite useless-but-cool chemical reaction is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Briggs%E2%80%93Rauscher_reaction&quot;&gt;Briggs-Rauscher oscillating clock reaction&lt;/a&gt;, a reaction common in chemistry classrooms which dramatically changes color from blue to clear every few seconds for a minute or more.
“Chemical oscillator” reactions like this one are famous among chemists because an oscillating reaction seems so counterintuitive (when they were first discovered, nobody believed the discoverer!).
Steady oscillation is a behavior you’d expect from a mechanical or electronic system but not from a purely chemical one.
In the case of the Briggs-Rauscher reaction, this behavior’s powered by a sort of entropic battery and can be understood in loose analogy to electromechanical clocks:
one reactant is steadily consumed to power an oscillating system of other chemicals, much like a grandfather clock is powered by the potential energy of a weight or an electronic oscillator is powered by a battery.
This clever system demonstrates (at least to me) that, if we’re creative, the medium of chemistry affords sufficient freedom to make much more than one imagines at first look.&lt;/p&gt;

&lt;p&gt;Let’s trust this is true – that pure chemical systems can do many things that you’d think you need electromechanics for – and let’s not worry about doing things that are useful, just things that are interesting.
What might we hope to make?
Could we make reactions that detect temperature?
Reproduce music?
Precipitate entire 3D objects?
Do math?
Are there chemical reactions which are Turing-complete?
Is there a simple set of “chemical modules,” analogous to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Logic_gate&quot;&gt;logic gates&lt;/a&gt; or the &lt;a href=&quot;https://en.wikipedia.org/wiki/Simple_machine&quot;&gt;simple machines&lt;/a&gt;, which one might use as basic building blocks for more complex processes?&lt;/p&gt;

&lt;p&gt;This broad direction seems interesting, and I know of no theoretical work discussing it.
We’ll revisit it at the end.
For now, however, I’d like to explore one specific application which I find particularly intriguing.
Let us ponder whether one could conceivably make…&lt;/p&gt;

&lt;h4 id=&quot;a-chemical-reaction-that-measures-the-volume-of-its-container&quot;&gt;A chemical reaction that measures the volume of its container&lt;/h4&gt;

&lt;p&gt;Is it possible for a purely chemical system to measure the volume of its vessel?
This is easily done with the mechanical methods of a volumetric flask, a scale, or a ruler, but can we do it with pure chemistry?&lt;/p&gt;

&lt;p&gt;I should clarify what exactly I mean by this.
I want a reaction such that we can (a) combine some chemicals in a beaker (without too much sensitivity to initial conditions), (b) wait some time for a reaction to occur, and (c) measure the final concentration of one of the reaction products and straightforwardly read out the volume of solution (for example, maybe the solution is saltier or more opaque in proportion to its volume).
Is such a system possible?&lt;/p&gt;

&lt;p&gt;There is an easy solution which we’ll disallow: if you just dissolve a known mass $m$ of solute in the solution, the volume $V$ can be read off from the final concentration $m/V$.
This violates our stipulation on sensitivity to initial conditions – we needed exactly mass $m$ – and, for clarity and good measure, let’s also disallow it with the natural requirement that, if you just combine the full contents of two beakers at time zero, you should read out the sum of their volumes at the end (a criterion this hacky solution does not satisfy).&lt;/p&gt;

&lt;p&gt;Making such a volumetric reaction is trickier than it might seem at first glance.
If you ponder it for some time, you’ll find that the difficulty is that volume is an &lt;em&gt;extensive&lt;/em&gt; quantity (one that increases proportional to the amount of stuff, like a system’s mass), but chemical reactions are local and only depend on &lt;em&gt;intensive&lt;/em&gt; quantities like concentrations or temperature (which are independent of the total size of the system).
How can we design a &lt;em&gt;local&lt;/em&gt; system that takes a &lt;em&gt;global&lt;/em&gt; measurement?&lt;/p&gt;

&lt;h4 id=&quot;the-reaction-diffusion-equations&quot;&gt;The reaction-diffusion equations&lt;/h4&gt;

&lt;p&gt;Now is the point when I confess I know little about practical chemistry and have no idea how to either design or test a chemical reaction.
That said, I &lt;em&gt;am&lt;/em&gt; a theorist, and I know some nice equations that can help out.
Our proposition is sufficiently interesting that it’s worth figuring out if it’s even possible in principle, and for answering that question, we can use the &lt;em&gt;reaction-diffusion equations&lt;/em&gt;, a mathematical model for a reacting chemical system spread across a volume.&lt;/p&gt;

&lt;p&gt;The reaction-diffusion equations describe the time-evolution of a concentration vector $\mathbf{u}(x, t) \in \mathbf{R}^n$, where $x$ is a spatial coordinate, $t$ is time, and the $n$ elements of $\mathbf{u}$ represent the concentrations of $n$ chemical species.
The concentration vector evolves according to&lt;/p&gt;

\[\begin{equation}
\label{eqn:rd}
\partial_t \mathbf{u} = \mathbf{D} \nabla^2 \mathbf{u} + \mathbf{r}(\mathbf{u}),
\end{equation}\]

&lt;p&gt;where $\mathbf{D} = \text{diag}(D_1, …, D_n)$ is a diagonal matrix of diffusion coefficients, $\nabla^2$ is the Laplacian, and $\mathbf{r}(\cdot)$ is an arbitrary local reaction function.
The first term in this evolution describes each species undergoing ordinary diffusion, which acts to spatially smooth out their concentrations, while the second term describes a local reaction in which chemical species multiply or change identities.
Let’s suppose we’re working in 1D with a total volume $V$ and periodic boundary conditions.
The question we pose to ourselves is: &lt;strong&gt;can we find diffusion constants $D_i$ and a reaction function $\mathbf{r}$ such that, as $t \rightarrow \infty$, one of the concentrations - say, $u_n$ - is proportional to V?&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;turing-patterns&quot;&gt;Turing patterns&lt;/h4&gt;

&lt;p&gt;If you’ve heard of reaction-diffusion equations before, it’s probably in conjunction with &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_pattern&quot;&gt;Turing patterns&lt;/a&gt;.
Turing patterns are beautiful, amorphous-looking spatial patterns in the concentration of chemical species that emerge spontaneously as a result of reaction diffusion evolution with certain kinds of reaction term.
Here are some examples (&lt;a href=&quot;https://www.youtube.com/watch?v=MR79V9UmM6s&quot;&gt;source here&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;https://james-simon.github.io/img/reaction_diffusion/turing_patterns.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(Note that these are 2D, while we’ll be working in 1D.)
&lt;a href=&quot;https://jasonwebb.github.io/reaction-diffusion-playground/&quot;&gt;Here’s&lt;/a&gt; a site with animations showing how these patterns develop spontaneously in time.
If Turing patterns look somewhat biological to you, that’s a good instinct - lots of coloration patterns on animals are created by similar processes.&lt;/p&gt;

&lt;p&gt;The fact that Turing patterns give spatial structure from nothing seems like a good start in our endeavor to make a volumetric reaction.
The bumps have a characteristic spacing, so it seems like all we have to do is let the pattern form and count the ridges.
We’ll then just have to make sure some other chemical species’ concentration encodes the final count.&lt;/p&gt;

&lt;p&gt;While that sounds great, it’s not obvious (at least to me) how to then make a reaction that counts the ridges, so we’ll need to think harder.
My first thought was to instantiate a big set of Turing patterns in different components of $\mathbf{u}$, each with a different characteristic wavelength, and see what the longest-wavelength species that grew a bump was, but then I had a better idea.&lt;/p&gt;

&lt;h4 id=&quot;the-plan&quot;&gt;The plan&lt;/h4&gt;

&lt;p&gt;Our core difficulty is that it’s difficult to escape the world of intrinsic quantities.
Measuring volume would be easy if we had &lt;em&gt;one&lt;/em&gt; extrinsic quantity to use as a foothold.
Here’s an idea to this effect: create exactly one “concentration bump” of a known shape at a random location.
The “bump density” $1/V$ (in units of bumps per volume) is easy to measure chemically, and it’s easy to convert from that to $V$!&lt;/p&gt;

&lt;p&gt;Stable bumps of known shape are easy to make using reaction-diffusion systems, as Turing patterns make evident.
Turing patterns have fixed bump density, though – not fixed bump count, which is what we need – so we’ll have to adapt the system so one bump suppresses all others.&lt;/p&gt;

&lt;h4 id=&quot;the-execution&quot;&gt;The execution&lt;/h4&gt;

&lt;p&gt;I figured out how to do this with a reaction-diffusion equation with six chemical species.
These consist of four “fast-diffusing” species, for which the time to diffuse across the vessel $V^2 / D_i$ is much shorter than all other timescales and concentration is uniform, and two “slow-difffusing” species which create spatial structure.
These species play the following roles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$u_1$ (fast): a &lt;em&gt;timer&lt;/em&gt; whose concentration steadly grows from zero and regulates other reactions.&lt;/li&gt;
  &lt;li&gt;$u_2$ (slow): a &lt;em&gt;randomly-diffusing agent&lt;/em&gt; which amplifies random variation in its initial concentration to create a random concentration profile.&lt;/li&gt;
  &lt;li&gt;$u_3$ (fast): a &lt;em&gt;maxfinder&lt;/em&gt; whose concentration converges to just below the max concentration of agent $2$, with only the peak above it.&lt;/li&gt;
  &lt;li&gt;$u_4$ (slow): a &lt;em&gt;bump-forming agent&lt;/em&gt; which forms a bump of a certain size starting from the site of max concentration of agent $2$ at a prescribed time.&lt;/li&gt;
  &lt;li&gt;$u_5$ (fast): a &lt;em&gt;bump measurer&lt;/em&gt; which is produced by the indicator bump and converges to concentration inversely-proportional to $V$.&lt;/li&gt;
  &lt;li&gt;$u_6$ (fast): a &lt;em&gt;readout agent&lt;/em&gt; which concerges to concentration $\propto 1 / u_5$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’ll defer further discussion and the equations themselves to the &lt;a href=&quot;#appendix&quot;&gt;Appendix&lt;/a&gt;.
Let’s just jump to the good part: here are videos of the reaction working.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/reaction_diffusion/rd_sim_v=3.gif&quot; width=&quot;30%&quot; /&gt;
&lt;img src=&quot;https://james-simon.github.io/img/reaction_diffusion/rd_sim_v=5.gif&quot; width=&quot;44%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:18%; margin-right:18%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 1.&lt;/b&gt; Simulations of the volumetric reaction for $V = 3,5$. A timer component regulates a multi-step process in which one bump of known size forms, emits a measurement agent, and allows the deduction of the system size.&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;Note how, in both cases, the concentration $u_6$ of the readout species is equal to to the volume of the system ($V=3,5$, respectively) at around $t=60$!
Figure 1 shows the spread of final concentrations for $V \in {1,…,6}$.
The medians land right on the true volumes.
(The long tails of outliers reporting smaller volumes reflects cases in which more than one bump was formed; the reaction’s pretty noisy, but I count this as success since it works most of the time.)&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/reaction_diffusion/volumetric_reaction_vol_test.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 2.&lt;/b&gt; Final readout concentrations $u_5(t_f)$ with $t_f = 70$ over 10 trials with $V \in \{1,...,6\}$.&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;h4 id=&quot;discussion&quot;&gt;Discussion&lt;/h4&gt;

&lt;p&gt;We’ve seen that it’s possible for a reaction-diffusion equation with random initial conditions to measure the global volume of its vessel despite its being a fundamentally local process.
So, could there be real chemical species that’d make this work in reality?
Perhaps the greatest stretch is the continuously-increasing timer ($u_1$) that regulates other reactions, but then we’ve already seen that clock reactions exist in practice, and in the worst case we could drop the timer and have the experimenter mix in new chemicals at specified times.
The readout $u_6$ converging to $\propto 1 / u_5$ is a bit suspicious, too – it’s rare for a reaction to be sensitive to the inverse of a concentration – but I suspect one could design a process that approximates that sensitivity in some regime, and in the worst case again, the chemist could just measure $u_4$ and make that calculation manually.
It’s worth noting that this process in reality would probably include additional species for support (just as a motor contains many more parts than those which are directly part of the core process).
That said, I see nothing that suggests that something like it couldn’t work in the real world.
If you know more chemistry than I and have thoughts, please reach out!&lt;/p&gt;

&lt;p&gt;So what?
This reaction is cool, but it’s not useful; its value is in its serving as a test case for a broader idea.
Having crafted a volumetric reaction-diffusion system, we could now use it as a module in other more complex systems, like one that renders an image or pattern autoscaled to the space available.
We’ve crafted a &lt;em&gt;transferrable module&lt;/em&gt; which &lt;em&gt;robustly does something interpretable&lt;/em&gt; and which &lt;em&gt;we can combine with other modules to make more complex systems.&lt;/em&gt;
This process of crafting modular abstractions is how all fields of engineering work (think of physical mechanisms, simple circuits, software packages, etc.).
Chemical engineering has basic reactions for doing simple manipulations to chemicals (cutting, splicing, oxidizing, polymerizing, etc.)
The things one can ultimately make are defined by the elements of this basic modular algebra, and so it’s fascinating to think that there could be a “different basis” of chemical modules which do things like measure, render, count, perform logic, etc.&lt;/p&gt;

&lt;p&gt;There are various senses in which an algebra of basic chemical processes could be “complete.”
One might be: can reaction-diffusion processes be used to simulate &lt;em&gt;any&lt;/em&gt; isotropic PDE?
Most PDEs are not reaction-diffusion processes – for example, one might contain terms like $u_1 \nabla^2 u_2$ – but perhaps a reaction-diffusion process with some auxiliary fields could simulate such terms (much like auxiliary particle fields create effective interaction terms in particle physics).
More generally, we could ask if one could make an arbitrary Turing machine.
The hard part seems to be breaking the left-right symmetry, but I’m fairly confident it’s doable.
It’d be rather beautiful if Turing patterns were Turing-complete.&lt;/p&gt;

&lt;p&gt;There is a game that could be played here which is akin to &lt;a href=&quot;https://en.wikipedia.org/wiki/Code_golf&quot;&gt;code golf&lt;/a&gt;: how few chemical species can we make this reaction with?
In my case, I used six, and had I cleverly reused some of them, I expect I could get it down to three or four.
I could see this game being fun given an interesting set of objectives.
Since all engineering fields use this paradigm, we could in principle play it in lots of other domains, too, trying to e.g. make a circuit that does a certain task with as few components (or as few logic gates) as possible or designing a physical mechanism with as few degrees of freedom as possible.&lt;/p&gt;

&lt;p&gt;My main field of study isn’t chemistry, it’s deep learning theory, and there are lessons I take from this exercise as to how we might think about neural nets.
Deep learning has a reputation for being modular – the basic architectural units in PyTorch are literally called &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Module.html&quot;&gt;Modules&lt;/a&gt; – but it’s unique in that &lt;em&gt;nobody knows how to think about what the modules are really doing.&lt;/em&gt;
Modern chemical engineering is composed of well-understood steps, but medieval alchemy, by contrast, was composed of lots of &lt;em&gt;poorly-understood&lt;/em&gt; steps.
Deep learning in 2022 is much more like alchemy: we have modules like transformer layers, batchnorm layers, and so on, but we don’t truly understand what they do or how we ought to combine them to accomplish a broader task.
I imagine thinking in terms of the actions of modules could’ve helped the alchemists clarify their thinking and develop chemistry, and I similarly expect thinking in terms of what quantitative effect each deep learning module does will prove clarifying for the field.
It’s my belief that, in doing so, we’ll eventually arrive at a state in which the design of a deep learning system resembles the selection and combination of well-defined modules like in every other field of engineering.&lt;/p&gt;

&lt;h4 id=&quot;appendix-details-of-the-reaction&quot;&gt;Appendix: details of the reaction&lt;/h4&gt;
&lt;p&gt;&lt;a name=&quot;appendix&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The reaction-diffusion system of Equation $\ref{eqn:rd}$ is defined by its diffusion constants and reaction function.
We choose the diffusion constants to be&lt;/p&gt;

\[D_1 = D_3 = D_5 = D_6 = \infty, \ \ D_2 = 2, \ \ D_4 = 1,\]

&lt;p&gt;where $D_i = \infty$ means that species $i$ diffuses much faster than all other timescales (implemented in simulation via global averaging every timestep).
We choose the six componts of the reaction function $\mathbf{r}(u)$ to be&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$r_1 = .1,$&lt;/li&gt;
  &lt;li&gt;$r_2 = .1 \, u_1 \ I(0,t_a),$&lt;/li&gt;
  &lt;li&gt;$r_3 = 10^3 \ \text{max}(u_2 - u_1, 0),$&lt;/li&gt;
  &lt;li&gt;$r_4 = 30 \, (1 - u_4) \ \mathbb{1}_{u_2 &amp;gt; u_3} \ I(t_a - .1, t_a) + .15 \ I(t_a, t_b) + (2 u_4 - 1) - (2 u_4 - 1)^3,$&lt;/li&gt;
  &lt;li&gt;$r_5 = (10 u_4 - u_5) \ I(t_b, \infty),$&lt;/li&gt;
  &lt;li&gt;$r_6 = (2.9 u_4^{-1} - u_5) \ I(t_b + .5, \infty),$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where $t_a = 2$, $t_b = 5$, and $I(t_1, t_2) \equiv 1_{t_1 \le u_1 &amp;lt; t_2}$.
All components are initialized as random functions with variance $.2$ (see the code for details).
I arrived at these formulae after a lot of trials and error, working from the top of the list down, developing each step of the mechanism sequentially.
The hardest part was reliably getting a single bump to develop in $u_4$; I was close to thinking this might be impossible to get one bump to spontaneously form and suppress all others, but using the initial randomness to break the symmetry let me do it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1kIMunZCWxD53LWlDAnOxuj5iRB2UfEyw#scrollTo=loonoeZ3Rjms&quot;&gt;Here’s&lt;/a&gt; the colab notebook I used to implement these.
Slightly different calibrated reaction constants were used to generate Figure 2.&lt;/p&gt;
</description>
        <pubDate>Mon, 10 Oct 2022 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/volumetric-reaction/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/volumetric-reaction/</guid>
        
        
        <category>chemistry, physics, fun-science</category>
        
      </item>
    
      <item>
        <title>Simulating cells fighting to the death</title>
        <description>&lt;p&gt;Physics is humanity’s finest tool for understanding the world around us, a rich and wonderful framework deserving of our highest reverence.
I recently used it to simulate a bunch of cells fighting to the death like little gladiators.
Here’s a video.&lt;/p&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;40%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/cell_fight/cell_fight.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I’ll give a lightning overview of how this works.
This is a simulation lying on a grid, with every color either empty or part of a cell.
The grid’s evolving in time according to a modified version of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cellular_Potts_model&quot;&gt;cellular Potts model&lt;/a&gt;, which is basically the &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;&gt;Ising model&lt;/a&gt; plus a few terms.
In the Ising model, each cell prefers to match its neighbors, so the grid forms big regions of the same color.
Here’s the Ising model:&lt;/p&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;40%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/cell_fight/ising_test.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;Skipping a mathematical explanation, the basic algorithm at play here is the following: at every timestep, (a) choose a random grid site and (b) flip its value with some probability.
The probability of flipping is higher if flipping will make it more like its neighbors, and this is coded in terms of an “energy” which is lower the more sites match their neighbors.&lt;/p&gt;

&lt;p&gt;The Potts model is a generalization to more than two values.
In the &lt;em&gt;cellular&lt;/em&gt; Potts model, we simply add another term to the energy.
For each value \(i\) besides the “empty value” \(0\), we count up the number of sites of type \(i\) and penalize the difference from a target volume \(V_i\).
This enforces that the sites with value \(i\) have total volume around \(V_i\), and the Ising term from before makes it so these sites tend to stick together, forming a round droplet like water molecules.
Here’s the cellular Potts model with a few randomly-placed cells:&lt;/p&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;40%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/cell_fight/cells_stationary.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;To make them fight, I just added a third energy term which makes a site more likely to flip to the color of cell \(i\) if doing so would move the center of mass of cell \(i\) towards its nearest neighbor, and then added the ad hoc rule that, when one cell loses a site to another cell, its target volume \(V_i\) decreases to track its loss of hitpoints.&lt;/p&gt;

&lt;p&gt;Superficially, this is just a cute simulation of some artificial cells, but, for me, it gets at one of the deep wonders of life.
These cells exhibit high-level “purposeful” behavior, but their motion is entirelly driven by extremely simple low-level rules (and they’re not even deterministic!).
Just like in a real organism - just like in &lt;em&gt;us&lt;/em&gt;, science believes - this purposeful behavior emerges from the interaction of many pseudorandom low-level components.
These cells were not programmed via top-down rules, as one might code a video game enemy (walk towards player, move limbs while doing so, fire when X feet away, etc.); their high-level behavior emerges fully from the bottom up.&lt;/p&gt;

&lt;p&gt;In a typical video game, the &lt;em&gt;player’s&lt;/em&gt; character, too, is controlled via top-down instructions, in this case from a live human.
Could one design a game in which the player’s influence is exerted from the bottom up, as a slight modification to the low-level rules instead of a top-level directive?
This is indeed doable with this simulator: I just replaced one cell’s stochastic bias towards its nearest neighbor with a bias towards the direction of keyboard input.
Here’s a fight between my brother and I.&lt;/p&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;40%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/cell_fight/cell_duel.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;Okay, I also added bullets.&lt;/p&gt;

&lt;p&gt;You can find code for these simulations &lt;a href=&quot;https://github.com/james-simon/cell-fight&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;!-- These two possible methods of exerting outside influence on a simulation are intriguingly similar to certain human conceptions of divine intervention in the universe.
The older notion is that a deity would directly change the world as desired (e.g., if they don't want a tree to exist, just smite it).
A newer notion is that a deity might act by minimal intervention, subtly changing low-level parameters of the universe (e.g. if they don't want a tree to exist, slightly deplete the CO2 in the vicinity during its growth, or spike a few neurons in a squirrel brain to make it find and eat the tree as a seed, or so on). --&gt;
</description>
        <pubDate>Sat, 24 Sep 2022 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/cell-fight/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/cell-fight/</guid>
        
        
        <category>physics, fun-science</category>
        
      </item>
    
      <item>
        <title>Time-reversed random walks</title>
        <description>&lt;p&gt;A year or so ago, at the height of the pandemic, a few friends and I watched &lt;em&gt;Tenet (2020)&lt;/em&gt;, an action movie whose core conceit is that certain people and objects move the opposite direction through time.
The idea’s not that they time-travel or that they merely age backwards, it’s that they’re truly time-reversed, appearing to ordinary observers like a video played backwards.
To get an idea of what I mean, &lt;a href=&quot;https://www.youtube.com/watch?v=4xj0KRqzo-0&quot;&gt;here’s&lt;/a&gt; a clip of the protagonists fighting two time-reversed opponents.&lt;/p&gt;

&lt;p&gt;Does this concept make sense?
Thinking about it for a bit, one immediately bumps up against a number of free-will-related paradoxes.
For example, if you start a fight with a time-reversed adversary, it seems you’re guaranteed you won’t incapacitate them because you observed them up and fighting at a time that’s earlier for you but later for them (if you are to win, you must have “recapactitated” them with your first blow).
Similarly, it seems you couldn’t break a time-reversed vase or light a time-reversed match, and things would probably end badly if you tried to eat a time-reversed sandwich.
Ultimately, however, these are paradoxes of human agency, and if you remove free will as a consideration, these events’ impossibility feels more believable.
Accepting the lack of free will, is the movie’s core idea physically possible?&lt;/p&gt;

&lt;p&gt;At a first glance, it looks like several scenes clearly violate Newtonian mechanics, such as &lt;a href=&quot;https://youtu.be/L3pk_TBkihU?t=41&quot;&gt;this scene&lt;/a&gt; in which a gear jumps off a table to meet a hand that dropped it backwards in time.
However, it turns out this is not strictly impossible: the thermally jiggling particles of the surface beneath the gear could all come together and move in the same direction, giving the gear a big push upwards and leaving the surface a little colder.
This is precisely the time-reversed process of the forwards event of dropping the gear.
This sequence of steps seems absurdly implausible - and in our world, it would be - but it doesn’t break conservation of momentum or energy or violate any ironclad law of physics.
These sorts of backwards processes are always possible because the laws of Newtonian physics and electromagnetism are &lt;a href=&quot;https://en.wikipedia.org/wiki/T-symmetry&quot;&gt;time-reversal symmetric&lt;/a&gt;&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;It turns out the apparent flow of time in the world we see around us is due principally to thermodynamics, a statistical set of effective laws that emerge from the basic rules of the universe when considering macroscopic systems like vases and Christopher Nolan.
Time-reversed versions of events are not fundamentally impossible even in ordinary circunstances, they’re merely much less likely than their forwards-time counterparts.
In practice, the gear will stay on the surface for as long as you watch it, but there is indeed a miniscule probability it’ll jump up to meet your hand each moment.
In a very deep sense, fragile objects shatter forwards in time because, well, they’re not broken now, and there are lots of ways they can break, so statistics doesn’t have to work too hard to make it happen.
If you sweep together a bunch of glass shards, however, there’s only one way they can all fit together, so their perfect coalescence is very unlikely.
In our universe, things happen to generally be more ordered back in the direction we call the past on account of the Big Bang.
However, if you took another universe and enforced the constraint that things are ordered and nice at some &lt;em&gt;final&lt;/em&gt; time, time-reversal would give you a universe flowing “backwards” in time, though its inhabitants would of course never know the difference.
This is the difference between specifying the initial and final conditions of a dynamical process, and this is no difference at all for a time-symmetric process.&lt;/p&gt;

&lt;p&gt;Okay, but how do you get things moving both forwards and backwards?
This is weird, since you normally can’t specify both initial &lt;em&gt;and&lt;/em&gt; final conditions for a dynamical process.
However, one way to do it and avoid a paradox might be to specify only &lt;em&gt;partial&lt;/em&gt; information at both points in time.
For example, suppose I enforce that the left half of a room contains person A at time \(t=0\), and the right half of the same room contains person B at time \(t=T\), and I don’t specify anything else.
This is an underdetermined system which is likely to have many solutions.
Actually finding one would be hard - you’d probably have to iterate forwards and backwards in time for a bunch of cycles in a nonlocal fashion until you satisfy all the boundary conditions - but it’s not impossible in principle.
Given appropriate brain states in these boundary conditions, the obtained dynamical solution might be these two people fighting.
This way of thinking about the problem is weird and nonlocal, but so is Tenet, and it feels more satisfying to me than prioritizing one time direction and viewing time-reversed events like the leap of the gear as simply a series of flukes.&lt;/p&gt;

&lt;p&gt;This is a concrete enough way of looking at the problem that we could start to think about how to simulate it.
The ultimate dream here would be a video game in which you fight and interact with creatures moving backwards in time.
In order to make that happen, though, we need to understand how to model rule-based processes from the point of view of an observer moving the other way in time.
The rest of this post obtains the math for doing this using the simplest dynamical model in thermodynamics, the random walk.&lt;/p&gt;

&lt;h3 id=&quot;the-1d-discrete-random-walk&quot;&gt;The 1D discrete random walk&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;&gt;random walk&lt;/a&gt; is a basic stochastic process in which a particle, which we refer to as a walker, explores a space by taking steps in random directions.
This process is used to model a swath of phenomena including the paths of diffusing particles, fluctuating stock prices, and the movements of foraging animals, and it’s found use in essentially every discipline of science.&lt;/p&gt;

&lt;p&gt;The simplest example of this process is a 1D random walk with discrete steps.
Let \(x_0, x_1, x_2, ..., x_t, ...\) be integers denoting the position of a random walker on the real line at each timestep.
The initial condition and transition dynamics are&lt;/p&gt;

\[x_0 = 0,\]

\[\begin{equation}
\label{eqn:forward_step}
x_{t+1} =
\left\{
\begin{array}{lr}
x_t, &amp;amp; p = \frac{1}{2} \\
x_t + 1, &amp;amp; p = \frac{1}{2}.
\end{array}
\right.
\end{equation}\]

&lt;p&gt;In words, the random walker begins at zero and, at each timestep, flips a coin and either remains put or moves to \(x \rightarrow x + 1\) accordingly.
(It’s worth noting that we could’ve also chosen the possible steps to be not \(\{0,1\}\) but rather \(\{-1,1\}\) to ensure that the random walk has mean zero.
It will prove mathematically simpler to use our current formulation, but we can always recover the latter setting by constructing \(\tilde{x}_t \equiv 2 x_t - t\).
We will use \(\tilde{x}_t\) in our visualizations below, and we will refer to the two possible steps as “leftwards” and “rightwards.”)&lt;/p&gt;

&lt;p&gt;Suppose we want to know \(p(x_t)\), the probability of the walker being at position \(x_t\) at time \(t\).
By placing this random walk on Pascal’s triangle, it’s easy to show that this distribution is given by&lt;/p&gt;

\[\begin{equation} \label{eqn:p_forwards}
p(x_t) = 2^{-t} \left( \begin{array}{c} t \\ x_t \end{array} \right).
\end{equation}\]

&lt;p&gt;Figure 1 gives an schematic illustration of this random walk as well as a visualization of many simulated runs&lt;sup id=&quot;fnref:d&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:d&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig1a.png&quot; width=&quot;30%&quot; /&gt;
   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig1b.svg&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;
	&lt;b&gt;Figure 1.&lt;/b&gt;
	Left: in the 1D discrete random walk, the walker moves left or right at each timestep with probability $1/2$.
	Right: simulated random walks.
	The upper plot shows a histogram of final positions (blue) and the theoretical distribution (red).
&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;h3 id=&quot;reversing-time&quot;&gt;Reversing time&lt;/h3&gt;

&lt;p&gt;The above random walks start at the origin at \(t=0\) and take uncorrelated steps from then on.
This is easy to simulate, as every step is independent of every other step.
Let us now imagine how these trajectories look to an observer moving backwards in time.
The walkers start in a wide distribution, take fairly random steps, but then all converge at the origin at \(t=0\).
Suppose you wish to find the statistics of this process from the point of view of this backwards observer: given that they observe a walker at position \(x_t\) at time \(t\), what are its transition probabilities for the step to time \(t - 1\)?
(Equivalently, we could forget about the time reversal and ask about the statistics of a path conditioned on its intersecting \((t, x_t)\), but that’s less fun.)&lt;/p&gt;

&lt;p&gt;We would like to obtain an update equation similar to Equation \(\ref{eqn:forward_step}\) which gives us the statistics of \(x_{t-1}\) as a function of \(x_t\) (because the random walk is a Markov process, we need not consider \(x_{t+1}\) and so on in finding the statistics of \(x_{t-1}\)).
We can do so using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;Bayes’ rule&lt;/a&gt;, which, applied to our case, tells us that&lt;/p&gt;

\[p \left( x_{t-1} | x_{t} \right) = \frac{ p \left( x_{t} | x_{t-1} \right) p \left( x_{t} \right) }{ p \left( x_{t-1}\right) }.\]

&lt;p&gt;Equation \(\ref{eqn:forward_step}\) gives \(p \left( x_{t} | x_{t-1} \right)\) and Equation \(\ref{eqn:p_forwards}\) tells us \(p(x_{t-1})\) and \(p(x_t)\).
Inserting the results and simplifying algebraically, we find that&lt;/p&gt;

\[\begin{equation}
\label{eqn:backward_step}
x_{t-1} =
\left\{
\begin{array}{lr}
x_{t} - 1, &amp;amp; p = \frac{x_t}{t} \\
x_{t}, &amp;amp; p = \frac{t - x_t}{t}.
\end{array}
\right.
\end{equation}\]

&lt;p&gt;The resulting backward process is illustrated schematically in the left subfigure of Figure 2.
Examining these transition probabilities, we see that, when run backwards in time, our random walk is biased towards taking a step right if \(x_t &amp;lt; \frac{t}{2}\) and biased towards taking a step left if \(x_t &amp;gt; \frac{t}{2}\), and that the probability of a step left interpolates linearly from \(0\) to \(1\) as \(x_t\) increases from \(0\) to \(t\).
This centerwards bias ensures the walker remains in the “lightcone” leading into the origin.
When \(x_t \approx \frac{t}{2}\), the step is roughly unbiased, and backward propagation is simply the same random walk as the forward propagation.
We generally expect \(x_t \approx \frac{t}{2}\) at large \(t\), and so the biasedness of the reverse process only comes into play at small \(t\) or when \(x_t\) drifts unusually close to the edges of \([0,t]\).&lt;/p&gt;

&lt;p&gt;The center and right subfigures of Figure 2 depict simulated reversed random walks starting at a particular point and run backwards to \(t=0\) using Equation \(\ref{eqn:backward_step}\).
The resulting trajectories look a lot like normal random walks for the first few steps, pick up a noticeable drift towards the origin as time proceeds, and quickly coalesce to the origin towards the end, in line with our expectation that the constraint of intersection with the origin is felt most strongly as \(t=0\) draws near.
It’s worth noting that we could have generated these trajectories by randomly sampling &lt;em&gt;forwards&lt;/em&gt; walks and only keeping those which intersect our chosen starting point, but directly simulating the backwards process is far more efficient, so our reversed-time transition probabilities are worth having.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig2a.png&quot; width=&quot;30%&quot; /&gt;
	&amp;nbsp;&amp;nbsp;&amp;nbsp;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig2b.svg&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;
	&lt;b&gt;Figure 2.&lt;/b&gt;
	Left: example transition probabilities at various points in a reversed random walk.
	Center: simulated reversed random walks starting at $t = 100, x_t = 10$.
	The red curve shows the mean position.
	Right: simulated reversed random walks starting at $t = 1000, x_t = 40$.
	The red curve shows the mean position.
&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;h3 id=&quot;jumps-of-many-timesteps&quot;&gt;Jumps of many timesteps&lt;/h3&gt;

&lt;p&gt;In all our analysis so far, we’ve only tried to move forwards or backwards in time one step at a time.
However, if we’re not interested in the intermediate trajectory, it’s easy to jump forwards multiple steps at a time, and it turns out we can do this backwards, too.
Consider two times \(t, t'\) with \(t' &amp;gt; t\).
Noting that the section of the random walk from \(t\) to \(t'\) is itself a random walk of length \(t' - t\), we find that&lt;/p&gt;

\[\begin{equation}
\label{eqn:p_forwards_jump}
p\left(x_{t'} | x_t\right) = 2^{-(t' - t)} \left( \begin{array}{c} t' - t \\ x_{t'} - x_t \end{array} \right).
\end{equation}\]

&lt;p&gt;With the same Bayesian approach as before, we can derive transition probabilities for a backwards jump.
This yields that&lt;/p&gt;

\[\begin{equation}
\label{eqn:p_backwards_jump}
p\left(x_{t} | x_{t'}\right) =
\frac{
	\left( \begin{array}{c} x_{t'} \\ x_{t} \end{array} \right)
	\left( \begin{array}{c} t' - x_{t'} \\ t - x_{t} \end{array} \right)
}{
	\left( \begin{array}{c} t' \\ t \end{array} \right)
},
\end{equation}\]

&lt;p&gt;which reduces to Equation \(\ref{eqn:backward_step}\) when \(t' - t = 1\).&lt;/p&gt;

&lt;h3 id=&quot;the-continuum-limit&quot;&gt;The continuum limit&lt;/h3&gt;

&lt;p&gt;While the discrete random walk is the easiest to grasp, in practice one often takes a &lt;em&gt;continuum limit&lt;/em&gt; in which the walker’s motion is continuous in both time and space.
This can be seen as the limit of a discrete-time random walk like the one studied above where, instead of taking \(\mathcal{O}(1)\) jumps with timesteps of size \(1\), our walker takes \(\mathcal{O}(dt^{1/2})\) jumps with timesteps of size \(dt\)&lt;sup id=&quot;fnref:e&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:e&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;To get to the continuum, we’ll first switch from a discrete random walk to one in which the steps are sampled from a Gaussian distribution.
This will make our walk continuous in &lt;em&gt;space&lt;/em&gt;, which will simplify the subsequent process of making it continuous in time.
We shall redefine our random walk as&lt;/p&gt;

\[x_0 = 0,\]

\[\begin{equation}
\label{eqn:forward_step_gaussian}
x_{t+dt} \sim \mathcal{N}(x_t, dt),
\end{equation}\]

&lt;p&gt;where \(dt\) is a small constant step size and \(\mathcal{N}(\mu,\sigma^2)\) is a Gaussian random variable with mean \(\mu\) and variance \(\sigma^2\).
Time-reversing this process with our same Bayesian trick, we find that&lt;/p&gt;

\[x_{t}
\sim
\mathcal{N}\left(
\frac{
x_{t+dt}
}{
1 + \frac{dt}{t}
},
\frac{
dt
}{
1 + \frac{dt}{t}
}
\right)
\approx
\mathcal{N}\left(
\left(1 - \frac{dt}{t}\right) x_{t+dt}
,
dt
\right),\]

&lt;p&gt;where in the last step we’ve simplified assuming that \(dt \ll t\).
Abusing notation a bit, we can write that&lt;/p&gt;

\[x_{t}
\approx
\left(1 - \frac{dt}{t}\right) x_{t+dt}
+
\mathcal{N}\left(0,dt\right).\]

&lt;p&gt;We can now simply take \(dt \rightarrow 0\) and arrive at a differential equation for the continuum limit, finding that&lt;/p&gt;

\[\begin{equation}
\label{eqn:sde}
\frac{d x(t)}{-dt} = - \frac{x(t)}{t} + \eta(t),
\end{equation}\]

&lt;p&gt;where \(\eta(t)\) is the classic white noise process defined by having mean zero and covariance \(\mathbb{E}[\eta(t)\eta(t')] = \delta(t - t')\).
This equation has a simple interpretation: this is an ordinary random walk with a pull towards the origin with strength \(t^{-1}\).
The strength of this pull diverges as \(t \rightarrow 0\), sucking the walker in to \(x = 0\).&lt;/p&gt;

&lt;p&gt;Neglecting the mean-zero driving noise and looking at only the drift term, we see that the mean of our distribution \(\mu(t) \equiv \mathbb{E}[x(t)]\) obeys&lt;/p&gt;

\[\frac{d \mu(t)}{-dt} = - \frac{\mu(t)}{t} \ \ \ \Rightarrow \ \ \ \mu(t) = C t\]

&lt;p&gt;for some constant \(C\).
This tells us that we expect that the mean of the distribution to approach zero linearly.
Looking at the red curves in Figure 2, we see this is exactly what happens.&lt;/p&gt;

&lt;h3 id=&quot;chasing-a-moving-target&quot;&gt;Chasing a moving target&lt;/h3&gt;

&lt;p&gt;Unlike the forward process, this reversed process isn’t stationary (i.e. time-translation invariant): as it runs, it approaches \(t = 0\), and its behavior changes.
Stationary processes are nice, though; is there simple a way we could modify it to make it \(t\)-independent?
One idea is to make it so the particle is chasing a moving target in a carrot-on-a-stick fashion: we fix some time \(T\) and “move” \(t=0\) so the random walk feels it is always a time \(T\) away from convergence.
The end of this process will always appear to be a fixed time away, much like the advent of quantum computing or the release of Half Life-3.
Mathematically, this corresponds to replacing \(t \rightarrow T\) in Equation \(\ref{eqn:sde}\), yielding&lt;/p&gt;

\[\frac{d x(t)}{-dt} = - \frac{x(t)}{T} + \eta(t).\]

&lt;p&gt;This process is stationary and has an even simpler interpretation.
In fact, neglecting the flipped direction of time, it’s exactly the classic &lt;a href=&quot;https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process&quot;&gt;Orenstein-Uhlenbeck process&lt;/a&gt;,
which describes a random walker in a quadratic potential.
Interestingly, the stationary distribution of this process is a centered Gaussian with mean \(\frac{T}{2}\), while from the forward walk we might expect it to have mean \(T\).
I was surprised by this; I made some sense of it by noting that, if we started with the stationary distribution of \(x_T\) and applied many backwards steps, it’d contract to a narrower distribution, but it still feels weird to me.&lt;/p&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;So, what are the prospects for our Tenet-inspired video game?
We’ve figured out how to model a simple random walk from the point of view of an observer moving backwards in time.
It’s easy to imagine doing a similar thing for a walker with more complicated rules, like wandering around a map or interacting with objects.
It’s also not too far-fetched to imagine rules that depend on the player, like a bias for moving towards the player or attacking with some probability, though we then run into the  difficulty that the player’s future actions are unknown (a problem that’s a cousin of our earlier paradoxes regarding free will).
This blocks our Bayesian calculation and needs a clever trick or two to give a good game mechanic.
If you’re game-design-minded and interested in thinking through this further, feel free to reach out!&lt;/p&gt;

&lt;h4 id=&quot;note-on-the-genesis-of-this-post&quot;&gt;Note on the genesis of this post&lt;/h4&gt;

&lt;p&gt;This idea was largely inspired by a post-&lt;em&gt;Tenet&lt;/em&gt; conversation with Vishal Talasani.
This writeup was made by starting with a blank file and randomly hitting a time-reversed delete key until it was done.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It turns out the weak nuclear force actually does violate time symmetry just a little, so you’d see a break in the laws of physics if you sent a certain atomic physics experiment backwards in time, but let’s ignore that for now. We’ll also neglect questions of quantum measurement. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:d&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The theoretical distribution plotted in Figure 1 (right) is actually a Gaussian distribution obtained by expanding Equation \(\ref{eqn:p_forwards}\) around its mean. This Gaussian is both easier to deal with mathematically and nicer to plot. &lt;a href=&quot;#fnref:d&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:e&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The \(dt^{1/2}\) scaling is necessary because we want the jump to have &lt;em&gt;variance&lt;/em&gt; \(dt\), because variances add, so the total variance of the process will grow like \(t\). If the steps were instead \(\mathcal{O}(dt)\), the total variance would grow like \(t dt \approx 0\), and our walker wouldn’t go anywhere. &lt;a href=&quot;#fnref:e&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 03 Sep 2022 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/time-reversed-random-walks/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/time-reversed-random-walks/</guid>
        
        
        <category>math, fun-science</category>
        
      </item>
    
      <item>
        <title>Reverse engineering the NTK</title>
        <description>&lt;p&gt;&lt;em&gt;This post also appeared on the &lt;a href=&quot;https://bair.berkeley.edu/blog/2022/08/29/reverse-engineering/&quot;&gt;BAIR blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Deep neural networks have enabled technological wonders ranging from voice recognition to machine transition to protein engineering, but their design and application is nonetheless notoriously unprincipled.
The development of tools and methods to guide this process is one of the grand challenges of deep learning theory.
In &lt;a href=&quot;https://arxiv.org/abs/2106.03186&quot;&gt;Reverse Engineering the Neural Tangent Kernel&lt;/a&gt;, we propose a paradigm for bringing some principle to the art of architecture design using recent theoretical breakthroughs: first design a good kernel function – often a much easier task – and then “reverse-engineer” a net-kernel equivalence to translate the chosen kernel into a neural network.
Our main theoretical result enables the design of activation functions from first principles, and we use it to create one activation function that mimics deep \(\textrm{ReLU}\) network performance with just one hidden layer and another that soundly outperforms deep \(\textrm{ReLU}\) networks on a synthetic task.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/rev_eng_fig1.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; Foundational works derived formulae that map from wide neural networks to their corresponding kernels. We obtain an inverse mapping, permitting us to start from a desired kernel and turn it back into a network architecture. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;neural-network-kernels&quot;&gt;&lt;strong&gt;Neural network kernels&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The field of deep learning theory has recently been transformed by the realization that deep neural networks often become analytically tractable to study in the &lt;em&gt;infinite-width&lt;/em&gt; limit.
Take the limit a certain way, and the network in fact converges to an ordinary kernel method using either the architecture’s &lt;a href=&quot;https://arxiv.org/abs/1806.07572&quot;&gt;“neural tangent kernel” (NTK)&lt;/a&gt; or, if only the last layer is trained (a la random feature models), its &lt;a href=&quot;https://arxiv.org/abs/1711.00165&quot;&gt;“neural network Gaussian process” (NNGP) kernel&lt;/a&gt;.
Like the central limit theorem, these wide-network limits are often surprisingly good approximations even far from infinite width (often holding true at widths in the hundreds or thousands), giving a remarkable analytical handle on the mysteries of deep learning.&lt;/p&gt;

&lt;!-- Consider, for perspective, how other fields of engineering operate: we start with a description of a problem, procedurally design a structure or system that solves it, and build it.
We normally find that our system behaves close to how we predicted, and if it doesn’t, we can understand its failings.
Deep learning, by contrast, is basically [alchemy](https://www.youtube.com/watch?v=x7psGHgatGM): despite much research, practitioners still have almost no principled methods for neural architecture design, and SOTA systems are often full of hacks and hyperparameters we might not need if we understood what we were doing.
As a result, the development of new methods is often slow and expensive, and even when we find clever new ideas, we often don't understand why they work as well as they do. --&gt;

&lt;h3 id=&quot;from-networks-to-kernels-and-back-again&quot;&gt;&lt;strong&gt;From networks to kernels and back again&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The original works exploring this net-kernel correspondence gave formulae for going from &lt;em&gt;architecture&lt;/em&gt; to &lt;em&gt;kernel&lt;/em&gt;: given a description of an architecture (e.g. depth and activation function), they give you the network’s two kernels.
This has allowed great insights into the optimization and generalization of various architectures of interest.
However, if our goal is not merely to understand existing architectures but to design &lt;em&gt;new&lt;/em&gt; ones, then we might rather have the mapping in the reverse direction: given a &lt;em&gt;kernel&lt;/em&gt; we want, can we find an &lt;em&gt;architecture&lt;/em&gt; that gives it to us?
In this work, we derive this inverse mapping for fully-connected networks (FCNs), allowing us to design simple networks in a principled manner by (a) positing a desired kernel and (b) designing an activation function that gives it.&lt;/p&gt;

&lt;p&gt;To see why this makes sense, let’s first visualize an NTK.
Consider a wide FCN’s NTK \(K(x_1,x_2)\) on two input vectors \(x_1\) and \(x_2\) (which we will for simplicity assume are normalized to the same length).
For a FCN, this kernel is &lt;em&gt;rotation-invariant&lt;/em&gt; in the sense that \(K(x_1,x_2) = K(c)\), where \(c\) is the cosine of the angle between the inputs.
Since \(K(c)\) is a scalar function of a scalar argument, we can simply plot it.
Fig. 2 shows the NTK of a four-hidden-layer (4HL) \(\textrm{ReLU}\) FCN.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/rev_eng_fig2.png&quot; width=&quot;65%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 2.&lt;/b&gt; The NTK of a 4HL $\textrm{ReLU}$ FCN as a function of the cosine between two input vectors $x_1$ and $x_2$. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This plot actually contains much information about the learning behavior of the corresponding wide network!
The monotonic increase means that this kernel expects closer points to have more correlated function values.
The steep increase at the end tells us that the correlation length is not too large, and it can fit complicated functions.
The diverging derivative at \(c=1\) tells us about the smoothness of the function we expect to get.
Importantly, &lt;em&gt;none of these facts are apparent from looking at a plot of \(\textrm{ReLU}(z)\)&lt;/em&gt;!
We claim that, if we want to understand the effect of choosing an activation function \(\phi\), then the resulting NTK is actually more informative than \(\phi\) itself.
It thus perhaps makes sense to try to design architectures in “kernel space,” then translate them to the typical hyperparameters.&lt;/p&gt;

&lt;h3 id=&quot;an-activation-function-for-every-kernel&quot;&gt;&lt;strong&gt;An activation function for every kernel&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Our main result is a “reverse engineering theorem” that states the following:&lt;/p&gt;

&lt;p style=&quot;padding: 10px; border: 2px solid black;&quot;&gt;
&lt;b&gt;Thm 1:&lt;/b&gt; For any kernel $K(c)$, we can construct an activation function $\tilde{\phi}$ such that, when inserted into a &lt;i&gt;single-hidden-layer&lt;/i&gt; FCN, its infinite-width NTK or NNGP kernel is $K(c)$.
&lt;/p&gt;

&lt;p&gt;We give an explicit formula for \(\tilde{\phi}\) in terms of Hermite polynomials
(though we use a different functional form in practice for trainability reasons).
Our proposed use of this result is that, in problems with some known structure, it’ll sometimes be possible to write down a good kernel and reverse-engineer it into a trainable network with various advantages over pure kernel regression, like computational efficiency and the ability to learn features.
As a proof of concept, we test this idea out on the synthetic &lt;em&gt;parity problem&lt;/em&gt; (i.e., given a bitstring, is the sum odd or even?), immediately generating an activation function that dramatically outperforms \(\text{ReLU}\) on the task.&lt;/p&gt;

&lt;h3 id=&quot;one-hidden-layer-is-all-you-need&quot;&gt;&lt;strong&gt;One hidden layer is all you need?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Here’s another surprising use of our result.
The kernel curve above is for a 4HL \(\textrm{ReLU}\) FCN, but I claimed that we can achieve any kernel, including that one, with just one hidden layer.
This implies we can come up with some new activation function \(\tilde{\phi}\) that gives this “deep” NTK in a &lt;em&gt;shallow network&lt;/em&gt;!
Fig. 3 illustrates this experiment.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/rev_eng_fig3.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 3.&lt;/b&gt; Shallowification of a deep $\textrm{ReLU}$ FCN into a 1HL FCN with an engineered activation function $\tilde{\phi}$. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;Surprisingly, this “shallowfication” actually works.
The left subplot of Fig. 4 below shows a “mimic” activation function \(\tilde{\phi}\) that gives virtually the same NTK as a deep \(\textrm{ReLU}\) FCN.
The right plots then show train + test loss + accuracy traces for three FCNs on a standard tabular problem from the UCI dataset.
Note that, while the shallow and deep ReLU networks have very different behaviors, our engineered shallow mimic network tracks the deep network almost exactly!&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img	/ntk-reveng/rev_eng_fig4.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 4.&lt;/b&gt; Left panel: our engineered &quot;mimic&quot; activation function, plotted with ReLU for comparison. Right panels: performance traces for 1HL ReLU, 4HL ReLU, and 1HL mimic FCNs trained on a UCI dataset. Note the close match between the 4HL ReLU and 1HL mimic networks.&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This is interesting from an engineering perspective because the shallow network uses fewer parameters than the deep network to achieve the same performance.
It’s also interesting from a theoretical perspective because it raises fundamental questions about the value of depth.
A common belief deep learning belief is that deeper is not only better but &lt;em&gt;qualitatively different&lt;/em&gt;: that deep networks will efficiently learn functions that shallow networks simply cannot.
Our shallowification result suggests that, at least for FCNs, this isn’t true: if we know what we’re doing, then depth doesn’t buy us anything.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;This work comes with lots of caveats.
The biggest is that our result only applies to FCNs, which alone are rarely state-of-the-art.
However, work on convolutional NTKs is &lt;a href=&quot;https://arxiv.org/abs/2112.05611&quot;&gt;fast progressing&lt;/a&gt;, and we believe this paradigm of designing networks by designing kernels is ripe for extension in some form to these structured architectures.&lt;/p&gt;

&lt;p&gt;Theoretical work has so far furnished relatively few tools for practical deep learning theorists.
We aim for this to be a modest step in that direction.
Even without a science to guide their design, neural networks have already enabled wonders.
Just imagine what we’ll be able to do with them once we finally have one.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is based on &lt;a href=&quot;https://arxiv.org/abs/2106.03186&quot;&gt;the paper&lt;/a&gt; “Reverse Engineering the Neural Tangent Kernel,” which is joint work with &lt;a href=&quot;https://www.sajant.com/&quot;&gt;Sajant Anand&lt;/a&gt; and &lt;a href=&quot;https://deweeselab.com/&quot;&gt;Mike DeWeese&lt;/a&gt;. We provide &lt;a href=&quot;https://github.com/james-simon/reverse-engineering&quot;&gt;code&lt;/a&gt; to reproduce all our results. We’d be delighted to field your questions or comments.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(It’s the belief of this author that deeper really is different for CNNs, and so studies aiming to understand the benefits of depth for generalization should focus on CNNs and other structured architectures. Also, careful readers might note that our analysis is in the kernel regime, but it’s possible that depth does buy us something in the feature-learning regime. However, there is weak evidence that &lt;em&gt;feature learning itself&lt;/em&gt; is undesirable for FCNs on realistic tasks, and that the best performance occurs in the kernel regime. For example, it is robustly found in many papers (including ours and &lt;a href=&quot;https://arxiv.org/abs/1711.00165&quot;&gt;Lee et. al (2018)&lt;/a&gt;) that “wider is better” for FCNs.) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 23 Aug 2022 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/reverse-engineering/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/reverse-engineering/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>Einstein vs. Bohr rap battle</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;They say an excellent scientist ought to know not just their field’s science but also its history. In a cynical move intended purely to improve our job prospects, Ashwin Singh and I decided to flaunt our copious knowledge of the history of physics by writing and performing an embarrassingly nerdy rap battle between Niels Bohr and Albert Einstein at our 2021 departmental holiday party and posting it on the internet. Potential employers can find the recording below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;600&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/k7Nj7IUHveY&quot;&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2 align=&quot;center&quot;&gt;
LYRICS
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;[BOHR]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Do you really want to reignite this debate? \ History’s already set the record straight
&lt;/p&gt;
&lt;p&gt;Why you chose to battle me I can only wonder \ It’ll go down as your greatest blunder&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You started anti-quantum but then slowly changed your ways \ I guess that your denial was just a Berry phase
&lt;/p&gt;
&lt;p&gt;Your life was full of sexual activity but \ you married your cousin? That’s “special relativity”!&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
All you know is maths \ and if you built an Einstein Rosen bridge, I bet it would collapse.
&lt;/p&gt;
&lt;p&gt;And your famous shorthand is pure castration \ Cutting off sums? That’s Einstein &lt;em&gt;dumb&lt;/em&gt; notation&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
To prove quantum, look at Einstein at his prime \ one man with two women at the same damn time!
&lt;/p&gt;
&lt;p&gt;But it only works theoretically \ A million extra qubits won’t correct that infidelity&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You’ve many talents, especially misogyny \ People tend to like you, except for your progeny
&lt;/p&gt;
&lt;p&gt;I’m a better father and it isn’t hard to tell \ Your son you didn’t know well; my son won a Nobel!&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m creative, made s p d and f \ and I’ve got *a dagger* but I’m boutta act left
&lt;/p&gt;
&lt;p&gt;Give up Einstein - you’re not a match for me \ or I’ll annihilate you in an ultraviolent catastrophe&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[EINSTEIN]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Einstein’s my name, but “mc” will suffice \ watch out - this rap god does not play nice
&lt;/p&gt;
&lt;p&gt;You’re one great Dane, I have to admit \ but now I’m taking the stage, so roll over and sit&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m General Relativity, the field’s commanding officer \ You? Eh, you’re more of a philosopher
&lt;/p&gt;
&lt;p&gt;I’m king of the cosmos; I fused space and time \ now watch me unify rhythm and rhyme&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
No modern man can match the breadth of things that I wrote on \ I'd still be above your level if I’d omitted the photon
&lt;/p&gt;
&lt;p&gt;I’m a hero of theory, stand a light-year tall \ Even in your rest frame, you’re relatively small&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I solved mysteries from gravity to light \ You couldn’t even get hydrogen right
&lt;/p&gt;
&lt;p&gt;Here’s a new Bohr model (and this one won’t flop): \ physics has levels, and I’m on top.&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m a star on both sides of the ocean \ Every time I speak I cause a Brownian commotion
&lt;/p&gt;
&lt;p&gt;Your lectures are dull, though I’d not known before \ that one could stretch time just by being a Bohr&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
In this deterministic world one can predict what comes to pass \ and I can prove my geodesic leads my foot into your [CENSORED]
&lt;/p&gt;
&lt;p&gt;Oh, and as for women, why don’t you just ask your wife \ if Einsteinium or Bohrium’s got the longer half-life?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[BOHR]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Those lines were so bad I’m nauseous \ Tell me did you learn them at the patent office?
&lt;/p&gt;
&lt;p&gt;I’m Bohr Dissing Einstein, got BDE \ You’re more degenerate than a BEC&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
In history you’re an eccentric defector \ who's easily perturbed like the Runge Lenz vector
&lt;/p&gt;
&lt;p&gt;Your math went missing on many occasions: \ you couldn’t even solve your own field equations&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
So much wrong about how things behave \ like “uh there’s no energy in gravitational waves”
&lt;/p&gt;
&lt;p&gt;Oh, and what do people think of EPR? \ Yeah, that theory’s gonna need some CPR&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You also never understood Schrödinger’s cat \ Imagine tonight at the start of the act
&lt;/p&gt;
&lt;p&gt;they see you and me both behind and ahead \ but after my verse they realize that you’re dead&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Oh my conjugate man \ you must be feeling uncertain cause I know where I stand
&lt;/p&gt;
&lt;p&gt;All eyes are on me, but you’re in superposition \ ‘cause to not observe you is a superb omission&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m done, and there’s one clear conclusion \ I'm the only winner here by Pauli’s exclusion
&lt;/p&gt;
&lt;p&gt;Oh, and for Einsteinium, let’s use a bit of rigor: \ every chemist knows that Bohrium is bigger&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[EINSTEIN]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
When I walk around Princeton, the ladies all stare \ even black holes tell me “ah, I wish I had your hair!”
&lt;/p&gt;
&lt;p&gt;In attractiveness, I am the crest, you’re the trough \ there’s a name for the phenomenon: “the Bohr magnet-off”&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Forget singularities, folks, we’ve found something denser \ scare me all you want, you can’t make Einstein tensor
&lt;/p&gt;
&lt;p&gt;I built my own fame, you relied on your betters; \ guess that explains why you published our letters&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I'm unchallenged master of problems abstract \ I think so fast your thoughts Lorentz-contract
&lt;/p&gt;
&lt;p&gt;Yeah, you might as well call me “c” \ ‘cause you’ve got no chance of catching up with me&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Socially, I’m a champion of simple coexistence \ resisting and assisting, consistently insisting
&lt;/p&gt;
&lt;p&gt;politically, your presence more or less is nonexistent \ you can’t spookily better the world from a distance.&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Here’s a little lesson in relativity \ there’s a twin paradox here between you and me:
&lt;/p&gt;
&lt;p&gt;see, since I ran circles ‘round you all life long \ your work’s long dead, but mine’s still going strong&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m generally special; my work’s especially general \ My greatness endures, Bohr, but yours was ephemeral
&lt;/p&gt;
&lt;p&gt;Your nanoscopic jabs at me are hardly worth refutin’ \ When all’s said and done, bud, you’re Leibniz, I’m Newton.&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Jan 2022 01:01:00 -0800</pubDate>
        <link>http://localhost:4000/blog/bohr-v-einstein/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/bohr-v-einstein/</guid>
        
        
        <category>physics, random</category>
        
      </item>
    
      <item>
        <title>Newton vs. Leibniz rap battle</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;The spiteful grudge between these calculus co-creators is among the best-known disputes in the history of science. In 2019, fellow first-year grad Ashwin Singh and I wrote a rap battle between these men and performed it at the Berkeley physics department’s annual holiday party. A post-facto recording is below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;600&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/COeKdP3EkXU&quot;&gt;
&lt;/iframe&gt;
&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Jan 2022 01:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/newton-v-leibniz/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/newton-v-leibniz/</guid>
        
        
        <category>physics, random, rap-battle</category>
        
      </item>
    
  </channel>
</rss>
