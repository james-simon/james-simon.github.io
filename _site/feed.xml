<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JS</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 29 Sep 2022 14:43:09 -0800</pubDate>
    <lastBuildDate>Thu, 29 Sep 2022 14:43:09 -0800</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Simulating cells fighting to the death</title>
        <description>&lt;p&gt;Physics is humanity’s finest tool for understanding the world around us, a rich and wonderful framework deserving of our highest reverence.
I recently used it to simulate a bunch of cells fighting to the death like little gladiators.
Here’s a video.&lt;/p&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;60%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/cell_fight/cell_fight.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I’ll give a lightning overview of how this works.
This is a simulation lying on a grid, with every color either empty or part of a cell.
The grid’s evolving in time according to a modified version of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cellular_Potts_model&quot;&gt;cellular Potts model&lt;/a&gt;, which is basically the &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;&gt;Ising model&lt;/a&gt; plus a few terms.
In the Ising model, each cell prefers to match its neighbors, so the grid forms big regions of the same color.
Here’s the Ising model:&lt;/p&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;60%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/cell_fight/ising_test.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;Skipping a mathematical explanation, the basic algorithm at play here is the following: at every timestep, (a) choose a random grid site and (b) flip its value with some probability.
The probability of flipping is higher if flipping will make it more like its neighbors, and this is coded in terms of an “energy” which is lower the more sites match their neighbors.&lt;/p&gt;

&lt;p&gt;The Potts model is a generalization to more than two values.
In the &lt;em&gt;cellular&lt;/em&gt; Potts model, we simply add another term to the energy.
For each value \(i\) besides the “empty value” \(0\), we count up the number of sites of type \(i\) and penalize the difference from a target volume \(V_i\).
This enforces that the sites with value \(i\) have total volume around \(V_i\), and the Ising term from before makes it so these sites tend to stick together, forming a round droplet like water molecules.
Here’s the cellular Potts model with a few randomly-placed cells:&lt;/p&gt;

&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;60%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/cell_fight/stationary_cells.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;To make them fight, I just added a third energy term which makes a site more likely to flip to the color of cell \(i\) if doing so would move the center of mass of cell \(i\) towards its nearest neighbor, and then added the ad hoc rule that, when one cell loses a site to another cell, its target volume \(V_i\) decreases to track its loss of hitpoints.&lt;/p&gt;

&lt;p&gt;Superficially, this is just a cute simulation of some artificial cells, but, for me, it gets at one of the deep wonders of life.
These cells exhibit high-level “purposeful” behavior, but their motion is entirelly driven by extremely simple low-level rules (and they’re not even deterministic!).
Just like in a real organism - just like in &lt;em&gt;us&lt;/em&gt;, arguably - this purposeful behavior emerges from the interaction of many pseudorandom low-level components.
These cells were not programmed via top-down rules, as one might code a videogame enemy (walk towards player, move limbs while doing so, fire when X feet away, etc.); their high-level behavior emerges from the bottom up.&lt;/p&gt;

&lt;p&gt;That said, it seemed like it’d be pretty fun to be able to control them manually, so I made them take keyboard input and gave them the ability to fire bullets.
Here’s a fight between my brother and I.&lt;/p&gt;

&lt;p&gt;[VIDEO]&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Sep 2022 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/cell-fight/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/cell-fight/</guid>
        
        
        <category>physics, fun-science</category>
        
      </item>
    
      <item>
        <title>Time-reversed random walks</title>
        <description>&lt;p&gt;A year or so ago, at the height of the pandemic, a few friends and I watched &lt;em&gt;Tenet (2020)&lt;/em&gt;, an action movie whose core conceit is that certain people and objects move the opposite direction through time.
The idea’s not that they time-travel or that they merely age backwards, it’s that they’re truly time-reversed, appearing to ordinary observers like a video played backwards.
To get an idea of what I mean, &lt;a href=&quot;https://www.youtube.com/watch?v=4xj0KRqzo-0&quot;&gt;here’s&lt;/a&gt; a clip of the protagonists fighting two time-reversed opponents.&lt;/p&gt;

&lt;p&gt;Does this concept make sense?
Thinking about it for a bit, one immediately bumps up against a number of free-will-related paradoxes.
For example, if you start a fight with a time-reversed adversary, it seems you’re guaranteed you won’t incapacitate them because you observed them up and fighting at a time that’s earlier for you but later for them (if you are to win, you must have “recapactitated” them with your first blow).
Similarly, it seems you couldn’t break a time-reversed vase or light a time-reversed match, and things would probably end badly if you tried to eat a time-reversed sandwich.
Ultimately, however, these are paradoxes of human agency, and if you remove free will as a consideration, these events’ impossibility feels more believable.
Accepting the lack of free will, is the movie’s core idea physically possible?&lt;/p&gt;

&lt;p&gt;At a first glance, it looks like several scenes clearly violate Newtonian mechanics, such as &lt;a href=&quot;https://youtu.be/L3pk_TBkihU?t=41&quot;&gt;this scene&lt;/a&gt; in which a gear jumps off a table to meet a hand that dropped it backwards in time.
However, it turns out this is not strictly impossible: the thermally jiggling particles of the surface beneath the gear could all come together and move in the same direction, giving the gear a big push upwards and leaving the surface a little colder.
This is precisely the time-reversed process of the forwards event of dropping the gear.
This sequence of steps seems absurdly implausible - and in our world, it would be - but it doesn’t break conservation of momentum or energy or violate any ironclad law of physics.
These sorts of backwards processes are always possible because the laws of Newtonian physics and electromagnetism are &lt;a href=&quot;https://en.wikipedia.org/wiki/T-symmetry&quot;&gt;time-reversal symmetric&lt;/a&gt;&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;It turns out the apparent flow of time in the world we see around us is due principally to thermodynamics, a statistical set of effective laws that emerge from the basic rules of the universe when considering macroscopic systems like vases and Christopher Nolan.
Time-reversed versions of events are not fundamentally impossible even in ordinary circunstances, they’re merely much less likely than their forwards-time counterparts.
In practice, the gear will stay on the surface for as long as you watch it, but there is indeed a miniscule probability it’ll jump up to meet your hand each moment.
In a very deep sense, fragile objects shatter forwards in time because, well, they’re not broken now, and there are lots of ways they can break, so statistics doesn’t have to work too hard to make it happen.
If you sweep together a bunch of glass shards, however, there’s only one way they can all fit together, so their perfect coalescence is very unlikely.
In our universe, things happen to generally be more ordered back in the direction we call the past on account of the Big Bang.
However, if you took another universe and enforced the constraint that things are ordered and nice at some &lt;em&gt;final&lt;/em&gt; time, time-reversal would give you a universe flowing “backwards” in time, though its inhabitants would of course never know the difference.
This is the difference between specifying the initial and final conditions of a dynamical process, and this is no difference at all for a time-symmetric process.&lt;/p&gt;

&lt;p&gt;Okay, but how do you get things moving both forwards and backwards?
This is weird, since you normally can’t specify both initial &lt;em&gt;and&lt;/em&gt; final conditions for a dynamical process.
However, one way to do it and avoid a paradox might be to specify only &lt;em&gt;partial&lt;/em&gt; information at both points in time.
For example, suppose I enforce that the left half of a room contains person A at time \(t=0\), and the right half of the same room contains person B at time \(t=T\), and I don’t specify anything else.
This is an underdetermined system which is likely to have many solutions.
Actually finding one would be hard - you’d probably have to iterate forwards and backwards in time for a bunch of cycles in a nonlocal fashion until you satisfy all the boundary conditions - but it’s not impossible in principle.
Given appropriate brain states in these boundary conditions, the obtained dynamical solution might be these two people fighting.
This way of thinking about the problem is weird and nonlocal, but so is Tenet, and it feels more satisfying to me than prioritizing one time direction and viewing time-reversed events like the leap of the gear as simply a series of flukes.&lt;/p&gt;

&lt;p&gt;This is a concrete enough way of looking at the problem that we could start to think about how to simulate it.
The ultimate dream here would be a video game in which you fight and interact with creatures moving backwards in time.
In order to make that happen, though, we need to understand how to model rule-based processes from the point of view of an observer moving the other way in time.
The rest of this post obtains the math for doing this using the simplest dynamical model in thermodynamics, the random walk.&lt;/p&gt;

&lt;h3 id=&quot;the-1d-discrete-random-walk&quot;&gt;The 1D discrete random walk&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;&gt;random walk&lt;/a&gt; is a basic stochastic process in which a particle, which we refer to as a walker, explores a space by taking steps in random directions.
This process is used to model a swath of phenomena including the paths of diffusing particles, fluctuating stock prices, and the movements of foraging animals, and it’s found use in essentially every discipline of science.&lt;/p&gt;

&lt;p&gt;The simplest example of this process is a 1D random walk with discrete steps.
Let \(x_0, x_1, x_2, ..., x_t, ...\) be integers denoting the position of a random walker on the real line at each timestep.
The initial condition and transition dynamics are&lt;/p&gt;

\[x_0 = 0,\]

\[\begin{equation}
\label{eqn:forward_step}
x_{t+1} =
\left\{
\begin{array}{lr}
x_t, &amp;amp; p = \frac{1}{2} \\
x_t + 1, &amp;amp; p = \frac{1}{2}.
\end{array}
\right.
\end{equation}\]

&lt;p&gt;In words, the random walker begins at zero and, at each timestep, flips a coin and either remains put or moves to \(x \rightarrow x + 1\) accordingly.
(It’s worth noting that we could’ve also chosen the possible steps to be not \(\{0,1\}\) but rather \(\{-1,1\}\) to ensure that the random walk has mean zero.
It will prove mathematically simpler to use our current formulation, but we can always recover the latter setting by constructing \(\tilde{x}_t \equiv 2 x_t - t\).
We will use \(\tilde{x}_t\) in our visualizations below, and we will refer to the two possible steps as “leftwards” and “rightwards.”)&lt;/p&gt;

&lt;p&gt;Suppose we want to know \(p(x_t)\), the probability of the walker being at position \(x_t\) at time \(t\).
By placing this random walk on Pascal’s triangle, it’s easy to show that this distribution is given by&lt;/p&gt;

\[\begin{equation} \label{eqn:p_forwards}
p(x_t) = 2^{-t} \left( \begin{array}{c} t \\ x_t \end{array} \right).
\end{equation}\]

&lt;p&gt;Figure 1 gives an schematic illustration of this random walk as well as a visualization of many simulated runs&lt;sup id=&quot;fnref:d&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:d&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig1a.png&quot; width=&quot;30%&quot; /&gt;
   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig1b.svg&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;
	&lt;b&gt;Figure 1.&lt;/b&gt;
	Left: in the 1D discrete random walk, the walker moves left or right at each timestep with probability $1/2$.
	Right: simulated random walks.
	The upper plot shows a histogram of final positions (blue) and the theoretical distribution (red).
&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;h3 id=&quot;reversing-time&quot;&gt;Reversing time&lt;/h3&gt;

&lt;p&gt;The above random walks start at the origin at \(t=0\) and take uncorrelated steps from then on.
This is easy to simulate, as every step is independent of every other step.
Let us now imagine how these trajectories look to an observer moving backwards in time.
The walkers start in a wide distribution, take fairly random steps, but then all converge at the origin at \(t=0\).
Suppose you wish to find the statistics of this process from the point of view of this backwards observer: given that they observe a walker at position \(x_t\) at time \(t\), what are its transition probabilities for the step to time \(t - 1\)?
(Equivalently, we could forget about the time reversal and ask about the statistics of a path conditioned on its intersecting \((t, x_t)\), but that’s less fun.)&lt;/p&gt;

&lt;p&gt;We would like to obtain an update equation similar to Equation \(\ref{eqn:forward_step}\) which gives us the statistics of \(x_{t-1}\) as a function of \(x_t\) (because the random walk is a Markov process, we need not consider \(x_{t+1}\) and so on in finding the statistics of \(x_{t-1}\)).
We can do so using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;Bayes’ rule&lt;/a&gt;, which, applied to our case, tells us that&lt;/p&gt;

\[p \left( x_{t-1} | x_{t} \right) = \frac{ p \left( x_{t} | x_{t-1} \right) p \left( x_{t} \right) }{ p \left( x_{t-1}\right) }.\]

&lt;p&gt;Equation \(\ref{eqn:forward_step}\) gives \(p \left( x_{t} | x_{t-1} \right)\) and Equation \(\ref{eqn:p_forwards}\) tells us \(p(x_{t-1})\) and \(p(x_t)\).
Inserting the results and simplifying algebraically, we find that&lt;/p&gt;

\[\begin{equation}
\label{eqn:backward_step}
x_{t-1} =
\left\{
\begin{array}{lr}
x_{t} - 1, &amp;amp; p = \frac{x_t}{t} \\
x_{t}, &amp;amp; p = \frac{t - x_t}{t}.
\end{array}
\right.
\end{equation}\]

&lt;p&gt;The resulting backward process is illustrated schematically in the left subfigure of Figure 2.
Examining these transition probabilities, we see that, when run backwards in time, our random walk is biased towards taking a step right if \(x_t &amp;lt; \frac{t}{2}\) and biased towards taking a step left if \(x_t &amp;gt; \frac{t}{2}\), and that the probability of a step left interpolates linearly from \(0\) to \(1\) as \(x_t\) increases from \(0\) to \(t\).
This centerwards bias ensures the walker remains in the “lightcone” leading into the origin.
When \(x_t \approx \frac{t}{2}\), the step is roughly unbiased, and backward propagation is simply the same random walk as the forward propagation.
We generally expect \(x_t \approx \frac{t}{2}\) at large \(t\), and so the biasedness of the reverse process only comes into play at small \(t\) or when \(x_t\) drifts unusually close to the edges of \([0,t]\).&lt;/p&gt;

&lt;p&gt;The center and right subfigures of Figure 2 depict simulated reversed random walks starting at a particular point and run backwards to \(t=0\) using Equation \(\ref{eqn:backward_step}\).
The resulting trajectories look a lot like normal random walks for the first few steps, pick up a noticeable drift towards the origin as time proceeds, and quickly coalesce to the origin towards the end, in line with our expectation that the constraint of intersection with the origin is felt most strongly as \(t=0\) draws near.
It’s worth noting that we could have generated these trajectories by randomly sampling &lt;em&gt;forwards&lt;/em&gt; walks and only keeping those which intersect our chosen starting point, but directly simulating the backwards process is far more efficient, so our reversed-time transition probabilities are worth having.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig2a.png&quot; width=&quot;30%&quot; /&gt;
	&amp;nbsp;&amp;nbsp;&amp;nbsp;
   &lt;img src=&quot;https://james-simon.github.io/img/reversed_rws/rev_rws_fig2b.svg&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;
	&lt;b&gt;Figure 2.&lt;/b&gt;
	Left: example transition probabilities at various points in a reversed random walk.
	Center: simulated reversed random walks starting at $t = 100, x_t = 10$.
	The red curve shows the mean position.
	Right: simulated reversed random walks starting at $t = 1000, x_t = 40$.
	The red curve shows the mean position.
&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;h3 id=&quot;jumps-of-many-timesteps&quot;&gt;Jumps of many timesteps&lt;/h3&gt;

&lt;p&gt;In all our analysis so far, we’ve only tried to move forwards or backwards in time one step at a time.
However, if we’re not interested in the intermediate trajectory, it’s easy to jump forwards multiple steps at a time, and it turns out we can do this backwards, too.
Consider two times \(t, t'\) with \(t' &amp;gt; t\).
Noting that the section of the random walk from \(t\) to \(t'\) is itself a random walk of length \(t' - t\), we find that&lt;/p&gt;

\[\begin{equation}
\label{eqn:p_forwards_jump}
p\left(x_{t'} | x_t\right) = 2^{-(t' - t)} \left( \begin{array}{c} t' - t \\ x_{t'} - x_t \end{array} \right).
\end{equation}\]

&lt;p&gt;With the same Bayesian approach as before, we can derive transition probabilities for a backwards jump.
This yields that&lt;/p&gt;

\[\begin{equation}
\label{eqn:p_backwards_jump}
p\left(x_{t} | x_{t'}\right) =
\frac{
	\left( \begin{array}{c} x_{t'} \\ x_{t} \end{array} \right)
	\left( \begin{array}{c} t' - x_{t'} \\ t - x_{t} \end{array} \right)
}{
	\left( \begin{array}{c} t' \\ t \end{array} \right)
},
\end{equation}\]

&lt;p&gt;which reduces to Equation \(\ref{eqn:backward_step}\) when \(t' - t = 1\).&lt;/p&gt;

&lt;h3 id=&quot;the-continuum-limit&quot;&gt;The continuum limit&lt;/h3&gt;

&lt;p&gt;While the discrete random walk is the easiest to grasp, in practice one often takes a &lt;em&gt;continuum limit&lt;/em&gt; in which the walker’s motion is continuous in both time and space.
This can be seen as the limit of a discrete-time random walk like the one studied above where, instead of taking \(\mathcal{O}(1)\) jumps with timesteps of size \(1\), our walker takes \(\mathcal{O}(dt^{1/2})\) jumps with timesteps of size \(dt\)&lt;sup id=&quot;fnref:e&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:e&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;To get to the continuum, we’ll first switch from a discrete random walk to one in which the steps are sampled from a Gaussian distribution.
This will make our walk continuous in &lt;em&gt;space&lt;/em&gt;, which will simplify the subsequent process of making it continuous in time.
We shall redefine our random walk as&lt;/p&gt;

\[x_0 = 0,\]

\[\begin{equation}
\label{eqn:forward_step_gaussian}
x_{t+dt} \sim \mathcal{N}(x_t, dt),
\end{equation}\]

&lt;p&gt;where \(dt\) is a small constant step size and \(\mathcal{N}(\mu,\sigma^2)\) is a Gaussian random variable with mean \(\mu\) and variance \(\sigma^2\).
Time-reversing this process with our same Bayesian trick, we find that&lt;/p&gt;

\[x_{t}
\sim
\mathcal{N}\left(
\frac{
x_{t+dt}
}{
1 + \frac{dt}{t}
},
\frac{
dt
}{
1 + \frac{dt}{t}
}
\right)
\approx
\mathcal{N}\left(
\left(1 - \frac{dt}{t}\right) x_{t+dt}
,
dt
\right),\]

&lt;p&gt;where in the last step we’ve simplified assuming that \(dt \ll t\).
Abusing notation a bit, we can write that&lt;/p&gt;

\[x_{t}
\approx
\left(1 - \frac{dt}{t}\right) x_{t+dt}
+
\mathcal{N}\left(0,dt\right).\]

&lt;p&gt;We can now simply take \(dt \rightarrow 0\) and arrive at a differential equation for the continuum limit, finding that&lt;/p&gt;

\[\begin{equation}
\label{eqn:sde}
\frac{d x(t)}{-dt} = - \frac{x(t)}{t} + \eta(t),
\end{equation}\]

&lt;p&gt;where \(\eta(t)\) is the classic white noise process defined by having mean zero and covariance \(\mathbb{E}[\eta(t)\eta(t')] = \delta(t - t')\).
This equation has a simple interpretation: this is an ordinary random walk with a pull towards the origin with strength \(t^{-1}\).
The strength of this pull diverges as \(t \rightarrow 0\), sucking the walker in to \(x = 0\).&lt;/p&gt;

&lt;p&gt;Neglecting the mean-zero driving noise and looking at only the drift term, we see that the mean of our distribution \(\mu(t) \equiv \mathbb{E}[x(t)]\) obeys&lt;/p&gt;

\[\frac{d \mu(t)}{-dt} = - \frac{\mu(t)}{t} \ \ \ \Rightarrow \ \ \ \mu(t) = C t\]

&lt;p&gt;for some constant \(C\).
This tells us that we expect that the mean of the distribution to approach zero linearly.
Looking at the red curves in Figure 2, we see this is exactly what happens.&lt;/p&gt;

&lt;h3 id=&quot;chasing-a-moving-target&quot;&gt;Chasing a moving target&lt;/h3&gt;

&lt;p&gt;Unlike the forward process, this reversed process isn’t stationary (i.e. time-translation invariant): as it runs, it approaches \(t = 0\), and its behavior changes.
Stationary processes are nice, though; is there simple a way we could modify it to make it \(t\)-independent?
One idea is to make it so the particle is chasing a moving target in a carrot-on-a-stick fashion: we fix some time \(T\) and “move” \(t=0\) so the random walk feels it is always a time \(T\) away from convergence.
The end of this process will always appear to be a fixed time away, much like the advent of quantum computing or the release of Half Life-3.
Mathematically, this corresponds to replacing \(t \rightarrow T\) in Equation \(\ref{eqn:sde}\), yielding&lt;/p&gt;

\[\frac{d x(t)}{-dt} = - \frac{x(t)}{T} + \eta(t).\]

&lt;p&gt;This process is stationary and has an even simpler interpretation.
In fact, neglecting the flipped direction of time, it’s exactly the classic &lt;a href=&quot;https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process&quot;&gt;Orenstein-Uhlenbeck process&lt;/a&gt;,
which describes a random walker in a quadratic potential.
Interestingly, the stationary distribution of this process is a centered Gaussian with mean \(\frac{T}{2}\), while from the forward walk we might expect it to have mean \(T\).
I was surprised by this; I made some sense of it by noting that, if we started with the stationary distribution of \(x_T\) and applied many backwards steps, it’d contract to a narrower distribution, but it still feels weird to me.&lt;/p&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;So, what are the prospects for our Tenet-inspired video game?
We’ve figured out how to model a simple random walk from the point of view of an observer moving backwards in time.
It’s easy to imagine doing a similar thing for a walker with more complicated rules, like wandering around a map or interacting with objects.
It’s also not too far-fetched to imagine rules that depend on the player, like a bias for moving towards the player or attacking with some probability, though we then run into the  difficulty that the player’s future actions are unknown (a problem that’s a cousin of our earlier paradoxes regarding free will).
This blocks our Bayesian calculation and needs a clever trick or two to give a good game mechanic.
If you’re game-design-minded and interested in thinking through this further, feel free to reach out!&lt;/p&gt;

&lt;h4 id=&quot;note-on-the-genesis-of-this-post&quot;&gt;Note on the genesis of this post&lt;/h4&gt;

&lt;p&gt;This idea was largely inspired by a post-&lt;em&gt;Tenet&lt;/em&gt; conversation with Vishal Talasani.
This writeup was made by starting with a blank file and randomly hitting a time-reversed delete key until it was done.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It turns out the weak nuclear force actually does violate time symmetry just a little, so you’d see a break in the laws of physics if you sent a certain atomic physics experiment backwards in time, but let’s ignore that for now. We’ll also neglect questions of quantum measurement. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:d&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The theoretical distribution plotted in Figure 1 (right) is actually a Gaussian distribution obtained by expanding Equation \(\ref{eqn:p_forwards}\) around its mean. This Gaussian is both easier to deal with mathematically and nicer to plot. &lt;a href=&quot;#fnref:d&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:e&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The \(dt^{1/2}\) scaling is necessary because we want the jump to have &lt;em&gt;variance&lt;/em&gt; \(dt\), because variances add, so the total variance of the process will grow like \(t\). If the steps were instead \(\mathcal{O}(dt)\), the total variance would grow like \(t dt \approx 0\), and our walker wouldn’t go anywhere. &lt;a href=&quot;#fnref:e&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 03 Sep 2022 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/time-reversed-random-walks/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/time-reversed-random-walks/</guid>
        
        
        <category>math, fun-science</category>
        
      </item>
    
      <item>
        <title>Reverse engineering the NTK</title>
        <description>&lt;p&gt;&lt;em&gt;This post also appeared on the &lt;a href=&quot;https://bair.berkeley.edu/blog/2022/08/29/reverse-engineering/&quot;&gt;BAIR blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Deep neural networks have enabled technological wonders ranging from voice recognition to machine transition to protein engineering, but their design and application is nonetheless notoriously unprincipled.
The development of tools and methods to guide this process is one of the grand challenges of deep learning theory.
In &lt;a href=&quot;https://arxiv.org/abs/2106.03186&quot;&gt;Reverse Engineering the Neural Tangent Kernel&lt;/a&gt;, we propose a paradigm for bringing some principle to the art of architecture design using recent theoretical breakthroughs: first design a good kernel function – often a much easier task – and then “reverse-engineer” a net-kernel equivalence to translate the chosen kernel into a neural network.
Our main theoretical result enables the design of activation functions from first principles, and we use it to create one activation function that mimics deep \(\textrm{ReLU}\) network performance with just one hidden layer and another that soundly outperforms deep \(\textrm{ReLU}\) networks on a synthetic task.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/rev_eng_fig1.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; Foundational works derived formulae that map from wide neural networks to their corresponding kernels. We obtain an inverse mapping, permitting us to start from a desired kernel and turn it back into a network architecture. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;neural-network-kernels&quot;&gt;&lt;strong&gt;Neural network kernels&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The field of deep learning theory has recently been transformed by the realization that deep neural networks often become analytically tractable to study in the &lt;em&gt;infinite-width&lt;/em&gt; limit.
Take the limit a certain way, and the network in fact converges to an ordinary kernel method using either the architecture’s &lt;a href=&quot;https://arxiv.org/abs/1806.07572&quot;&gt;“neural tangent kernel” (NTK)&lt;/a&gt; or, if only the last layer is trained (a la random feature models), its &lt;a href=&quot;https://arxiv.org/abs/1711.00165&quot;&gt;“neural network Gaussian process” (NNGP) kernel&lt;/a&gt;.
Like the central limit theorem, these wide-network limits are often surprisingly good approximations even far from infinite width (often holding true at widths in the hundreds or thousands), giving a remarkable analytical handle on the mysteries of deep learning.&lt;/p&gt;

&lt;!-- Consider, for perspective, how other fields of engineering operate: we start with a description of a problem, procedurally design a structure or system that solves it, and build it.
We normally find that our system behaves close to how we predicted, and if it doesn’t, we can understand its failings.
Deep learning, by contrast, is basically [alchemy](https://www.youtube.com/watch?v=x7psGHgatGM): despite much research, practitioners still have almost no principled methods for neural architecture design, and SOTA systems are often full of hacks and hyperparameters we might not need if we understood what we were doing.
As a result, the development of new methods is often slow and expensive, and even when we find clever new ideas, we often don't understand why they work as well as they do. --&gt;

&lt;h3 id=&quot;from-networks-to-kernels-and-back-again&quot;&gt;&lt;strong&gt;From networks to kernels and back again&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The original works exploring this net-kernel correspondence gave formulae for going from &lt;em&gt;architecture&lt;/em&gt; to &lt;em&gt;kernel&lt;/em&gt;: given a description of an architecture (e.g. depth and activation function), they give you the network’s two kernels.
This has allowed great insights into the optimization and generalization of various architectures of interest.
However, if our goal is not merely to understand existing architectures but to design &lt;em&gt;new&lt;/em&gt; ones, then we might rather have the mapping in the reverse direction: given a &lt;em&gt;kernel&lt;/em&gt; we want, can we find an &lt;em&gt;architecture&lt;/em&gt; that gives it to us?
In this work, we derive this inverse mapping for fully-connected networks (FCNs), allowing us to design simple networks in a principled manner by (a) positing a desired kernel and (b) designing an activation function that gives it.&lt;/p&gt;

&lt;p&gt;To see why this makes sense, let’s first visualize an NTK.
Consider a wide FCN’s NTK \(K(x_1,x_2)\) on two input vectors \(x_1\) and \(x_2\) (which we will for simplicity assume are normalized to the same length).
For a FCN, this kernel is &lt;em&gt;rotation-invariant&lt;/em&gt; in the sense that \(K(x_1,x_2) = K(c)\), where \(c\) is the cosine of the angle between the inputs.
Since \(K(c)\) is a scalar function of a scalar argument, we can simply plot it.
Fig. 2 shows the NTK of a four-hidden-layer (4HL) \(\textrm{ReLU}\) FCN.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/rev_eng_fig2.png&quot; width=&quot;65%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 2.&lt;/b&gt; The NTK of a 4HL $\textrm{ReLU}$ FCN as a function of the cosine between two input vectors $x_1$ and $x_2$. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This plot actually contains much information about the learning behavior of the corresponding wide network!
The monotonic increase means that this kernel expects closer points to have more correlated function values.
The steep increase at the end tells us that the correlation length is not too large, and it can fit complicated functions.
The diverging derivative at \(c=1\) tells us about the smoothness of the function we expect to get.
Importantly, &lt;em&gt;none of these facts are apparent from looking at a plot of \(\textrm{ReLU}(z)\)&lt;/em&gt;!
We claim that, if we want to understand the effect of choosing an activation function \(\phi\), then the resulting NTK is actually more informative than \(\phi\) itself.
It thus perhaps makes sense to try to design architectures in “kernel space,” then translate them to the typical hyperparameters.&lt;/p&gt;

&lt;h3 id=&quot;an-activation-function-for-every-kernel&quot;&gt;&lt;strong&gt;An activation function for every kernel&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Our main result is a “reverse engineering theorem” that states the following:&lt;/p&gt;

&lt;p style=&quot;padding: 10px; border: 2px solid black;&quot;&gt;
&lt;b&gt;Thm 1:&lt;/b&gt; For any kernel $K(c)$, we can construct an activation function $\tilde{\phi}$ such that, when inserted into a &lt;i&gt;single-hidden-layer&lt;/i&gt; FCN, its infinite-width NTK or NNGP kernel is $K(c)$.
&lt;/p&gt;

&lt;p&gt;We give an explicit formula for \(\tilde{\phi}\) in terms of Hermite polynomials
(though we use a different functional form in practice for trainability reasons).
Our proposed use of this result is that, in problems with some known structure, it’ll sometimes be possible to write down a good kernel and reverse-engineer it into a trainable network with various advantages over pure kernel regression, like computational efficiency and the ability to learn features.
As a proof of concept, we test this idea out on the synthetic &lt;em&gt;parity problem&lt;/em&gt; (i.e., given a bitstring, is the sum odd or even?), immediately generating an activation function that dramatically outperforms \(\text{ReLU}\) on the task.&lt;/p&gt;

&lt;h3 id=&quot;one-hidden-layer-is-all-you-need&quot;&gt;&lt;strong&gt;One hidden layer is all you need?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Here’s another surprising use of our result.
The kernel curve above is for a 4HL \(\textrm{ReLU}\) FCN, but I claimed that we can achieve any kernel, including that one, with just one hidden layer.
This implies we can come up with some new activation function \(\tilde{\phi}\) that gives this “deep” NTK in a &lt;em&gt;shallow network&lt;/em&gt;!
Fig. 3 illustrates this experiment.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/rev_eng_fig3.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 3.&lt;/b&gt; Shallowification of a deep $\textrm{ReLU}$ FCN into a 1HL FCN with an engineered activation function $\tilde{\phi}$. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;Surprisingly, this “shallowfication” actually works.
The left subplot of Fig. 4 below shows a “mimic” activation function \(\tilde{\phi}\) that gives virtually the same NTK as a deep \(\textrm{ReLU}\) FCN.
The right plots then show train + test loss + accuracy traces for three FCNs on a standard tabular problem from the UCI dataset.
Note that, while the shallow and deep ReLU networks have very different behaviors, our engineered shallow mimic network tracks the deep network almost exactly!&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img	/ntk-reveng/rev_eng_fig4.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 4.&lt;/b&gt; Left panel: our engineered &quot;mimic&quot; activation function, plotted with ReLU for comparison. Right panels: performance traces for 1HL ReLU, 4HL ReLU, and 1HL mimic FCNs trained on a UCI dataset. Note the close match between the 4HL ReLU and 1HL mimic networks.&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This is interesting from an engineering perspective because the shallow network uses fewer parameters than the deep network to achieve the same performance.
It’s also interesting from a theoretical perspective because it raises fundamental questions about the value of depth.
A common belief deep learning belief is that deeper is not only better but &lt;em&gt;qualitatively different&lt;/em&gt;: that deep networks will efficiently learn functions that shallow networks simply cannot.
Our shallowification result suggests that, at least for FCNs, this isn’t true: if we know what we’re doing, then depth doesn’t buy us anything.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;This work comes with lots of caveats.
The biggest is that our result only applies to FCNs, which alone are rarely state-of-the-art.
However, work on convolutional NTKs is &lt;a href=&quot;https://arxiv.org/abs/2112.05611&quot;&gt;fast progressing&lt;/a&gt;, and we believe this paradigm of designing networks by designing kernels is ripe for extension in some form to these structured architectures.&lt;/p&gt;

&lt;p&gt;Theoretical work has so far furnished relatively few tools for practical deep learning theorists.
We aim for this to be a modest step in that direction.
Even without a science to guide their design, neural networks have already enabled wonders.
Just imagine what we’ll be able to do with them once we finally have one.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is based on &lt;a href=&quot;https://arxiv.org/abs/2106.03186&quot;&gt;the paper&lt;/a&gt; “Reverse Engineering the Neural Tangent Kernel,” which is joint work with &lt;a href=&quot;https://www.sajant.com/&quot;&gt;Sajant Anand&lt;/a&gt; and &lt;a href=&quot;https://deweeselab.com/&quot;&gt;Mike DeWeese&lt;/a&gt;. We provide &lt;a href=&quot;https://github.com/james-simon/reverse-engineering&quot;&gt;code&lt;/a&gt; to reproduce all our results. We’d be delighted to field your questions or comments.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(It’s the belief of this author that deeper really is different for CNNs, and so studies aiming to understand the benefits of depth for generalization should focus on CNNs and other structured architectures. Also, careful readers might note that our analysis is in the kernel regime, but it’s possible that depth does buy us something in the feature-learning regime. However, there is weak evidence that &lt;em&gt;feature learning itself&lt;/em&gt; is undesirable for FCNs on realistic tasks, and that the best performance occurs in the kernel regime. For example, it is robustly found in many papers (including ours and &lt;a href=&quot;https://arxiv.org/abs/1711.00165&quot;&gt;Lee et. al (2018)&lt;/a&gt;) that “wider is better” for FCNs.) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 23 Aug 2022 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/reverse-engineering/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/reverse-engineering/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>Einstein vs. Bohr rap battle</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;They say an excellent scientist ought to know not just their field’s science but also its history. In a cynical move intended purely to improve our job prospects, Ashwin Singh and I decided to flaunt our copious knowledge of the history of physics by writing and performing an embarrassingly nerdy rap battle between Niels Bohr and Albert Einstein at our 2021 departmental holiday party and posting it on the internet. Potential employers can find the recording below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;600&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/k7Nj7IUHveY&quot;&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2 align=&quot;center&quot;&gt;
LYRICS
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;[BOHR]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Do you really want to reignite this debate? \ History’s already set the record straight
&lt;/p&gt;
&lt;p&gt;Why you chose to battle me I can only wonder \ It’ll go down as your greatest blunder&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You started anti-quantum but then slowly changed your ways \ I guess that your denial was just a Berry phase
&lt;/p&gt;
&lt;p&gt;Your life was full of sexual activity but \ you married your cousin? That’s “special relativity”!&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
All you know is maths \ and if you built an Einstein Rosen bridge, I bet it would collapse.
&lt;/p&gt;
&lt;p&gt;And your famous shorthand is pure castration \ Cutting off sums? That’s Einstein &lt;em&gt;dumb&lt;/em&gt; notation&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
To prove quantum, look at Einstein at his prime \ one man with two women at the same damn time!
&lt;/p&gt;
&lt;p&gt;But it only works theoretically \ A million extra qubits won’t correct that infidelity&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You’ve many talents, especially misogyny \ People tend to like you, except for your progeny
&lt;/p&gt;
&lt;p&gt;I’m a better father and it isn’t hard to tell \ Your son you didn’t know well; my son won a Nobel!&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m creative, made s p d and f \ and I’ve got *a dagger* but I’m boutta act left
&lt;/p&gt;
&lt;p&gt;Give up Einstein - you’re not a match for me \ or I’ll annihilate you in an ultraviolent catastrophe&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[EINSTEIN]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Einstein’s my name, but “mc” will suffice \ watch out - this rap god does not play nice
&lt;/p&gt;
&lt;p&gt;You’re one great Dane, I have to admit \ but now I’m taking the stage, so roll over and sit&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m General Relativity, the field’s commanding officer \ You? Eh, you’re more of a philosopher
&lt;/p&gt;
&lt;p&gt;I’m king of the cosmos; I fused space and time \ now watch me unify rhythm and rhyme&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
No modern man can match the breadth of things that I wrote on \ I'd still be above your level if I’d omitted the photon
&lt;/p&gt;
&lt;p&gt;I’m a hero of theory, stand a light-year tall \ Even in your rest frame, you’re relatively small&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I solved mysteries from gravity to light \ You couldn’t even get hydrogen right
&lt;/p&gt;
&lt;p&gt;Here’s a new Bohr model (and this one won’t flop): \ physics has levels, and I’m on top.&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m a star on both sides of the ocean \ Every time I speak I cause a Brownian commotion
&lt;/p&gt;
&lt;p&gt;Your lectures are dull, though I’d not known before \ that one could stretch time just by being a Bohr&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
In this deterministic world one can predict what comes to pass \ and I can prove my geodesic leads my foot into your [CENSORED]
&lt;/p&gt;
&lt;p&gt;Oh, and as for women, why don’t you just ask your wife \ if Einsteinium or Bohrium’s got the longer half-life?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[BOHR]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Those lines were so bad I’m nauseous \ Tell me did you learn them at the patent office?
&lt;/p&gt;
&lt;p&gt;I’m Bohr Dissing Einstein, got BDE \ You’re more degenerate than a BEC&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
In history you’re an eccentric defector \ who's easily perturbed like the Runge Lenz vector
&lt;/p&gt;
&lt;p&gt;Your math went missing on many occasions: \ you couldn’t even solve your own field equations&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
So much wrong about how things behave \ like “uh there’s no energy in gravitational waves”
&lt;/p&gt;
&lt;p&gt;Oh, and what do people think of EPR? \ Yeah, that theory’s gonna need some CPR&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
You also never understood Schrödinger’s cat \ Imagine tonight at the start of the act
&lt;/p&gt;
&lt;p&gt;they see you and me both behind and ahead \ but after my verse they realize that you’re dead&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Oh my conjugate man \ you must be feeling uncertain cause I know where I stand
&lt;/p&gt;
&lt;p&gt;All eyes are on me, but you’re in superposition \ ‘cause to not observe you is a superb omission&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m done, and there’s one clear conclusion \ I'm the only winner here by Pauli’s exclusion
&lt;/p&gt;
&lt;p&gt;Oh, and for Einsteinium, let’s use a bit of rigor: \ every chemist knows that Bohrium is bigger&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;[EINSTEIN]&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
When I walk around Princeton, the ladies all stare \ even black holes tell me “ah, I wish I had your hair!”
&lt;/p&gt;
&lt;p&gt;In attractiveness, I am the crest, you’re the trough \ there’s a name for the phenomenon: “the Bohr magnet-off”&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Forget singularities, folks, we’ve found something denser \ scare me all you want, you can’t make Einstein tensor
&lt;/p&gt;
&lt;p&gt;I built my own fame, you relied on your betters; \ guess that explains why you published our letters&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I'm unchallenged master of problems abstract \ I think so fast your thoughts Lorentz-contract
&lt;/p&gt;
&lt;p&gt;Yeah, you might as well call me “c” \ ‘cause you’ve got no chance of catching up with me&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Socially, I’m a champion of simple coexistence \ resisting and assisting, consistently insisting
&lt;/p&gt;
&lt;p&gt;politically, your presence more or less is nonexistent \ you can’t spookily better the world from a distance.&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
Here’s a little lesson in relativity \ there’s a twin paradox here between you and me:
&lt;/p&gt;
&lt;p&gt;see, since I ran circles ‘round you all life long \ your work’s long dead, but mine’s still going strong&lt;/p&gt;

&lt;p style=&quot;margin:0;padding-top:0;&quot;&gt;
I’m generally special; my work’s especially general \ My greatness endures, Bohr, but yours was ephemeral
&lt;/p&gt;
&lt;p&gt;Your nanoscopic jabs at me are hardly worth refutin’ \ When all’s said and done, bud, you’re Leibniz, I’m Newton.&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Jan 2022 01:01:00 -0900</pubDate>
        <link>http://localhost:4000/blog/bohr-v-einstein/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/bohr-v-einstein/</guid>
        
        
        <category>physics, random</category>
        
      </item>
    
      <item>
        <title>Newton vs. Leibniz rap battle</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;The spiteful grudge between these calculus co-creators is among the best-known disputes in the history of science. In 2019, fellow first-year grad Ashwin Singh and I wrote a rap battle between these men and performed it at the Berkeley physics department’s annual holiday party. A post-facto recording is below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;600&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/COeKdP3EkXU&quot;&gt;
&lt;/iframe&gt;
&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Jan 2022 01:00:00 -0900</pubDate>
        <link>http://localhost:4000/blog/newton-v-leibniz/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/newton-v-leibniz/</guid>
        
        
        <category>physics, random, rap-battle</category>
        
      </item>
    
      <item>
        <title>A theory of generalization for wide neural nets</title>
        <description>&lt;p&gt;&lt;em&gt;This post also appeared on the &lt;a href=&quot;https://bair.berkeley.edu/blog/2021/10/25/eigenlearning/&quot;&gt;BAIR blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;60%&quot; style=&quot;display:block; margin: 0 auto;&quot;&gt;
    &lt;source src=&quot;https://james-simon.github.io/img/eigenlearning_exp_matches_th.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;&lt;b&gt;Fig 1.&lt;/b&gt; Measures of generalization performance for neural networks trained on four different boolean functions (colors) with varying training set size. For both MSE (left) and learnability (right), theoretical predictions (curves) closely match true performance (dots). &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;Deep learning has proven a stunning success for countless problems of interest, but this success belies the fact that, at a fundamental level, we do not understand why it works so well. Many empirical phenomena, well-known to deep learning practitioners, remain mysteries to theoreticians. Perhaps the greatest of these mysteries has been the question of generalization: &lt;em&gt;why do the functions learned by neural networks generalize so well to unseen data?&lt;/em&gt; From the perspective of classical ML, neural nets’ high performance is a surprise given that they are so overparameterized that they could easily represent countless poorly-generalizing functions.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Questions beginning in “why” are difficult to get a grip on, so we instead take up the following quantitative problem: &lt;em&gt;given a network architecture, a target function \(f\), and a training set of \(n\) random examples, can we efficiently predict the generalization performance of the network’s learned function \(\hat{f}\)?&lt;/em&gt; A theory doing this would not only explain why neural networks generalize well on certain functions but would also tell us which function classes a given architecture is well-suited for and potentially even let us choose the best architecture for a given problem from first principles, as well as serving as a general framework for addressing a slew of other deep learning mysteries.&lt;/p&gt;

&lt;p&gt;It turns out this is possible: in our recent &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;paper&lt;/a&gt;, &lt;em&gt;we derive a first-principles theory that allows one to make accurate predictions of neural network generalization&lt;/em&gt; (at least in certain settings). To do so, we make a chain of approximations, first approximating a real network as an idealized infinite-width network, which is known to be equivalent to kernel regression, then deriving new approximate results for the generalization of kernel regression to yield a few simple equations that, despite these approximations, closely predict the generalization performance of the original network.&lt;/p&gt;

&lt;h2 id=&quot;finite-network-approx-infinite-width-network--kernel-regression&quot;&gt;&lt;strong&gt;Finite network \(\approx\) infinite-width network \(=\) kernel regression&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;A major vein of deep learning theory in the last few years has studied neural networks of infinite width. One might guess that adding more parameters to a network would only make it harder to understand, but, by results akin to central limit theorems for neural nets, infinite-width nets actually take very simple analytical forms. In particular, a wide network trained by gradient descent to zero MSE loss will always learn the function&lt;/p&gt;

\[\hat{f}(x) = K(x, \mathcal{D}) K(\mathcal{D}, \mathcal{D})^{-1} f(\mathcal{D}),\]

&lt;p&gt;where \(\mathcal{D}\) is the dataset, \(f\) and \(\hat{f}\) are the target and learned functions respectively, and \(K\) is the network’s &lt;a href=&quot;https://arxiv.org/abs/1806.07572&quot;&gt;“neural tangent kernel” (NTK)&lt;/a&gt;. This is a matrix equation: \(K(x, \mathcal{D})\) is a row vector, \(K(\mathcal{D}, \mathcal{D})\) is the “kernel matrix,” and \(f(\mathcal{D})\) is a column vector. The NTK is different for every architecture class but (at least for wide nets) the same every time you initialize. Because of this equation’s similarity to the normal equation of linear regression, it goes by the name of “kernel regression.”&lt;/p&gt;

&lt;p&gt;The sheer simplicity of this equation might make one suspect that an infinite-width net is an absurd idealization with little resemblance to useful networks, but experiments show that, as with the regular central limit theorem, infinite-width results usually kick in sooner than you’d expect, at widths in only the hundreds. Trusting that this first approximation will bear weight, our challenge now is to understand kernel regression.&lt;/p&gt;

&lt;h2 id=&quot;approximating-the-generalization-of-kernel-regression&quot;&gt;&lt;strong&gt;Approximating the generalization of kernel regression&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In deriving the generalization of kernel regression, we get a lot of mileage from a simple trick: we look at the learning problem in the eigenbasis of the kernel. Viewed as a linear operator, the kernel has eigenvalue/vector pairs $(\lambda_i, \phi_i)$ defined by the condition that&lt;/p&gt;

\[\int\limits_{\text{input space}} \! \! \! \! K(x, x’) \phi_i(x’) d x’ = \lambda_i \phi_i(x).\]

&lt;p&gt;Intuitively speaking, a kernel is a similarity function, and we can interpet its high-eigenvalue eigenfunctions as mapping “similar” points to similar values.&lt;/p&gt;

&lt;p&gt;The centerpiece of our analysis is a measure of generalization we call “learnability” which quantifies the alignment of \(f\) and \(\hat{f}\). With a few minor approximations, we derive the extremely simple result that the learnability of each eigenfunction is given by&lt;/p&gt;

\[\mathcal{L}(\phi_i) = \frac{\lambda_i}{\lambda_i + C},\]

&lt;p&gt;where \(C\) is a constant. Higher learnability is better, and thus this formula tells us that &lt;em&gt;higher-eigenvalue eigenfunctions are easier to learn!&lt;/em&gt; Moreover, we show that, as examples are added to the training set, \(C\) gradually decreases from \(\infty\) to \(0\), which means that each mode’s \(\mathcal{L}(\phi_i)\) gradually increases from \(0\) to \(1\), with higher eigenmodes learned first. Models of this form have a strong inductive bias towards learning higher eigenmodes.&lt;/p&gt;

&lt;p&gt;We ultimately derive expressions for not just learnability but for &lt;em&gt;all first- and second-order statistics of the learned function,&lt;/em&gt; including recovering previous expressions for MSE. We find that these expressions are quite accurate for not just kernel regression but finite networks, too, as illustrated in Fig 1.&lt;/p&gt;

&lt;h2 id=&quot;no-free-lunch-for-neural-networks&quot;&gt;&lt;strong&gt;No free lunch for neural networks&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In addition to approximations for generalization performance, we also prove a simple exact result we call the “no-free-lunch theorem for kernel regression.” The classical &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;no-free-lunch theorem for learning algorithms&lt;/a&gt; roughly states that, averaged over all possible target functions \(f\), any supervised learning algorithm has the same expected generalization performance. This makes intuitive sense - after all, most functions look like white noise, with no discernable patterns - but it is also not very useful since the set of “all functions” is usually enormous. Our extension, specific to kernel regression, essentially states that&lt;/p&gt;

\[\begin{align}
	\sum_i \mathcal{L}(\phi_i) = \text{[training set size]}.
\end{align}\]

&lt;p&gt;That is, the sum of learnabilities across all kernel eigenfunctions equals the training set size. This exact result paints a vivid picture of a kernel’s inductive bias: the kernel has exactly $\text{[training set size]}$ units of learnability to parcel out to its eigenmodes - no more, no less - and thus eigenmodes are locked in a zero-sum competition to be learned. As shown in Fig 2, we find that this basic conservation law holds exactly for NTK regression and even approximately for finite networks. To our knowledge, this is the first result quantifying such a tradeoff in kernel regression or deep learning. It also applies to linear regression, a special case of kernel regression.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/eigenlearning_conservation_of_lrn.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 2.&lt;/b&gt; For four different network architectures (fully-connected &lt;/i&gt;ReLU&lt;i&gt; and &lt;/i&gt;tanh&lt;i&gt; nets with one or four hidden layers), total learnability summed across all eigenfunctions is equal to the size of the training set. Colored components show learnabilities of individual eigenfunctions. For kernel regression with the network’s NTK (left bar in each pair), the sum is exactly the trainset size, while real trained networks (right bar in each pair) sum to approximately the trainset size. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;These results show that, despite neural nets’ notorious inscrutability, we can nonetheless hope to understand when and why they work well. As in other fields of science, if we take a step back, we can find simple rules governing what naively appear to be systems of incomprehensible complexity. More work certainly remains to be done before we truly understand deep learning - our theory only applies to MSE loss, and the NTK’s eigensystem is yet unknown in all but the simplest cases - but our results so far suggest we have the makings of a bona fide theory of neural network generalization on our hands.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;This post is based on &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;the paper&lt;/a&gt; “Neural Tangent Kernel Eigenvalues Accurately Predict Generalization,” which is joint work with labmate Maddie Dickens and advisor Mike DeWeese. We provide &lt;a href=&quot;https://github.com/james-simon/eigenlearning&quot;&gt;code&lt;/a&gt; to reproduce all our results. We’d be delighted to field your questions or comments.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Oct 2021 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/eigenlearning/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/eigenlearning/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>The gravitree</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;I am now in the 3rd year of my physics PhD, a path motivated, more or less, by a strong desire to figure out the basic rules of the universe. There are many reasons we should try to understand nature: in addition to simple curiosity, it’s my sincere hope that the small light I shine into the unknown might help discover things that one day make the world a better place. That, surely, will be the noblest use of the scientific knowledge to which I find myself a fortunate heir. In the meantime, though, I really like to use it to make weird stuff.&lt;/p&gt;

&lt;p&gt;I’m writing this to introduce and explain the &lt;strong&gt;gravitree&lt;/strong&gt;, a type of balancing kinetic sculpture that I’ve been 3D-printing for years. It requires only high school physics to understand (and that was all I knew when I came up with it), but its hypnotic and counterintuitive motion is unlike anything else I’ve seen. It consists of a number of freely-moving pieces, arranged one atop another in a stack, with just the bottommost piece supported. Here’s what it looks like.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_main.jpg&quot; width=&quot;30%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_main.gif&quot; width=&quot;22%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The stack is easy to assemble and absurdly stable; when people first hold it, they’re usually surprised by how easily it stays upright and the way it stays balanced &lt;a href=&quot;https://youtu.be/3jd2jpJoXgA&quot;&gt;even during drastic motions&lt;/a&gt;. I currently have several of them up as room decorations and desk ornaments, and I’m pretty sure they’ll stand through a sizable earthquake. What’s the trick?&lt;/p&gt;

&lt;p&gt;It turns out the gravitree is best understood from the top down: we’ll first take a close look at the top piece, then at the top two together, then the top three, all the way down. As we’ll see, it’s just using the same trick again and again.&lt;/p&gt;

&lt;p&gt;The key to understanding the sculpture is the concept of &lt;em&gt;center of mass&lt;/em&gt;. Think for a moment about how gravity acts on your body. It pulls down on every gram of you, all throughout your volume. The total force is equal to your weight, but the way that force is spread out in space depends on your shape and your pose.&lt;/p&gt;

&lt;p&gt;An object, like you, can have quite a complex shape, so you’d naturally think it would be very complicated to figure out how gravity will pull and spin a particular object. It turns out, however, that as long as the object doesn’t bend much, we can pretend that all the force is pulling at one point, called the object’s center of mass, and we’ll correctly predict its motion. As the name suggests, the center of mass is the average location of mass in the object, and it’s closer to bigger, denser parts.&lt;/p&gt;

&lt;p&gt;Let’s look at the center of mass of the top piece of the gravitree. First, I’ll attach two weights to the bottom, which will bring the center of mass to well below the balance point. This makes the object a lot like a pendulum, which just consists of a point of rotation and a big mass some distance below it. Like a pendulum, it’s very stable - gravity tends to try to pull it back to vertical - and when we poke it, we see it follow quick oscillations.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/superstable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/superstable.gif&quot; width=&quot;17%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;By contrast, if I put the weight at the top, the center of mass is now above the balance point, and it’s &lt;em&gt;unstable&lt;/em&gt;: gravity now tends to pull it away from vertical. If the previous weighting’s like a pendulum, this one’s like a pencil balanced on your finger. It won’t stand on its own.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/unstable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/unstable.gif&quot; width=&quot;16%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;For a balanced object, these are the choices. It can either be stable or unstable, switching from one to the other as the center of mass crosses the pivot point. This raises an natural question, though: what happens if you start with a stable object and push the center of mass closer and closer to the pivot but never crossing it? How would such a barely stable object move?&lt;/p&gt;

&lt;p&gt;That’s the secret of the gravitree. This piece is just barely stable, and because of that, it swings like a pendulum, but &lt;em&gt;very slowly&lt;/em&gt;. The technical reason’s that, since the center of mass is so close to the pivot, the torque from gravity is so small it gives the object only a tiny angular acceleration relative to its moment of inertia.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/stable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/stable.gif&quot; width=&quot;16%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This explains both why the top piece is stable and why it moves so eerily, seeming to move through space without swinging around like you’d expect it to. What about all the other pieces, though? It turns out they all use the same trick, but when balancing a lower piece, you have to imagine the weight of all the higher layers acting on its top. To illustrate that point, let’s look at the second-highest piece. Without that extra weight, it’s very stable, but with that weight, it’s barely stable like the topmost piece.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/layer2_superstable.jpg&quot; width=&quot;20%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/layer2_stable.jpg&quot; width=&quot;20%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We can use this trick again and again all the way down. When I designed this sculpture, I used Autodesk Inventor’s center-of-mass-finding feature to balance the pieces from the top down, adjusting the sphere sizes to make each new piece barely stable under the weight of those above it. The end result is an entire tower that’s paradoxically both barely stable and yet very hard to accidentally knock over. This basic idea can take many different forms - here are several other gravitrees I’ve made over the years.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_christmas.jpg&quot; width=&quot;15%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_arrows.webp&quot; width=&quot;27%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_cages.webp&quot; width=&quot;21%&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_centipede.webp&quot; width=&quot;12%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_spider.webp&quot; width=&quot;23%&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/gravitree/gravitree_spinner.gif&quot; width=&quot;28%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There’s a deeper trick we’re playing here, one that I see in inventions from &lt;a href=&quot;https://en.wikipedia.org/wiki/Reaction_wheel&quot;&gt;reaction wheels&lt;/a&gt; to &lt;a href=&quot;https://www.youtube.com/watch?v=jyQwgBAaBag&quot;&gt;faster-than-wind sailboats&lt;/a&gt;: once we know the precise rules of the universe, we can saunter just up to the edge of impossibility and thumb our noses at Nature like a toddler who knows they’re not &lt;em&gt;technically&lt;/em&gt; breaking their mother’s rules. For all of science’s immense capacity for social good, that sort of thing - the feeling of doing what seemed impossible until you stopped to think about it - is a key part of what compels me to study physics, and I think it’s a potent way to inspire the curious to think a little closer about how the universe really works.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Oct 2021 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/gravitrees/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/gravitrees/</guid>
        
        
        <category>physics, fun-science</category>
        
      </item>
    
      <item>
        <title>Could you propel a spacecraft using sports projectiles?</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;In the fall of 2020, Ryan Roberts and I gave a remote talk for high schoolers through Berkeley Splash, our third exploring real physics through absurd scenarios instead of technical math. His talk discussed methods and consequences of making the Moon as big as the Earth, while mine aimed to find the best way to propel a spacecraft using only sports equipment. This is a writeup of the answer.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since the dawn of civilization, humans have gathered for sports competitions, with rounds of faceoffs ultimately whittling the athletes down to a sole champion. This time-honored tradition has flowered throughout history; the modern Olympic Games involve dozens of sports split into hundreds of events. This explosion of athletic diversity has made determining an ultimate champion a lot more complicated, though; wouldn’t it be nice if there were some way we could determine who, of all the gold medalists, is actually the best? What if there were some grand final competition among the event champions to determine an ultimate victor?&lt;/p&gt;

&lt;p&gt;For the consideration of the International Olympic Committee, I hereby propose the event of Rocketball as the Games’ final event. The rules are simple. To start, every gold medalist will be launched into Earth orbit with nothing but their wits, a spacesuit, and an absurd amount of equipment from their sport. They have no traditional rockets; to change course, the athletes have to hit, throw, fire, bowl, or otherwise propel sports equipment the other way, powered solely by their bodies. Instead of a goal line, there’s a target speed: whoever can accelerate themselves to reach that final speed first wins the Olympics&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This proposal has one&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; minor logistical problem: some sports are better than others. For example, it’d be way easier to accelerate by hitting tennis balls than by hitting ping-pong balls; a hit tennis ball is both heavier and faster, so (as we’ll see) as a rocket propellant it’s strictly better. Actually, in some cases, which sport is best can even depend on how fast the target speed is! To understand this, figure out how matches would play out, and see if we can come up with a fix, first we need to understand a little about how rockets work.&lt;/p&gt;

&lt;p&gt;At their core, all rockets operate on the same principle: they fling matter in one direction, and the recoil force pushes the rocket in the opposite direction. This recoil force is the same sort of pushback you’d feel when firing a gun, holding a fire hose, or doing a fast chest pass in basketball, but in a typical rocket the matter being launched is exhaust from burning fuel moving several kilometers per second. True to this principle, rockets have two main parts: the &lt;em&gt;fuel,&lt;/em&gt; which is gradually fired to propel the rocket, and the &lt;em&gt;payload,&lt;/em&gt; which is the important other bits that the fuel’s there to accelerate. The fuel section also usually includes engines that help burn the fuel but fall off when they’re no longer needed. Here’s a diagram of the Saturn V rocket split into the launch vehicle (fuel + engines) and spacecraft (payload).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/saturn_v.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Our sportsball rockets will work basically the same way, but instead of liquid fuel they carry sports equipment, and instead of the most powerful engines ever built, they’ll be powered by lone humans flinging objects into the vacuum of space.&lt;/p&gt;

&lt;p&gt;If you’re not familiar with rocketry or are very used to cars, you might wonder why the Saturn V has so much fuel for such a small payload. At some level, the reason’s that we want the payload to reach a very high speed (over 10 km/s), so we need a lot of fuel, but there’s another reason specific to rockets: &lt;em&gt;the earlier fuel has to accelerate all the later fuel, so you need more of it.&lt;/em&gt; If a competitor gradually hit away a million golf balls, the first ball would give a very small bump in speed because it has to push the other 999999 balls, while the last one would give a relatively big bump in speed because it’s only accelerating the athlete. A rocket’s acceleration is inversely proportional to its current mass. Because of this, it turns out that the amount of fuel you need is an exponential function of the final speed, and ending up slightly faster can potentially take many times more fuel.&lt;/p&gt;

&lt;p&gt;Rockets are complicated, but for a rocket accelerating in a straight line, it turns out we can find its speed over time if we know just a few numbers. These are the payload mass (which we’ll call \(m\)), the starting mass of the fuel and payload together (\(M\)), the relative speed the propellant’s fired at (\(u\)), and the thrust of the rocket (this is the average kickback force the rocket feels; we’ll call it \(F\)). Here’s a diagram illustrating what some of these are.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocket_math_diagram.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;If you do the rocket science, you find that to reach a final speed \(v\), the rocket needs a starting mass of \(M = m e^{v/u}\), and it runs out of fuel and reaches speed \(v\) at a time \(t = \frac{(M-m)u}{F}\). For a given target speed \(v\), this choice of \(M\) is optimal - with less fuel, it won’t reach the target speed, but with any more, it’ll reach it slower.&lt;/p&gt;

&lt;p&gt;We can use these formulae to figure out roughly what’ll happen in our intersport showdown. For a given sport, \(u\) is just the projectile speed (around 76 m/s for a golf ball and 4 m/s for a pool ball) and the thrust is equal to (projectile speed)\(\times\)(projectile mass)\(\times\)(fire rate). If we assume a constant fire rate of 1 shot/s across all sports for simplicity, we can get the speed and thrust for every sport. Here’s a plot showing the results.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_scatterplot.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;A quick look at this might reveal some oddities. Jai alai (also called Basque pelota) was once an Olympic sport; it’s like wallball but with a giant curved launcher on your throwing arm. Bowling and pool I’ve included even though they’re not Olympic sports. Crossbow shooting I included because our question is ultimately about projectiles launched solely with energy from human muscles, and it’s one of the best examples of that. For the same reason, I didn’t include gun sports. Lastly, running and swimming events don’t have projectiles, so I assumed their respective gold medalists will be throwing shoes and spitting mouthfuls of pool water&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This plot makes it clear that some sports are better than others, but it’s not apparent which is best. To decide, we’ll have to pick a target speed. Let’s say the athlete and their suit are 100 kg, and they’re trying to reach a final speed of 10 m/s. We’ll also assume that the athlete can repeatedly launch projectiles in the same direction without uncontrollably spinning or getting knocked off, but that seems fair; after all, they are professionals. About how long will each sport’s champion take to finish?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_10.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This plot shows that the best sport to reach 10 m/s is shot put. It’d actually only take 24 solid throws to finish, which is 24 seconds with our assumption of one shot per second. Most other sports take a few minutes; the swimmers will be spitting into the void for several days.&lt;/p&gt;

&lt;p&gt;What if we instead want to reach 100 m/s?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_100.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now the best option is to accelerate with 2053 jai alai throws, which takes a little over half an hour. Due to the exponential in our time formula, however, accelerating with pool balls will now take over a hundred million years. Surprisingly, most high-energy projectile sports are all relatively close, with a time less than a few hours. Actually, at a target speed of 130 m/s, the entire swath from crossbow to javelin is about even! This, I propose, is the ideal choice for Rocketball; the sports without high-energy projectiles can get a handicap.&lt;/p&gt;

&lt;p&gt;What if we make the target 1000 m/s?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_1000.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;With this high target speed, the \(e^{v/u}\) term is so dominant that the thrust is basically irrelevant compared to the exhaust speed. The best option now is to use 35 million crossbow bolts, which will take around a year. This is one of the few situations where your best choice is to use a bow in space. All the sports slower than javelin will take longer than the current age of the universe.&lt;/p&gt;

&lt;p&gt;This is a decent analysis for the purposes of Rocketball, but these speeds are miniscule on the scale of the Solar system. What if we want to reach 10000 m/s, roughly Earth escape velocity? As you might guess, exhaust speed is even more dominant here, and crossbow bolts are still the best choice. However, you’d need \(10^{43}\) of them. The good news is that they’d hold together under their own gravity, so you wouldn’t need a quiver. The bad news is that the arrow ball would have a bigger diameter than the orbit of Saturn and would immediately collapse into a black hole.&lt;/p&gt;

&lt;p&gt;There is, however, a sport we’ve overlooked. The tires of a bike contain air under high pressure, and if there’s a hole or open valve, that pressure will accelerate the escaping air to high velocities. Normal bike valves are narrow and slow down the escaping air a lot, but if you designed a much bigger one (or just punched a hole in the tire) it’d come out much faster - a 50 psi tire would, in the ideal case, accelerate air to 760 m/s! It turns out that you could reach 10000 m/s by ideally releasing enough 50-psi air to fill a sphere with a diameter of 270 m.&lt;/p&gt;

&lt;p&gt;How long would that take? By our constraint of only man-powered projectiles, you’d have to pump up the air. This is about 47 billion bike tires’ worth of air, so it’d take around a millenium even if you’re the &lt;a href=&quot;https://www.guinnessworldrecords.com/world-records/419351-fastest-time-to-pump-a-bicycle-tyre&quot;&gt;world’s fastest tire pumper&lt;/a&gt;. The release, however, would be quick.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/air_jetpack.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;and gets to come back to Earth. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(this is a lower bound) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Spacesuit helmets would admittedly block the spit-out pool water, but since humans can survive zero pressure for short periods and swimmers are already good at not breathing, let’s say they take their helmets off to spit. Another option’s to throw chunks of ice, which would fall between “shoe throw” and “shotput.” &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 29 Nov 2020 00:00:00 -0900</pubDate>
        <link>http://localhost:4000/blog/rocketball/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/rocketball/</guid>
        
        
        <category>physics, fun-science</category>
        
      </item>
    
      <item>
        <title>The principle of least power dissipation</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;I’m currently in a class called Physics of Computation on new types of computing paradigms that don’t solve problems in discrete, sequential, logical steps like modern computers.  Instead, the whole computing system goes through a complex analog evolution, with every variable free to change simultaneously, and eventually settles into a stable final state representing the answer.  Naturally, one of the core challenges is in designing a system that predictably settles into a steady state with some desired properties.&lt;/p&gt;

&lt;p&gt;Our professor’s pointed out that many such computing systems can be seen to accomplish this by using a little-known concept called the ‘principle of least energy dissipation.’  When a battery or current source is hooked up to a DC circuit, a sudden flow of current flares through the different branches of the circuit, sometimes much more or less at first than is sustainable.  Eventually, though, these initial errors fade away, and the circuit ends up in a steady state, with currents and voltages everywhere that don’t change in time.  The principle of least energy claims that &lt;strong&gt;of all the possible steady states satisfying the voltage and current constraints, the real steady state dissipates the least amount of power.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’ve solved tons of DC circuits, and I’d never heard of this.  To my shock, though, it worked for a few simple circuits we discussed in class.  However, these simple examples neither showed it always works or explained why it does.  To try to understand, I read &lt;a href=&quot;https://www.feynmanlectures.caltech.edu/II_19.html#footnote_1&quot;&gt;Feynman’s lecture on minimization principles&lt;/a&gt;; it turns out that at the end he mentions this law but doesn’t really explain it, so I decided to fill in the gap.&lt;/p&gt;

&lt;p&gt;It’s actually easier to see that it’s true if you start with a general, continuous 3D system; everything’s captured in one differential equation instead of having to deal with discrete voltages and resistors.  Suppose we have a volume \(V\) filled with a resistive medium with resistivity \(\rho(\mathbf{r})\).  It could have different resistances at different points; that’ll be important later.  Now, suppose there’s an electric potential (a voltage) \(\phi(\mathbf{r})\) throughout the medium.  We also need some way to add a constraint to the system (like plugging a source into a circuit), so we’ll suppose that the potential at the surface is completely fixed; perhaps it’s higher at one end and lower at the other to cause a current flow.  In real circuits, currents cause magnetic fields that can affect charges a little, but we’ll ignore magnetism for this problem.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/electrostatic_setup.png&quot; width=&quot;20%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The currents and voltages throughout a resistive circuit are determined entirely by the sources and the circuit structure, so we expect that \(\phi\) and the current should be determined entrely by boundary conditions and \(\rho\).  How do we find them?  Well, the typical way is to say that the potential corresponds to an electric field \(\mathbf{E}(\mathbf{r}) = - \mathbf{\nabla}\phi\) which then creates a current \(\mathbf{J} = \rho^{-1} \mathbf{E}\) according to the continuous version of Ohm’s Law.  In the steady state, charge isn’t building up anywhere, which implies that \(\mathbf{\nabla} \cdot \mathbf{J} = -\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\).  Solving this to match the boundary conditions on \(\phi\) gives the potential, which gives the current.  Note that this would reduce to \(\nabla^2 \phi = 0\), which you might recognize as the equation voltage satisfies when there’s zero charge density, if \(\rho\) were a constant; the reason it doesn’t is that, even in steady-state DC circuits, there’s actually built-up charge on the interfaces between components of different resistivities.&lt;/p&gt;

&lt;p&gt;Let’s see if we can derive that differential equation not by assuming that \(\mathbf{\nabla} \cdot \mathbf{J} = 0\) but just by assuming that the power dissipation is as small as possible.  The total power dissipation is given by \(P = \int_V (\mathbf{J} \cdot \mathbf{E}) \mathop{}\!\mathrm{ \mathbf{dr} } = \int_V \rho^{-1} (\mathbf{\nabla}\phi)^2 \mathop{}\!\mathrm{ \mathbf{dr} }\).  Feynman’s lecture neatly explains how to find minimal solutions, so I’ll just tear through the steps quickly.  Since we’re looking for a minimum, we want to find a choice of \(\phi\) that makes \(P\) invariant to perturbations to first-order.  If we add a small perturbation \(\delta\phi\) to \(\phi\), so the new potential field is now \(\phi + \delta\phi\), the power dissipation becomes \(P + \delta P\), where \(P\) is the same as before and \(\delta P = 2 \int_V \rho^{-1} \mathbf{\nabla}\phi \cdot \mathbf{\nabla}(\delta\phi) \mathop{}\!\mathrm{ \mathbf{dr} }\).  The usual integration by parts trick gives \(\delta P = -2 \int_V \mathbf{\nabla} \cdot \big[ \rho^{-1} \mathbf{\nabla}\phi \big] \delta\phi \mathop{}\!\mathrm{ \mathbf{dr} }\), and since \(\delta\phi\) was arbitrary, this implies that \(-\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\)!  That’s it - minimizing power dissipated is equivalent to the same steady-state differential equation you get from working through the electrodynamics the normal way.  Any \(\phi\) that minimizes power dissipation is a real physical solution, and any physical solution minimizes power dissipation.&lt;/p&gt;

&lt;p&gt;The math is all well and good, but how can we really &lt;strong&gt;understand&lt;/strong&gt; it?  What’s the physical reason why power should be minimized?  Well, the differential equation we derived from power minimization is just the fact that the current \(\mathbf{J}\) has zero divergence.  That actually makes sense in terms of minimizing power dissipation!  Imagine a point in the medium with nonzero current divergence, so it’s a sink (or a source) of current.  In the vicinity of that point, on top of whatever divergence-free flow there is, there’s an extra part of the current that flows into or out of the point.  In a rough sense, it turns out that you can decrease the dissipated power by just dropping that extra part of the current.&lt;/p&gt;

&lt;p&gt;That’s the case of a problem where the voltage is prescribed.  Circuits can have both voltage and current sources, though; what about a case where the current on the boundary is given?  Well, first, it’s important to note that only the current &lt;em&gt;normal to the boundary&lt;/em&gt; can be prescribed; to illustrate why, if you could prescribe arbitrary currents on the boundary, you could have the current flow in a circle around the boundary, which we know is already unphysical.  We’ll also have to assume that \(\mathbf{\nabla} \cdot \mathbf{J} = 0\) - if we just let charge accumulate in the medium, the minimum power dissipation solution will just be to have zero current everywhere and have charge build up forever on the surface.  However, if we again minimize power dissipated using vector calculus tricks, we get a new, interesting condition: \(\mathbf{\nabla} \times (\rho\mathbf{J}) = 0\); the curl of \(\rho \mathbf{J}\) is zero.  Since curl-free vector fields are gradients of potentials, this gives us the condition that \(\mathbf{J} = -\rho^{-1}\mathbf{\nabla}\phi\) for some potential \(\phi\), and since we assumed the current has zero divergence, this gives us \(-\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\)!  Again, the new condition that minimizing power dissipation bought us actually makes physical sense; if the curl of \(\rho \mathbf{J} = \mathbf{E}\) weren’t zero, there would be current flowing uselessly around in loops and dissipating extra power.&lt;/p&gt;

&lt;p&gt;Alright, now we’ve decently explained why power dissipation is minimized in scenarios when current’s flowing through a continuous resistive medium.  We were originally interested in circuits, though; how does this relate?  We can actually cleverly fashion a resistive circuit by just changing \(\rho(\mathbf{r})\)!  For most of the volume, we’ll let \(\rho \rightarrow \infty\), which turns it into a very good insulator, like the air between the wires in a circuit.  Then we’ll let \(\rho \rightarrow 0\) in some long, thin regions that will become wires.  Along the wires we’ll make some regions have intermediate values of \(\rho\), which makes them resistors.  Lastly, if we want, we can choose our boundary so all of space &lt;em&gt;except&lt;/em&gt; where a source will go is enclosed in it, then specify the voltage or current along the boundary.  This gives a circuit of sources, wires and resistors that might look as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/resistive_circuit.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now we have a circuit.  How does minimal power dissipation explain its current and voltage distribution?  From the point of view of choosing voltages at each node of the circuit, any voltage distribution besides the true one will lead to charges accumulating somewhere, which requires some extra current and dissipates extra power.  From the point of view of choosing how currents branch at every node, any split besides the true one will correspond to extra current running around in a loop, which wastes power, and also an electric field with a nonzero curl, which doesn’t correspond to a potential.&lt;/p&gt;

&lt;p&gt;We’ve shown that the principle of least power dissipation gives the right answers for steady-state circuit problems.  If this is really a general law of nature, though, it’d probably show up outside of just this one narrow context.  It turns out that it can also be applied to &lt;em&gt;fluid circuits&lt;/em&gt;, which are sometimes studied as somewhat-similar sister systems to electric circuits.  In the sort of fluid circuit I’m imagining, there are networks of pipes of different thicknesses instead of resistors with different resistances.  Water flow rate \(Q\) replaces current, pressure \(P\) replaces voltage, the rate of power dissipation is \(PQ\) instead of \(VI\), and pipes have an effective resistance.  An ideal pipe with laminar flow has \(\Delta P\) proportional to \(Q\) in &lt;a href=&quot;https://en.wikipedia.org/wiki/Hagen%E2%80%93Poiseuille_equation&quot;&gt;an equation analogous to Ohm’s Law&lt;/a&gt;, and the quantity analogous to resistance depends on the pipe’s dimensions and the viscosity of the fluid.  The fact that these equations map onto each other so perfectly means that fluid through a network of pipes minimizes power dissipation just as electrical circuits do.  I’d be willing to bet that, just as we derived part of the differential equation for current flow, you could derive part of the Navier-Stokes equations just from minimizing power dissipation.&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Sep 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/least_power_dissipation/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/least_power_dissipation/</guid>
        
        
        <category>physics, research</category>
        
      </item>
    
      <item>
        <title>Multiplicative neural networks</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the past decade, neural networks have proven to be stunningly effective tools for a slew of machine learning tasks as diverse as classifying images, generating 3D graphics, and playing games.  These incredible successes all stem from neural networks’ remarkable ability to perform a central task in machine intelligence: given complex data, find underlying patterns.&lt;/p&gt;

&lt;p&gt;Boiled down to their core, neural networks are just a series of &lt;em&gt;hierarchical nonlinear transformations&lt;/em&gt;.  Each layer of the neural network is one layer in this hierarchy, transforming a feature vector and passing it to the next layer up, until it reaches the output.  Each layer usually involves a step that mixes the elements of the feature vector according to a set of weights followed by a nonlinear function applied to each element of the vector.  All types of neural networks use a series of these layers to do the basic information processing.&lt;/p&gt;

&lt;p&gt;Within that hierarchical nonlinear structure, however, there are many variations.  There are a vast array of different ways layers can be connected and parameterized, giving different architectures like convolutional nets, recurrent nets, and resnets specialized to certain types of task.  However, there are also lower-level things that can be changed.  One example is the nonlinear function, usually denoted \(\sigma\).  The field’s explored many options for \(\sigma\) over the years, and the most popular choice has changed over time.  The \(\tanh\) function (and similar sigmoid) were once the leading choice, but the \(\text{ReLU}\) (defined by \(\sigma(x) = \max(x, 0)\)) is now more popular.  Choosing a different nonlinearity can improve performance slightly, but to me it is more striking that the choice of nonlinearity matters so &lt;em&gt;little&lt;/em&gt;.  On the face of it, the \(\text{ReLU}\) and \(\tanh\) functions are basically as different as they could be within reason.  \(\tanh\) is smooth, saturating, nonlinear on every finite set, and is symmetric about the origin.  \(\text{ReLU}\) has a kink, is unbounded, piecewise-linear, and never takes negative values, not to mention that it outputs zero on half of its domain.  The fact that both of these functions could work reasonably well speaks to the fact that even though the hierarchical nonlinear structure is core to the effectiveness of neural nets, the exact choice of nonlinearity is, surprisingly, not crucial.&lt;/p&gt;

&lt;p&gt;In fact, even the typical layer structure of matrix multiplication + elementwise nonlinearity isn’t crucial to performance.  It has been shown that tensor networks, a completely different type of hierarchical nonlinear model commonly used in physics, can also learn machine learning tasks.  &lt;a href=&quot;https://arxiv.org/abs/1605.05775&quot;&gt;This paper&lt;/a&gt; develops the idea and gets 1% test error on MNIST, which is pretty good for a totally new approach.  Interestingly, neural networks have recently been used instead of tensor networks in computational physics problems and achieved good performance there.  This raises a natural question: how do we know that the matrix multiplication + elementwise nonlinearity structure is really the best one?  To my knowledge, there’s no known fundamental, theoretical reason why it would be better than other options.&lt;/p&gt;

&lt;h2 id=&quot;multiplicative-nets&quot;&gt;Multiplicative nets&lt;/h2&gt;

&lt;p&gt;Here’s an idea for a different type of nonlinear hierarchical model.  What if we took a neural network and replaced the matrix multiplication step \(\sum_j w_{ij} x_j\) with a product step \(\prod_j x_j ^ {w_{ij}}\), with the weights as exponents in a product instead of coefficients in a sum?  This would still mix together the elements of the feature vector \(x\), just like a matrix operation, but in a different, nonlinear way.  Given the way deep learning systems can sometimes be surprisingly invariant to details of implementation, maybe this new, different sort of model could still work well.&lt;/p&gt;

&lt;p&gt;Architectures using this idea are called &lt;strong&gt;multiplicative neural networks&lt;/strong&gt;.  The idea was &lt;a href=&quot;https://dl.acm.org/doi/10.1162/neco.1989.1.1.133&quot;&gt;first proposed in 1989&lt;/a&gt; by neuroscientists seeing multiplication as more biologically plausible and potentially more powerful than addition.  They were &lt;a href=&quot;https://clgiles.ist.psu.edu/papers/NIPS94.product.units.pdf&quot;&gt;tested experimentally&lt;/a&gt; on some very small problems in the next decade and once &lt;a href=&quot;https://sci2s.ugr.es/keel/pdf/specific/articulo/Schmidtt%20on-the-complexity-of.pdf&quot;&gt;analyzed from a complexity standpoint&lt;/a&gt; in the decade after.  The architectures these papers describe are a little more general - they consider combining additive and multiplicative neurons in the same net and using hybrid terms with more weights of the form \(v_1 \prod_j x_j ^ {w_{ij1}} + v_2 \prod_j x_j ^ {w_{ij2}} + ... + v_n \prod_j x_j ^ {w_{ijn}}\) - but their key innovation is the multiplicative form.  However, despite this research, multiplicative nets never broke through to practical use; we still use additive neural networks with normal matrix operations.&lt;/p&gt;

&lt;p&gt;This is pretty understandable - at a first glance, multiplicative nets seem like they’d have some problems.  One immediate question is what to do when \(x_i &amp;lt; 0\) and \(w_{ij}\) isn’t an integer, in which case \(x_i ^ {w_{ij}}\) isn’t  uniquely defined, but to avoid that problem let’s just set things up so that all the \(x_i\) are positive, which I’ll show we can do with the right choice of data and nonlinearity.  Even dealing with this, though, these nets have some pretty prolific potential practical problems.  First, there’s been a lot of research and hardware development into making matrix operations faster, and this wouldn’t use them.  Second, replacing simple matrix operations with something more complex could lead to weird gradients and failed optimization.  Third, as a friend noted, even from a pure computational complexity standpoint, calculating \(x^y\) is more expensive than calculating \(x*y\), so this would be slow.  Finally, the advantages of switching to this architecture are far from clear, so there’s no incentive to overcome these problems.&lt;/p&gt;

&lt;p&gt;The purpose of this post is to explore something interesting that isn’t discussed in the above papers: &lt;strong&gt;the multiplicative nets you get by replacing \(\sum_j w_{ij} x_j\) with \(\prod_j x_j ^ {w_{ij}}\) are basically identical to normal, additive neural nets with a different choice of nonlinearity.&lt;/strong&gt;  The basic reason is that, as the 1989 paper did note, \(\prod_j x_j ^ {w_{ij}} = \exp \big( \sum_j w_{ij} \ln(x_j) \big)\), so a multiplicative layer is the same as an additive layer where you take the logarithm before and an exponential after.  The main trick we’ll use is the fact that you can then absorb the \(\ln\) and \(\exp\) into the nonlinearity to get a new nonlinearity \(\sigma'(x) = \exp\sigma(\ln(x)))\), or, with compositional notation, \(\sigma' = \exp \circ \sigma \circ \ln\).&lt;/p&gt;

&lt;p&gt;My argument could easily be laid out with standard symbolic math.  However, that’s not how I think about it, and I don’t think it’s the clearest way to explain it.  Notation matters; different ways of writing the same thing inspire different kinds of manipulations, and cleaner notations are better to learn with.  Instead of algebra, I’ll use diagrams.&lt;/p&gt;

&lt;p&gt;Basic neural nets are built of a few basic pieces.  There are &lt;em&gt;input nodes&lt;/em&gt;, where input data vectors go into the net.  There are &lt;em&gt;output nodes&lt;/em&gt;, where the net’s output comes out.  There are elementwise &lt;em&gt;nonlinear functions&lt;/em&gt;, mapping real numbers to real numbers.  There are &lt;em&gt;additive&lt;/em&gt; or &lt;em&gt;matrix operations&lt;/em&gt;, each with its own 2D array of parameters \(W\), which in normal neural nets come between applications of nonlinear functions.  Lastly, we’ll include &lt;em&gt;multiplicative operations&lt;/em&gt;, which also each have their own 2D array array of parameters &lt;span style=&quot;color:purple&quot;&gt;\(W\)&lt;/span&gt; as discussed above.  We’ll use color to differentiate between additive and multiplicative operations; additive will be black, multiplicative will be purple.  Note that changing an operation from additive to multiplicative or vice versa doesn’t change its parameter matrix; the same numerical weights are just interpreted differently in a different operation.  We could also add pieces like biases or specialized layers like softmax or maxpooling, but we’ll leave those out for now for simplicity.  We could cast these pieces into diagrammatic form as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_components.png&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We can now build neural nets out of these components.  For example, the following is a standard additive neural net with 3-dimensional input, 3-dimensional output, two hidden layers with width 3, and a nonlinearity \(\sigma\).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_example.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There are a few different ways we can manipulate these diagrams while still keeping the model the same.  The first way is &lt;em&gt;function composition&lt;/em&gt;.  If there are two nonlinear functions in a row, we can just replace them with one new function.  This is just saying that the double function application \(f(g(x))\) is equivalent to the single function application \(h(x)\), defining \(h = f \circ g\).  One special, familiar case is when the two functions are each other’s inverses, and the new function is simply the identity; for example, \(\ln(\exp(x)) = x\).  Instead of writing the identity function explicitly, we can just write a line.  Note that in the notation \(f \circ g\), functions are applied right to left, but in our diagramas, everything flows from left to right, so the ordering of the composed functions might seem backwards at first.  There’s also the multiplicative-additive net identity we talked about earlier: \(\prod_j x_j ^ {w_{ij}} = \exp \big( \sum_j w_{ij} \ln(x_j) \big)\).  A multiplicative layer is the same as an additive layer with logarithms before and exponentials afterwards.  We can write these rules like this:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_diagram_rules.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;These rules lay out ways we can manipulate diagrams.  As a test of this notation, with just these simple rules we can derive a companion rule to the multiplicative-additive identity.  Just as you can take a function of both sides of an algebraic equation, we’re allowed to modify a diagrammatic equation by attaching pieces to dangling ends of both diagrams in the same way.  In this case, we’ll attach exponentials on all dangling ends on the left and logarithms on the right of both diagrams.  This gives us a new diagrammatic equation.  We can then use function composition to simplify the adjacent logarithms and exponentials to get a new, simple identity.  This one tells us that \(\ln \big( \prod_j  \exp(x_j)^{w_{ij}} \big) = \sum_j w_{ij} x_j\), which you can algebraically check is true.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/manipulation_example.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now, let’s put together a multiplicative net and see what we can derive.  Our starting point will be the same simple 3-3-3-3 net as before, but with multiplicative layers; it will be clear that our operations will generalize to different sizes.  First, we use the multiplicative-additive identity to get an additive net.  However, instead of just having one nonlinear function acting on each element of the feature vector, there are now three in succession.  Using function composition, we can just group these into a new nonlinearity we define as \(\sigma' = \ln \circ \sigma \circ \exp\).  We now arrive at an additive net &lt;em&gt;exactly equivalent to the multiplicative net&lt;/em&gt;.  The only major oddity of the new additive net is elementwise logarithms at the start and exponentials at the end.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/multiplicative_net_transformation.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;These logarithms and exponentials at the start and end aren’t surprising.  We’re requiring that multiplicative nets need positive input and give positive output, so since logarithms of negative numbers aren’t real, these logarithms enforce the positivity of the input.  The exponentials similarly enforce the positivity of the output.  Also, as long as \(\sigma\) maps positive numbers to positive numbers, the net only operates on positive numbers intermediately, and we don’t have the undefined power problem.  We can make our diagram even simpler by absorbing the logarithms and exponentials into the inputs as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/end_absorption.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;It turns out that a multiplicative net is basically the same as an additive net &lt;em&gt;with the same weights!&lt;/em&gt;  Our choice of notation emphasizes the similarity of form.  However, is this really a meaningful, useful correspondence?  The main question is this: what’s the nonlinearity \(\sigma'\) like?  It’s possible that \(\sigma' = \ln \circ \sigma \circ \exp\) is some bizarre function that’d be totally nonfunctional in an additive net, which would bode badly for our multiplicative net.  To answer this, a few examples are plotted below.  The blue curves show different choices of nonlinearity for multiplicative nets, defined only for positive inputs, and the orange curves show the corresponding additive nonlinearities.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/nonlinearities.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As shown in (a), when \(\sigma\) is the \(\tanh\) function, \(\sigma'\) looks a lot like a smoothed \(\text{ReLU}\), also called a softplus!  It’s flipped across the x- and y-axes, but that doesn’t change the usefulness of a nonlinearity in an additive neural net.  This would definitely work as an activation function.  Surprisingly, (b) shows that when \(\sigma\) is a sigmoid, \(\sigma'\) is also roughly a sigmoid (but again flipped across both axes), which also works as a nonlinearity.  Shown in (c) is the case when the multiplicative net uses a \(\text{ReLU}\) nonlinearity.  This is a distinctly horrendous choice for only positive inputs, since it’s the same as the identity on positive input, which is reflected in the fact that \(\sigma'(x) = x\).  A multiplicative neural net with \(\text{ReLU}\) nonlinearities is basically linear; adding extra layers doesn’t give it any power since, for additive networks, a multilayer linear net is reducible to a linear model.  With a slight modification, however, we get a very useful nonlinearity.  As shown in (d), if the nonlinear net uses a modified \(\text{ReLU}\) - specifically, \(\sigma(x) = \max(x, 1)\) - the corresponding linear net has exactly \(\text{ReLU}\) activations.&lt;/p&gt;

&lt;p&gt;It might not be clear what, exactly, this means for multiplicative nets.  Let’s suppose that we’re using a multiplicative neural net to do a task - say, classifying images - and we operate it by exponentiating the pixel values at input and taking the log of the output.  We have shown that, for a reasonable nonlinearity, the function the multiplicative net computes as a function of weights and data - say, \(f(X ; W)\) - is exactly the same as for a reasonable additive net.  That means that the loss surface is the same.  That means that the gradients are the same, and that means that the trainability is the same, and all this means that the theoretical usefulness of multiplicative neural nets, including their representational power and optimization behavior, are the same as the standard additive neural nets that have been the focus of such intense study.  Multiplicative nets are just as effective, there’s a correspondence to additive nets, and we have the conversion algorithm.&lt;/p&gt;

&lt;p&gt;This fact is intriguing, but what does it mean in a broader sense?  This doesn’t make multiplicative nets practically useful; it’s still faster to perform a matrix multiplication than a multiplicative step, and since they amount to the same thing there’s no reason to prefer the latter.  To me, the reason this fact bears consideration is the hint it might be towards what truly gives deep learning its unreasonable effectiveness.  Multiplicative and additive nets have starkly different functional forms, and at a first glance I’d guess they have wholly different behavior as models.  The fact that they actually have equivalent power and utility seems like a hint that the fundamental magic of deep learning doesn’t lie in the details of implementation, like the choice of activation function or even the choice of matrix multiplication for mixing; the latter’s just preferred for convenience.  It seems likely that the key to deep learning’s success is something more deep and general about hierarchical nonlinear structures, and wildly disparate hierarchical models, from standard neural nets to multiplicative nets to tensor networks, all succeed because in some deep way they all fit this broad category.  Perhaps efforts to understand deep learning will eventually uncover a mathematical understanding of something like this.&lt;/p&gt;

&lt;h2 id=&quot;extras&quot;&gt;Extras&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Wait,” you might say, “there’s no clean correspondence to an additive net if the multiplicative net has a more complicated form, like involving a sum of product terms.  This analysis only works in one case when it happens to be trivial!”  It’s true that the exact correspondence is easy to break by adding a bell or a whistle to the multiplicative net.  The question, however, is whether these modifications add anything fundamentally different, or whether they’d be incremental changes at most.  I can only conjecture for now - maybe I’ll do experiments if this becomes a paper - but I’d guess it’s likely that there are many ways to superficially change the form of multiplicative nets without fundamentally changing their behavior because the same is true of additive nets.  For example, you can add extra parameters to neural nets by parameterizing the activation functions themselves; &lt;a href=&quot;https://arxiv.org/pdf/2006.03179.pdf&quot;&gt;this paper&lt;/a&gt; not only tried that, but ran an evolutionary algorithm to build complicated activation functions, and their error rates were only a few percent lower than with simple \(\text{ReLU}\)s.  Many such engineering intricacies lead to no big change in performance.  For that reason, I think it’s a decent hypothesis that multiplicative nets behave similarly to additive nets even when there’s no exact correspondence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;In the diagrams I drew, the nets have only weights, not biases.  Since \(\exp \big( \sum_j w_{ij} \ln( x_j ) + b_j \big) = e^{b_j} \prod_j x_j ^ {w_{ij}}\), we can add them by multiplying the product by \(e^{b_j}\), where \(b_j\) is a new bias parameter.&lt;/li&gt;
  &lt;li&gt;The choice of base \(e\) (i.e. \(\exp\) and \(\ln\)) in this article was arbitrary, and any other base would’ve worked.&lt;/li&gt;
  &lt;li&gt;Many types of specialized neural net layer can also be translated into multiplicative nets.  For example, a softmax is similar, but it doesn’t even require taking the exponentals.  \(\exp\) and \(\ln\) are monotonic, so they commute through max operations, so maxpool layers are the same for multiplicate nets.&lt;/li&gt;
  &lt;li&gt;Are there other choices for the mixing operation besides the matrices of additive nets and the multiplication operation shown here?  One way to generalize the concept to include both is to make the mixing transform \(f \big( \sum_j w_{ij} f^{-1}(x_j) \big)\), with \(f\) an invertible function mapping reals to reals.  In the case of \(f = \text{identity}\), this gives additive nets.  In the case of \(f = \exp\), this gives multiplicative nets.  In a case like \(f(x) = x^3\), though, it would give something new, but still equivalent to an additive net by the same sort of proof.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sci2s.ugr.es/keel/pdf/specific/articulo/Schmidtt%20on-the-complexity-of.pdf&quot;&gt;The most recent paper I’ve found on these nets&lt;/a&gt;, which was mentioned above, argues that multiplicative neurons perform operations that need large numbers of summing units, stating that “networks of summing units do not seem to be an adequate substitute for genuine multiplicative units.”  This post basically disproves that!&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 31 Aug 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/multiplicative-neural-nets/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/multiplicative-neural-nets/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
  </channel>
</rss>
