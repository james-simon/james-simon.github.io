<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JS</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 05 Jun 2024 12:33:54 -0700</pubDate>
    <lastBuildDate>Wed, 05 Jun 2024 12:33:54 -0700</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>A eigenframework for the generalization of 1NN</title>
        <description>&lt;p&gt;In this blogpost, I’ll present an eigenframework giving the generalization of the nearest-neighbor algorithm (1NN) on two simple domains — the unit circle and the 2-torus — and discuss the prospects for a general theory.&lt;/p&gt;

&lt;h2 id=&quot;why-1nn&quot;&gt;Why 1NN?&lt;/h2&gt;

&lt;p&gt;I think 1NN is a good candidate for &lt;a href=&quot;/blog/lets-solve-more-learning-rules&quot;&gt;“solving” in the omniscient sense&lt;/a&gt; because it’s relatively simple, and it’s also a &lt;em&gt;linear learning rule&lt;/em&gt; in the sense that, condition on a dataset, the predicted function $\hat{f}$ is a linear function of the true function $f$. This means that we might expect a nice eigenframework to work for MSE.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;1nn-on-the-unit-circle-aka-the-1-torus&quot;&gt;1NN on the unit circle (aka the 1-torus)&lt;/h2&gt;

&lt;p&gt;The setting here will be pretty natural: we have some target function $f: [0,1) \rightarrow \mathbb{R}$ we wish to learn, we draw $n$ samples \(\{x_i\}_{i=1}^n\) from $U[0,1)$ and obtain noisy function evaluations $y_i = f(x_i) + \mathcal{N}(0, \epsilon^2)$, and then on each test point $x$ predict $y_{i(x)}$ where $i(x)$ is the index of the closest point to $x$, with circular boundary conditions on the domain. Here’s what it looks like to learn, say, a sawtooth function with 20 points:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/1nn_eigenframework/sawtooth_realspace_1nn_plot.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We will describe generalization in terms of the &lt;em&gt;Fourier decomposition&lt;/em&gt; of the target function $f(x) = \sum_{k=-\infty}^{\infty} e^{2 \pi i k x} v_k,$ where ${ v_k }$ are the Fourier coefficients. In terms of the Fourier decomposition, we have that&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div style=&quot;border: 1px solid black; padding: 10px; display: inline-block;&quot;&gt;
$$
[\text{test MSE}]_\text{1D} = \sum_k \frac{2 \pi^2 k^2}{\pi^2 k^2 + n^2} \, v_k^2 + 2 \epsilon^2. 
$$
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This is the eigenframework for 1NN in 1D! The generalization of 1NN on &lt;em&gt;any&lt;/em&gt; target function in 1D is described by this equation.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Here are some experiments that show we’ve got it right.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/1nn_eigenframework/1nn_1d_learning_curves.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This equation can be found a few ways, but most simply you (a) argue by symmetry that the eigenmodes contribute independently to MSE and then (b) find the test MSE for a particular eigenmode, which isn’t too hard to do. I made two approximations: (1) instead of having exactly $n$ samples, I let the samples be distributed as a Poisson process (i.e., each point has an i.i.d. chance of containing a sample), and (2) I assume the domain is infinite when computing a certain integral. Neither of these matters when $n$ is moderately large.&lt;/p&gt;

&lt;p&gt;Before moving on to the 2D case, I want to pause to highlight a few cool features of this framework:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Fourier modes do not interact — no crossterms $v_j v_k$ appear in the framework. Because 1NN is a linear learning rule, it’s inevitably possible to “diagonalize” the theory in this way for a particular $n$, but the fact that it’s simultaneously diagonalizable for all $n$ is pretty cool!&lt;/li&gt;
  &lt;li&gt;Ignoring the factors of $\pi$, we see that mode $k$ is unlearned when $n \ll k$ and fully learned when $n \gg k$. This makes a lot of sense — it’s sort of a soft version of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Nyquist_frequency&quot;&gt;Nyquist frequency&lt;/a&gt; cutoff.&lt;/li&gt;
  &lt;li&gt;Things simplify a bit if, instead of using the index $k$, we write in terms of the Laplacian eigenvalues $\lambda_k = 4 \pi^2 k^2$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1nn-on-the-2-torus&quot;&gt;1NN on the 2-Torus&lt;/h2&gt;

&lt;p&gt;We now undertake the same problem, but we’re in 2D: our data is i.i.d. from $[0,1)^2$, again with circular boundary conditions. Our Fourier decomposition now looks like&lt;/p&gt;

\[f(\mathbf{x}) = \sum_{\mathbf{k} \in \mathbb{Z}^2} e^{2 \pi i \, \mathbf{k} \cdot \mathbf{x}}.\]

&lt;p&gt;The same procedure as in the 1D case yields that&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div style=&quot;border: 1px solid black; padding: 10px; display: inline-block;&quot;&gt;
$$
[\text{test MSE}]_\text{2D} = \sum_\mathbf{k} \left( 2 - 2 e^{- \pi \mathbf{k}^2 / n} \right) \, v_\mathbf{k}^2 + 2 \epsilon^2. 
$$
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
Here are some experiments that show that we’ve got it right:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/1nn_eigenframework/1nn_2d_learning_curves.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;can-you-extend-these-to-k-nearest-neighbors-instead-of-just-one&quot;&gt;Can you extend these to k nearest neighbors instead of just one?&lt;/h4&gt;

&lt;p&gt;Probably! I haven’t tried, though.&lt;/p&gt;

&lt;h2 id=&quot;can-we-find-a-general-theory&quot;&gt;Can we find a general theory?&lt;/h2&gt;

&lt;p&gt;The most remarkable feature of the eigenframework for linear regression is that one set of equations works for every distribution. Can we find that here?&lt;/p&gt;

&lt;p&gt;I’m not sure. At first blush, the 1D and 2D equations look pretty different — if we were in search of some master equation, we’d almost certainly want it to unify those two cases, but the functional forms look pretty different! I suspect it might be possible, though — the linear regression eigenframework requires solving an implicit equation that depends on all the eigenvalues and can yield pretty different-looking final expressions for different spectra, so it’s believable to me that there’s some common thread here.&lt;/p&gt;

&lt;p&gt;A big question in the search for a general eigenframework is: what’s the right notion of &lt;em&gt;eigenfunction?&lt;/em&gt; In these highly symmetric domains, it’s easy — the Fourier modes are the eigenfunctions of any translation-invariant operator we might choose. In general, though, I don’t know what operator we want to diagonalize, or how to use its eigenvalues. This puzzles me. I don’t know the answer! So strange.&lt;/p&gt;

&lt;p&gt;Even without a general theory, though, I think it’d be really interesting just to continue this chain of special cases up to the arbitrary $d$-torus! I got stuck at $d=2,$ but I’d bet it can be done. Then we could think about how 1NN differs from kernel regression in high dimensions!&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; My understanding is that 1NN is worse, but I don’t actually know any quantification of this. (If you know one, drop me a line!)&lt;/p&gt;

&lt;p&gt;So that’s where things stand: these two special cases serve as proof of principle that there &lt;em&gt;might&lt;/em&gt; be a general eigenframework for 1NN… and if that exists, it’d be a cool proof of principle that more learning rules than linear regression can be “solved.” Seems like a neat problem! If I make much more progress on it, maybe I’ll write up a little paper. Let me know if you have thoughts or want to collaborate.&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Intriguingly, the fact that it’s linear also means that 1NN exactly satisfies the &lt;a href=&quot;[https://arxiv.org/abs/2110.03922](https://arxiv.org/abs/2110.03922)&quot;&gt;“conservation of learnability” condition&lt;/a&gt; obeyed by linear and kernel regression, which means that you can ask questions about how these two different algorithms allocate their budget of learnability differently. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Our Fourier decomposition is easily adapted to nonuniform data distros on our domain.] In order to understand the generalization of 1NN on a target function, it suffices to compute its Fourier transform and stick the result into this equation. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is particularly interesting because it could let us get at a puzzle posed by Misha Belkin &lt;a href=&quot;https://arxiv.org/abs/2105.14368&quot;&gt;here&lt;/a&gt; of why &lt;em&gt;inverse&lt;/em&gt; methods like KRR outperform &lt;em&gt;direct&lt;/em&gt; methods like kNN and kernel smoothing in high dimensions. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 05 Jun 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/1nn-eigenframework/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/1nn-eigenframework/</guid>
        
        
        <category>research</category>
        
      </item>
    
      <item>
        <title>Let's solve more learning rules</title>
        <description>&lt;p&gt;&lt;em&gt;TLDR: the field now has an “omniscient solution” for the generalization of linear regression. We should try to find comparable solutions for other simple learning rules, like k-nearest-neighbors and kernel smoothing!
This post is motivation for &lt;a href=&quot;/blog/1nn-eigenframework&quot;&gt;part two&lt;/a&gt;, where I give a partial omniscient solution to the nearest-neighbor algorithm.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;intro-what-does-it-mean-to-solve-a-learning-rule&quot;&gt;Intro: what does it mean to “solve” a learning rule?&lt;/h2&gt;

&lt;p&gt;Essentially all of machine learning theory aims to understand the performance and behavior of different schemes for making predictions on unseen data. This is often very difficult. Given the amount of effort expended in this direction, it’s worth stepping back and asking what we’re ultimately trying to do here — that is, how will we know when we’re done?&lt;/p&gt;

&lt;p&gt;To me, one gold-standard answer is the following: &lt;strong&gt;for each learning rule under study, we want a simple set of equations that tells us how well the learning rule will perform&lt;/strong&gt; in terms of the task eigenstructure and number of samples $n$&lt;strong&gt;.&lt;/strong&gt; This isn’t the only thing we might want, but it’s clearly a powerful milestone: you could use such a theory to work out when the algorithm will perform well or poorly, and in order to derive the theory you’d have to understand a lot about the behavior of the learning rule. Viewing each learning rule as presenting a puzzle for theorists, I’d consider the development of such a theory as tantamount to “solving” the learning rule.&lt;/p&gt;

&lt;p&gt;An important clarification is that this is an “omniscient” solution in the sense that you assume complete and exact knowledge of the training task. This isn’t very useful by itself as a practical tool, but it is very useful for conceptual understanding of the learning rule (e.g. for characterizing its inductive bias). In light of this, to be more precise, we’ll say that this sort of theory is an “omniscient solution” for a learning rule.&lt;/p&gt;

&lt;p&gt;In my view, one of the most significant recent developments in machine learning theory is that &lt;em&gt;we have an omniscient solution for linear regression!&lt;/em&gt; That is, thanks to &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2001/hash/d68a18275455ae3eaa2c291eebb46e6d-Abstract.html&quot;&gt;lots&lt;/a&gt; &lt;a href=&quot;https://www.arxiv.org/abs/2002.02561&quot;&gt;and&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.09796&quot;&gt;lots&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2210.08571&quot;&gt;of&lt;/a&gt; &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/9704a4fc48ae88598dcbdcdf57f3fdef-Paper.pdf&quot;&gt;recent&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1903.08560&quot;&gt;papers&lt;/a&gt;, we now have a simple set of closed-form equations that accurately predict the test performance of linear regression in terms of the task eigenstructure.&lt;/p&gt;

&lt;p&gt;Here’s some flavor for how this theory looks. First, we define an orthonormal basis of functions ${ \phi_k }$ over the input space. (In the case of linear regression, these are linear functions and are the principal directions of the feature covariance matrix.) We then decompose the target function $f$ into this basis as&lt;/p&gt;

\[f(x) = \sum_k v_k \phi_k(x)\]

&lt;p&gt;where ${ v_k }$ are the eigencoefficients. (Noise may also be included, but I omit it here for simplicity.) The eigenframework then takes the form&lt;/p&gt;

\[\text{test MSE} = \sum_k g(n, k)  \, v_k^2,\]

&lt;p&gt;where $g(n,k)$ tells you what fraction of the signal in mode $k$ contributes to test error at $n$ samples. (The function $g(n,k)$ typically decreases to zero as $n$ grows.)&lt;/p&gt;

&lt;p&gt;Here are some remarkable facts about this eigenframework:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It gives accurate predictions of test MSE even on real data!&lt;/li&gt;
  &lt;li&gt;The eigenmodes don’t interact! That is, there are no crossterms $v_j v_k$ in the final expression.&lt;/li&gt;
  &lt;li&gt;It’s very mathematically simple! In particular, the function $g$ isn’t too complicated, I just didn’t want to get into the details here. It’s simple enough that you can study it in various limits to really get intuition for how linear regression generalizes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;I’m a big fan of results of this type.&lt;/strong&gt; They feel like the way to make progress in this field: rather than showing every new result from square one, first we derive this simple and general eigenframework &lt;em&gt;once,&lt;/em&gt; and then can use it as a starting point to compute other quantities — error bounds, convergence rates, effects of dimensionality, effects of regularization, and more.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;This is how physics (and specifically statistical mechanics) works — when studying a complex system, you first seek a simple effective theory for the system, and then can easily ask lots of downstream questions about the &lt;em&gt;effective&lt;/em&gt; theory.&lt;/p&gt;

&lt;h2 id=&quot;call-to-action-lets-solve-more-learning-rules&quot;&gt;Call to action: &lt;em&gt;let’s solve more learning rules!&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;The above omniscient solution for linear regression has been very impactful. We should try to do this for more learning rules.&lt;/p&gt;

&lt;p&gt;Learning rules like k-nearest-neighbors (kNN), kernel smoothing, spline interpolation, decision trees, clustering, SVMs, and generalized additive models are widely used in practice, but we do not understand any of them as well as we now understand linear regression. It would be very impactful to solve any of them. Furthermore, the first three — kNN, kernel smoothing, and spline interpolation — are &lt;em&gt;linear&lt;/em&gt; learning rules in the sense that the predicted function depends linearly on the training targets (i.e., the training $y$’s), which suggests that an eigenframework of the &lt;em&gt;same&lt;/em&gt; type as that for linear regression might be possible. These last three seem simple enough that you might be able to solve them in one paper!&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I suspect that omnisciently solving more learning rules would have a large impact — potentially a paradigm-altering impact, actually. Right now, all these learning rules are just a bag of disjoint algorithms: we don’t really understand any of them individually, and we sure don’t understand how they relate to each other. Any intuition I have about their relative performance is extremely ad-hoc — for example, kNN works best in low dimensions and at low noise, SVMs are often better in high dimensions, decision trees work well when different features have different notions of distance, and so on. &lt;strong&gt;If we understood all these learning rules in the same way as we understand linear regression, we could compare them on the same footing!&lt;/strong&gt; Instead of a bunch of ad-hoc intuitions, we could start to think of all these learning rules in one unified way. This feels like a place that machine learning ought to eventually get to.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;(/blog/1nn-eigenframework)&quot;&gt;part two&lt;/a&gt;, I’ll give the solution for 1NN on the unit circle and 2-torus and discuss what it might look like to solve 1NN in general!&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;To illustrate this point, &lt;a href=&quot;https://arxiv.org/abs/2207.06569&quot;&gt;here&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/pdf/2306.13185&quot;&gt;are&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2311.14646&quot;&gt;four&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.03922&quot;&gt;papers&lt;/a&gt; in which I’ve used this eigenframework as a starting point to easily derive other results. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Two other learning rules: obviously we’d all love to understand neural networks, but they’re still too poorly understood for a theory of generalization, I think — we need to understand more about their dynamics first. Second, random feature (RF) regression is a nice generalization of linear regression — and I derived an eigenframework for RF regression for &lt;a href=&quot;https://arxiv.org/abs/2311.14646&quot;&gt;a recent paper&lt;/a&gt; ;) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 04 Jun 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/lets-solve-learning-rules/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/lets-solve-learning-rules/</guid>
        
        
        <category>research</category>
        
      </item>
    
      <item>
        <title>Ibuprofen vs. aspirin vs naproxen vs acetaminophen</title>
        <description>&lt;p&gt;Ibuprofen, aspirin, naproxen, and acetaminophen are the most common drugs for pain relief.
They’re staples of the modern pharmaceutical arsenal, and they’re in virtually every American home.
After years of not bothering to learn the difference between them, I finally sat down and did it yesterday, and I figured I’d write this as a reference.
I’ll give a bit about each drug, and then close with some bigger questions that feel interesting to me.&lt;/p&gt;

&lt;h2 id=&quot;basic-drug-facts&quot;&gt;Basic drug facts&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Ibuprofen&lt;/strong&gt; (brand names Advil, Motrin) is a “non-steroidal anti-inflammatory drug” (NSAID).
As the term suggests, it’s an anti-inflammatory drug, so it’s particularly useful for treating pain caused by inflammation, including headaches, menstrual cramps, and arthritis.
It works by temporarily binding to (and thus interfering with) “cyclooxygenase” (COX) enzymes which produce prostaglandins, which are involved in pathways for inflammation.
It was developed in the 1960s as a safer alternative to aspirin.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aspirin&lt;/strong&gt; (which apparently has no brand names) is a different NSAID.
It’s sometimes prescribed for long-term use as a means of reducing the risk of heart attacks.
It also works by binding to COX enzymes, but interesting, in the case of aspirin, this binding is permanent – it stays bound until the enzyme is destroyed!
It’s pretty old – it was developed in the late 1890s!
It’s more dangerous than ibuprofen, though, with more risk of side effects on the digestive system, which I imagine is why ibuprofen is the more common over-the-counter medication these days.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Naproxen&lt;/strong&gt; (brand name Aleve) is also an NSAID and also dates from the 1960s.
It acts very similarly to ibuprofen, but it lasts maybe 2x longer (but takes a bit longer to kick in).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Acetaminophen&lt;/strong&gt; (brand name Tylenol; also called paracetamol internationally) is &lt;em&gt;not&lt;/em&gt; a NSAID.
It still affects the same pathway as NSAIDs, though, interfering with COX enzymes, and generally seems pretty similar, though &lt;a href=&quot;https://now.tufts.edu/2022/09/14/how-does-acetaminophen-work&quot;&gt;it’s still a mystery how exactly it works.&lt;/a&gt;
It has fewer side effects, &lt;em&gt;but&lt;/em&gt; seems to have much higher risks of overdose – in fact, it’s responsible for more overdoses than any other drug in the Western world!
It seems to primarily harm the liver and combine particularly poorly with alcohol.&lt;/p&gt;

&lt;p&gt;In my home growing up, there was also a small bottle of something called “excedrin.” Somewhat ridiculously, this looks to be a combination of aspirin, acetaminophen, and &lt;em&gt;caffeine&lt;/em&gt;!&lt;/p&gt;

&lt;h2 id=&quot;so-what-are-these-things-anyways&quot;&gt;So what &lt;em&gt;are&lt;/em&gt; these things anyways?&lt;/h2&gt;

&lt;p&gt;These drug facts are well and good – it’s nice to know the &lt;em&gt;effects&lt;/em&gt; of these substances – but what &lt;em&gt;are&lt;/em&gt; they?
Each substance consists of just one fairly small molecule.
Here they are – see if you can guess which is which. Answers are at the bottom of the page.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/pain_relievers/structures.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;I find it pretty amazing that these small molecules can have such specific effects!
The body is huge and has a truly immense number of processes going all the time, and these molecules are taken orally and presumably spread throughout most of the body, and they &lt;em&gt;don’t seem complicated enough&lt;/em&gt; to be addressed to a very specific function, and yet they specifically target one particular enzyme in one particular pathway.
It’s even more surprising to me that multiple simple molecules all do this job – though perhaps that’s just an indication that that enzyme is easily messed with.&lt;/p&gt;

&lt;p&gt;Looking at these molecular structures, I’m reminded anew of how weird and unintuitive biochemistry tends to be.
If you show me a picture of a mechanical part and tell me what it does, I can generally imagine how it might perform that role, and how a similar part might not be as good a choice for it, but I almost never have this experience looking at a biomolecule, and when I find out how the biomolecule works, it usually sounds ad hoc and overly complicated to me!
I wonder if learning some basic biochemistry would make things seem more mechanically intuitive.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;The molecules are 1) naproxen, 2) aspirin, 3) ibuprofen, and 4) acetaminophen. I have no idea how you could guess this, so congratulations if you did.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 01 Jun 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/otc-pain-relievers/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/otc-pain-relievers/</guid>
        
        
        <category>unlisted</category>
        
      </item>
    
      <item>
        <title>Geometric patterns in croplands</title>
        <description>&lt;p&gt;I’ve lived in California for five years now and have had the good fortune to roam up and down the state and see its mountains, valleys, and deserts.
As any outdoorsy Californian will know, these treks often featured long drives through the Central Valley, past wide fields of California crops: fruits, nuts, grain.
As a frequent passenger in these drives, I’ve come to look forward to gazing out a changing windowful of cropland, particularly when it makes a certain sort of geometric pattern.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
	&lt;a href=&quot;https://www.google.com/maps/@35.8815158,-119.858065,3a,75y,51.88h,81.48t/data=!3m6!1e1!3m4!1sXlBlznQpA42OVCiQcwKAEg!2e0!7i16384!8i8192?entry=ttu&quot;&gt;
		&lt;img src=&quot;/img/croprows/croprow_pic_2.jpg&quot; width=&quot;100%&quot; /&gt;
	&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;If you’re not familiar with this effect, take a minute to really examine the strip of field planted with the baby trees.
As you scan across the strip, you’ll notice places where the saplings form lines leading off into the distance.
The lines have different degrees of definition, and the gaps between the lines have different sizes in different places.
The “regions” of lines seem to have a curious fractal pattern: if you look between two neighboring line-regions, you’ll find another, smaller set of more closely spaced lines, and so on recursively.
I find it mysterious and beautiful.
For fun, you can click on the above image to be taken to the corresponding spot in Google Streetview if you want to explore it for yourself.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
	&lt;img src=&quot;/img/croprows/cropland_with_regions.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The regions pop out to the eye even more when you’re moving! Here’s a gif of the view out my window as I drove by the same place.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
	&lt;img src=&quot;/img/croprows/croprow_gif.gif&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;What’s going on? In this post, I’ll explain why we see these “open” directions and try to explain why the overall visual effect is so fractally.
The basic explanation of the open regions will need only some simple math used in the physics of crystals.
When we try to understand the fractally effect, we’ll encounter some cool number theory.
I’ll end with a neat numerical experiment and an open question.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-open-directions-of-an-infinite-lattice&quot;&gt;The open directions of an infinite lattice&lt;/h2&gt;

&lt;p&gt;The nature of the special directions is easily understood starting from the observation that these saplings are planted in a near-perfect rectangular grid.&lt;sup id=&quot;fnref:f&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:f&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
In such a grid, one expects the plants to leave “aisles” along the grid directions, and if one plays around with a drawing of a grid, one soon observes that the points also form narrower isles in other, off-axis directions.
See the colored “aisles” in the illustration below.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
	&lt;img src=&quot;/img/croprows/grid_gap_illustration.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The three examples in the above illustration correspond to particular “lattice vector” – that is, vectors \((a, b)\) with integer components, with \(a, b\) “irreducible” – that is, sharing no factors greater than \(1\).&lt;sup id=&quot;fnref:q&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:q&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
It turns out that all “open directions” – that is, directions one can look without hitting (or passing arbitrarily close to) a tree – correspond to such lattice vector!&lt;/p&gt;

&lt;p&gt;The above illustration highlights the gaps between the rows of trees, but we might similarly talk about the rows of trees lying &lt;em&gt;between&lt;/em&gt; the colored rectangles.
This is the perspective taken in crystallography.
The atoms in a crystal form a regular lattice by definition, and this lattice can be decomposed into many parallel rows (or &lt;em&gt;sheets&lt;/em&gt; in 3D), and light of the appropriate frequency will scatter off these surfaces as if they were partially reflective mirrors.
This is a common experimental technique for &lt;a href=&quot;https://en.wikipedia.org/wiki/X-ray_crystallography&quot;&gt;identifying different crystals&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;which-open-directions-will-you-see&quot;&gt;Which open directions will you see?&lt;/h2&gt;

&lt;p&gt;Great – we now have a rule describing our open directions: every direction in which one can gaze unblocked off to infinity is parallel to a lattice vector \((a, b)\), where \(a, b\) are an irreducible pair of integers.
When we look out at a big field, we should thus see a total number of… *&lt;em&gt;furiously counting on fingers&lt;/em&gt;* &lt;strong&gt;infinitely many&lt;/strong&gt; open directions, one for every \((a, b)\).
Well, that doesn’t seem right, looking at our photograph above – what’s gone wrong?&lt;/p&gt;

&lt;p&gt;Three things.
First, the field in the photo above is finite, and there are some directions we don’t see that would emerge if the field were extended.
Second, our camera has finite resolution.
Third, the trees have &lt;em&gt;nonzero width&lt;/em&gt; – some lattice directions give wider “aisles” than others, and if an aisle’s narrower than the size of a tree, we won’t see it even provided an infinite field and perfect camera.&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;All three of these effects point to a general conclusion: &lt;strong&gt;wider “aisles” are more visually salient.&lt;/strong&gt;
It’d thus be useful to know the “aisle width” (or equivalent the “lattice row spacing”) corresponding to a particular lattice direction.
Some quick geometry tells us that, with pointlike trees on a unit lattice, the aisle width in direction \((a, b)\) is \(w_{a,b} = \frac{1}{\sqrt{a^2 + b^2}}.\)
This gives us a satisfying ordering of lattice directions: a lattice direction \((a, b)\) is more visually salient the smaller \(\sqrt{a^2 + b^2}\) is – that is, the closer it is to the origin!
We should thus expect the directions \((0, \pm 1)\) and \((\pm 1, 0)\) to appear most visually striking, followed by \((\pm 1, \pm 1)\), then \((\pm 1, \pm 2)\) and \((\pm 2, \pm 1)\), and so on and so forth.&lt;/p&gt;

&lt;p&gt;I’ve annotated our original image with the actual lattice directions.
This is precisely what we see:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
	&lt;img src=&quot;/img/croprows/cropland_with_regions_labeled.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Note that the different lattice directions look like they’re parallel in this image, but they actually point radially outwards from the camera at different angles.
If we had an infinite field and a camera of good enough precision, we’d expect to see open directions corresponding to all irreducible \((a, b)\) such that \(w_{a, b}\) is less than the width of one sapling.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fancy-math-farey-sequences-and-recursive-structure&quot;&gt;Fancy math: Farey sequences and recursive structure&lt;/h2&gt;

&lt;p&gt;The math so far is pretty familiar to physicists – we’re used to thinking about lattices and lattice vectors, having seen them a hundred times in quantum courses.
As I’ve looked out at scenes like this, the thing I really wanted to understand is: &lt;em&gt;why does it look so fractal?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let me try to be more precise: the little triangles of open “aisles” in our field photograph sure give the impression of a sort of recursive structure – given two big triangles, one usually finds a medium triangle between them, and there are never two big triangles too close to each other, sort of like a compass rose:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
	&lt;img src=&quot;/img/croprows/compass_rose.jpeg&quot; width=&quot;20%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;It very much looks to me like, if you started adding triangular regions in order of decreasing size, you’d tend to insert each new triangle closer than chance to the midpoint of its neighbors.
I actually simulated this and it’s not true – to my surprise the distribution appears to be uniform – but in my trying to understand this I did come across some cool math.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Farey_sequence&quot;&gt;Farey sequences&lt;/a&gt; are sequences of rational fractions defined in the following manner:
the Farey sequence of order \(n\) consists of all unique rational numbers in \([0,1]\) with denominator less than or equal to \(n\).
This actually maps nicely onto our problem restricted such that \(b \ge a \ge 0\), in which case the valid \(\frac{a}{b}\) are exactly the rational numbers in \([0, 1]\).
The Farey sequences have many weird properties, one of which is the following: if \(\frac{a}{b}\) and \(\frac{c}{d}\) are neighbors in a Farey sequence, then as you increase \(n\), the first element to appear between them is always \(\frac{a + c}{b + d}\).
In our problem, that means that if there are open directions at \((a, b)\) and \((c, d)\) and no bigger open direction between them, then the &lt;em&gt;biggest&lt;/em&gt; open direction between them will lie at \((a + c, b + d)\)!
For example, the biggest open direction between \((0,1)\) and \((1,0)\) is \((1,1)\), and the biggest open direction between Farey-neighbors \((3,5)\) and \((2,3)\) is \((5, 8)\).
The wiki page for the Farey sequences has all sorts of interesting recursive diagrams that fall out of this recursive structure.
(Sidebar: as of writing, I don’t understand why this property is true. I’d be happy to have someone explain it to me!)&lt;/p&gt;

&lt;p&gt;What does this tell us about the visual effect one sees when looking out at a regularly-planted field?
Well, it means that, when adding the next-biggest open region between two existing regions, it’ll tend to be placed closer to the &lt;em&gt;smaller&lt;/em&gt; of the two existing regions.
Looking back at our field photograph (esp the version annotated with arrows), this is certainly the case!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;can-apparent-openness-vs-angle-be-described-by-a-smooth-function&quot;&gt;Can “apparent openness” vs. angle be described by a smooth function?&lt;/h2&gt;

&lt;p&gt;Number theory is very beautiful, but essentially discrete in nature.
The special directions we find are pointlike: look in one &lt;em&gt;particular direction&lt;/em&gt; and you’ll see forever, but a little bit to either side and you won’t.
However, when I look out at a cropfield like this – particularly at the gif – my eye isn’t just drawn to certain points: a bigger region leading up to the point at infinity pops out to me visually.
This makes sense – in the idealized setting, if I look slightly off an infinite-sight direction, I’ll still see pretty far!
This makes me want some kind of &lt;em&gt;continuous&lt;/em&gt; function of angle that tells me, say, how far I’ll typically see when looking in that direction.
Perhaps this function is parameterized by some notion of tree diameter \(d\), and has peaks that appear at the rational angles as \(d\) decreases.&lt;/p&gt;

&lt;p&gt;I’ll say upfront that I tried for a while and didn’t come up with anything nice.
This is the open question: can one write down a nice analytical function of angle that in some sense captures “how far” one can see along angle \(\theta\) through a forest of regularly-spaced trees of diameter \(d\)?&lt;/p&gt;

&lt;p&gt;What I do have is a hacky numerical experiment and some cool plots.
I’ll use a proxy metric I can compute in Python: with trees of diameter \(d\), what’s the farthest one could possibly see in direction \(\theta\) if one stood in the optimal place?
Well, actually, I’ll use a metric that’s almost the same (in particular being infinite in the same places) but easier to compute: with pointlike trees, what’s the longest rectangle of width \(d\) that one can fit anywhere in the lattice with long side at angle \(\theta\) from an axis?&lt;/p&gt;

&lt;p&gt;Here’s a gif of the result.&lt;sup id=&quot;fnref:b&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:b&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
	&lt;img src=&quot;/img/croprows/openness_plots.gif&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There’s a lot of cool stuff to see in this!
Most obviously, you can see the lattice directions opening up in the predicted sequence: the cardinal directions (\((1,0)\) etc.) are open at the start, and then the \((1,1)\) direction and friends open up when \(d = \frac{1}{\sqrt{2}}\), and then the \((1,2)\) directions and friends open up, and so on.
Interestingly, a direction is always a local minimum just before it opens up – when \(d\) is just larger than \(w_{a,b}\), changing angle just a little bit in either direction will let you “see” one tree further.
This is a neat one to try to understand by playing with a diagram yourself.
There also seem to be no local maxima that aren’t vertical asymptotes.
These properties give the process a sort of fractally feel: concave-up regions between vertical asymptotes get recursively split in two by new asymptotes.&lt;/p&gt;

&lt;p&gt;This is cool and all – it’s a parameterized function which gains vertical asymptotes corresponding to all the rational numbers as the parameter decreases! – but I still want a nice analytical version of this.
If you have any ideas, let me know!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;code-and-acknowledgements&quot;&gt;Code and acknowledgements&lt;/h4&gt;

&lt;p&gt;You can find my code for generating these plots &lt;a href=&quot;https://github.com/james-simon/lattice-math&quot;&gt;here&lt;/a&gt;.
Thanks to Vincent Huang for pointing me to Farey sequences.&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:f&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I’m actually quite impressed that these plants are spaced regularly enough over such a large area to give such regular patterns! &lt;a href=&quot;#fnref:f&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:q&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;By this, we mean to disallow vectors like \((4, 6)\) and \((2, 0)\) where either (a) the elements are not coprime or (b) zero is paired with an integer not equal to one. &lt;a href=&quot;#fnref:q&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Noise in the placement of trees has essentially the same effect as nonzero tree width. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:b&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Note that I’m changing the size of the trees in this animation here even though it’s really the width of an imaginary rectangle that’s changing. &lt;a href=&quot;#fnref:b&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 21 Apr 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/cropland-crystallography/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/cropland-crystallography/</guid>
        
        
        <category>fun-math, random</category>
        
      </item>
    
      <item>
        <title>Favorite quotes from &lt;em&gt;Letters to a Young Poet&lt;/em&gt;</title>
        <description>&lt;style&gt;
blockquote {
    border-left: 4px solid #ccc;  /* Adds a left border to the block quote */
    margin: 1em 10px;             /* Adds margin for better spacing */
    padding-left: 15px;           /* Adds padding to the left for text indent */
    font-style: italic;           /* Makes the text italic */
    color: #333;                  /* Sets the text color */
    background-color: #f9f9f9;    /* Sets a light background for visibility */
}
&lt;/style&gt;

&lt;p&gt;Rainier Maria Rilke’s &lt;em&gt;Letters to a Young Poet&lt;/em&gt; is a short and singular book, consisting of a sequence of ten letters written by the famous poet and novelist to an aspiring young poet who initiated their correspondence in search of advice in poetry. Rilke’s letters are remarkable for their candor and sincerity and depth of feeling: Rilke’s advice extends beyond the question of how to compose literature to the question of how to live.&lt;/p&gt;

&lt;p&gt;I read this collection at a time in which I’m trying to work out how best to express some artistic tendencies of mine, and it was useful to hear Rilke’s thoughts on authentic living and artistic authenticity. Here are some of my favorite quotes.&lt;/p&gt;

&lt;p&gt;On living questions without knowing the answers:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You are so young, so before all beginning, and I want to beg you, as much as I can. dear sir, to be patient toward all that is unsolved in your heart and to try to love the questions themselves like locked rooms and like books that are written in a very foreign tongue. Do not now seek the answers, which cannot be given you because you would not be able to live them. And the point is, to live everything. Live the questions now. Perhaps you will then gradually, without noticing it, live along some distant day into the answer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On solitude:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The necessary thing is after all but this: solitude, great inner solitude. Going into oneself and for hours meeting no one – this one must be able to attain. To be solitary, the way one was solitary as a child, when the grownups went around involved with things that seemed important and big because they themselves looked so busy and because one comprehended nothing of their doings.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On Rilke’s own inner sadness:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;And if there is one thing more that I must say to you, it is this: Do not believe that he who seeks to comfort you lives untroubled among the simple and quiet words that sometimes do you good. His life has much difficulty and sadness and remains far behind yours. Were it otherwise he would never have been able to find those words.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On working in traditional genres:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Avoid at first those forms that are too facile and commonplace: they are the most difficult, for it takes a great, fully matured power to give something of your own where good and even excellent traditions come to mind in quantity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On one’s relationship with one’s skeptical parents:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Ask no advice from them and count upon no understanding; but believe in a love that is being stored up for you like an inheritance and trust that in this love there is a strength and a blessing, out beyond which you do not have to step in order to go very far!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On whether one should write:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You ask whether your verses are good. You ask me. You have asked others before. You send them to magazines. You compare them with other poems, and you are disturbed when certain editors reject your efforts. Now (since you have allowed me to advise you) I beg you to give up all that. You are looking outward, and that above all you should not do now.&lt;/p&gt;

  &lt;p&gt;Nobody can counsel and help you, nobody. Search for the reason that bids you write; find out whether it is spreading out its roots in the deepest places of your heart, acknowledge to yourself whether you would have to die if it were denied you to write. This above all—ask yourself in the stillest hour of your night: must I write? Delve into yourself for a deep answer.&lt;/p&gt;

  &lt;p&gt;And if this should be affirmative, if you may meet this earnest question with a strong and simple, “I must,” then build your life according to this necessity; your life even into its most indifferent and slightest hour must be a sign of this urge and a testimony to it.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 17 Apr 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/quotes-from-l-t-a-y-p/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/quotes-from-l-t-a-y-p/</guid>
        
        
        <category>random</category>
        
      </item>
    
      <item>
        <title>How many babies were born during the 2024 eclipse?</title>
        <description>&lt;p&gt;I was privileged enough to see the 2024 total eclipse. I watched it in a clearing in dry west Texas, surrounded by awed humans, most of whom, like me, had roadtripped from far away for the glorious event. The trip deserves its own post, but for the meantime, here’s a short little calculational anecdote.&lt;/p&gt;

&lt;p&gt;Some time after totality, while discussing the number of marriages during the eclipse, someone wondered: how many babies were born during totality? It’s an interesting question because, unlike weddings, you mostly can’t control it, but any baby born during totality will have an amazing story for their entire life. Another guy and I decided to try to estimate it.&lt;/p&gt;

&lt;p&gt;We’ll restrict our focus to babies born in the US. I’ll give two solutions: first, the rough estimate we did in the field without looking up any numbers, and second, a more refined estimate using real numbers.&lt;/p&gt;

&lt;h2 id=&quot;rough-estimate&quot;&gt;Rough estimate&lt;/h2&gt;

&lt;p&gt;We essentially want two numbers for the US: $\text{[births per unit (area} \times \text{time)]}$ and $\text{[total eclipse (area} \times \text{time)]}$. We can then multiply these numbers to get an expected total number of births.&lt;/p&gt;

&lt;p&gt;First we estimate the area of the US by approximating it as a giant rectangle:&lt;/p&gt;

\[\text{[contiguous US area]} \approx 3000 \times 1000 \ \text{mi}^2 = 3 \times 10^6 \ \text{mi}^2\]

&lt;p&gt;Then we the estimate the US birth rate by imagining that its entire population refreshes every 100 years:&lt;/p&gt;

\[\text{[US birth rate]} \approx \frac{\text{[US population]}}{\text{[human lifespan]}} \approx \frac{3 \times 10^8 \ \text{people}}{100 \ \text{years}} = 3 \times 10^6 \ \frac{\text{people}}{\text{year}}\]

&lt;p&gt;We estimate the eclipse path size as a long thin rectangle whose dimensions we’re guessing from memory:&lt;/p&gt;

\[\text{[eclipse path size]} \approx 1000 \times 50 \ \text{mi}^2 = 5 \times 10^4 \ \text{mi}^2\]

&lt;p&gt;And we use a number for the duration of totality:&lt;/p&gt;

\[\text{[eclipse duration]} \approx 10 \ \text{min}\]

&lt;p&gt;Finally, combining everything and approximating $\text{1 year} \approx 5 \times 10^5 \ \text{min}$, we find that&lt;/p&gt;

\[\begin{aligned}
\text{[number of eclipse births]} &amp;amp;\approx \frac{\text{[US birth rate]}}{\text{[contiguous US area]}} \times \text{[eclipse path size]} \times \text{[eclipse duration]} \\
&amp;amp;\approx \frac{3 \times 10^6 \ \text{people / year} \times 5 \times 10^4 \ \text{mi}^2 \times 10 \ \text{min}}{3 \times 10^6 \ \text{mi}^2} \\
&amp;amp;\approx 1 \ \text{person} \ (!!)
\end{aligned}\]

&lt;p&gt;Astonishingly, everything cancels, and we’re left with a prediction of, on average, &lt;em&gt;one baby born during totality in the United States!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;refined-estimate&quot;&gt;Refined estimate&lt;/h2&gt;

&lt;p&gt;Alright, let’s see how close our feeder quantities were to the real numbers. Standard references (Wikipedia, the CDC, etc.) tell us that, in reality,&lt;/p&gt;

\[\text{[contiguous US area]} \approx 3.1 \times 10^6 \ \text{mi}^2\]

\[\text{[US birth rate]} \approx  3.7 \times 10^6 \ \frac{\text{people}}{\text{year}}\]

&lt;p&gt;Looking an actual eclipse map, a better estimate for the size of the path of totality seems like&lt;/p&gt;

\[\text{[eclipse path size]} \approx 2100 \times 110 \ \text{mi}^2 = 2.31 \times 10^5 \ \text{mi}^2\]

&lt;p&gt;While it’s all over the internet that the duration of totality on the centerline of the path was about four minutes, I didn’t readily find any maps that show how that duration falls off as you get closer to the edge of the path. However, the fact that nobody seemed to worry much about being near the centerline suggests that the duration doesn’t fall off too fast — probably slower than linear! Working through the geometry of one circle translating and occluding a second, slightly smaller circle, it seems to me like a plot of duration vs. position traces a semicircle — that is, $\text{[duration]} \propto \sqrt{\text{[path half-width]}^2 - \text{[distance from centerline]}^2}$:&lt;sup id=&quot;fnref:q&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:q&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/eclipse_baby/eclipse_duration_plot.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; Since the plot here has a steep slope near its edges, you'll get close to maximum duration even if you're close to the edge of the path. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;Anyways, the upshot is that, actually, we can just take four minutes as the eclipse duration across the whole path and be mostly correct, and if we want to be more precise, we can average over the path width, which amounts to multiplying by a factor of $\frac{\pi}{4}$.&lt;/p&gt;

&lt;p&gt;Putting it all together, we get&lt;/p&gt;

\[\begin{aligned}
\text{[number of eclipse births]} &amp;amp;\approx \frac{3.7 \times 10^6 \ \text{people / year} \times 2.31 \times 10^5 \ \text{mi}^2 \times 4 \times \frac{\pi}{4} \ \text{min}}{3.1 \times 10^6 \ \text{mi}^2} \\
&amp;amp;\approx 1.6 \ \text{person}
\end{aligned}\]

&lt;p&gt;So, in light of this better calculation, how’d the rough estimate turn out? Surprisingly, our field estimates of the area and birth rate of the US were really good, and we underestimated the area of the eclipse but overestimated its duration in ways that came close to cancelling, so our refined estimate is just a bit bigger than our original estimate! It always surprises me how often these rough Fermi estimates are quite close to better calculations… I’d be really interested in a paper studying how errors usually accumulate in Fermi calculations to get some intuition for when to expect an accurate result.&lt;/p&gt;

&lt;h2 id=&quot;so-was-there-actually-an-eclipse-baby&quot;&gt;So was there actually an eclipse baby?&lt;/h2&gt;

&lt;p&gt;Googling thing related to “eclipse baby” returns &lt;a href=&quot;https://abcnews.go.com/GMA/Family/mom-welcomes-baby-named-sol-total-solar-eclipse/story?id=109038075&quot;&gt;this story&lt;/a&gt; of a Texas family whose baby was born on the eclipse day. This article says that it was born during the total eclipse, but the birth time they give is about half an hour before totality, and in &lt;a href=&quot;https://www.foxweather.com/watch/play-710fe6a0f001782&quot;&gt;this interview&lt;/a&gt; the parents say they saw the total eclipse after the birth, so it seems this is just a near miss.&lt;/p&gt;

&lt;p&gt;The question’s still out, then — was there a baby born during totality in 2024? I If you meet this baby, please introduce us.&lt;/p&gt;

&lt;!-- divider above footnotes --&gt;
&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:q&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;You get this functional form regardless of the relative sizes of the circles! It reduces to the question of whether the center of one circle is within a certain distance of the center of the second circle. &lt;a href=&quot;#fnref:q&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 15 Apr 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/eclipse-baby/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/eclipse-baby/</guid>
        
        
        <category>random</category>
        
      </item>
    
      <item>
        <title>Roadtripping to North Dakota</title>
        <description>&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/hover_effects.css&quot; /&gt;

&lt;p&gt;A few weeks ago, I did a solo roadtrip, spending two weeks driving a curving path from Nashville, TN to Bismarck, ND to go to every state I’d never been to. This was a deeply satisfying and rewarding experience – I got to drink deeply of solitude and nature and freedom, got to practice living on the road, and hit 50 states just before my father and brother, who were at 48 and 49 respectively and didn’t know I was doing this. Here I’ll sketch my journey and conclude with a few things I’m taking away from this journey.&lt;/p&gt;

&lt;h2 id=&quot;the-route&quot;&gt;The route&lt;/h2&gt;

&lt;p&gt;I had eight states to knock off: Georgia, Kentucky, Arkansas, Oklahoma, Kansas, Missouri, Nebraska, and North Dakota. I’d been picking off stragglers for years, leaving myself a big mostly-contiguous chunk in the midwest ripe for a high-value roadtrip. I viewed this as a more extreme version of how, when playing Tetris, one tends to carefully preserve a vertical gap for a long piece and then knocks out four rows at once.&lt;/p&gt;

&lt;p&gt;The one fixed pin of this trip – and the reason I did it when I did – was an engagement party one weekend in Nashville, so I decided to plan a long layover in Atlanta on my way there and then drive everywhere else. The final route looks like a path you’d only travel if you were trying to drive through a very specific sequence of states:&lt;sup id=&quot;fnref:a&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:a&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/img/states_roadtrip/route_map.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;some-logistics&quot;&gt;Some logistics&lt;/h2&gt;

&lt;p&gt;I lived out of a 2024 Hyundai Elantra for these two weeks. I’d actually rented the cheapest car available, but when I got to the lot, the rental agent told me he “wouldn’t do me dirty like that” and instead gave me a stupidly nice brand new car that, thanks to automated lanekeeping, basically drives itself on the highway.&lt;/p&gt;

&lt;p&gt;I stayed in a few Airbnbs and a motel, but I greatly preferred my nights in the woods. As it turns out, dispersed camping is allowed in national forests (which are nicely visualized on &lt;a href=&quot;https://www.fs.usda.gov/ivm/&quot;&gt;this wonderful map&lt;/a&gt;), and it’s somehow quite freeing to drive into a big wild forest and get to camp basically anywhere, picking an arbitrary dirt road and pitching camp too deep in the woods to be likely to see another soul.&lt;/p&gt;

&lt;p&gt;I set out from Nashville on the morning of March 17th. I’ll flash some scenes from the trip. Mouse over the photos for vignettes!&lt;/p&gt;

&lt;style&gt;
.hover-container {
  position: relative;
  width: 100%;
}

.image {
  display: block;
  width: 100%;
  height: auto;
}

.overlay {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  right: 0;
  height: 100%;
  width: 100%;
  opacity: 0;
  transition: .1s ease;
  background-color: #000000;
}

.hover-container:hover .overlay {
  opacity: .8;
}

.text {
  color: white;
  font-size: 12px;
  position: absolute;
  top: 10%; /* Closer to the top edge */
  left: 10%; /* Closer to the left edge */
  transform: translate(-5%, -5%); /* Adjust these translate values to fine-tune the position */
  text-align: center;
}
&lt;/style&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-2&quot;&gt;&lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/atlanta.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;In Atlanta, I took public transit into the city and rested in Centennial Park. The air was warm. I chatted briefly with a few homeless folks but mostly kept to myself.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/mammoth_caves.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;To check off Kentucky, I went to Mammoth Caves with a friend of friends from the engagement party. He turned out to be a mathematician, and we knocked some math puzzles back and forth as we walked around the largest cave in the world.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/memphis_pyramid.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;This ridiculous Bass Pro Shops pyramid stands in Memphis. Like the greater pyramids of Egypt, this structure sports four triangular sides, various anti-robbery mechanisms, and an assortment of dead animals, and is similarly anthropologically fascinating. Instead of mummies, however, this modern pyramid only contains old people.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/arkansas_2.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Ironically, I found myself unintentionally on the path of totality of the 2024 solar eclipse just a few weeks too early, and I’d drive far away and then do a whole second roadtrip back to the path from California.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/kansas_3.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Kansas was expansive, calm, and flat. As I drove north from Arkanasas, I could see the land change, quickly turning from rocky forested hills to flat Southern forest to open plains, all in the span of an hour or two.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/kansas_1.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;This Kansas town had a classic middle-America look that'd seem at home in the 1950's to me. I slept in Kansas City, Missouri that evening.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/sioux_falls.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I stayed a few days with Matthew Schallenkamp, a friend and coworker, in Sioux Falls, SD, working on a coding project and seeing the town. Remarkably for a city of a quarter million people, Sioux Falls has a huge set of rapids rolling through the middle of town.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/devils_tower.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Matthew joined me on a trip west to Devils Tower. What a weird formation.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/nashville.gif&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;In Nashville, I athletically somersaulted onto a mechanical bull.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/arkansas_1.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I spent two nights in the Arkansas Ozarks. It’s beautiful hill country.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/oklahoma.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;To me, Oklahoma consists of Tiger King, some associations with panhandles, and this hybrid gas station/casino.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/kansas_2.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I stopped for lunch by a pond.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/nebraska.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;Nebraska allegedly has some cool buttes and bluffs off on its western side (actually, all the plains states seem to get cooler as you go west), but I skirted its eastern border, following the Missouri River and camping on the shore.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/badlands.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I love the Badlands. They’re so cool. I hadn’t been since I was a child, and I hadn’t remembered that these wonderful formations are just dirt, not rock. They’re so fragile — one determined person with a shovel could probably really change the view! In line with this, it’s a pretty geologically young place, with erosion starting only some 500k years ago.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;div class=&quot;hover-container&quot;&gt;
                &lt;img src=&quot;/img/states_roadtrip/north_dakota.jpg&quot; class=&quot;image&quot; /&gt;
                &lt;div class=&quot;overlay&quot;&gt;
                    &lt;div class=&quot;text&quot;&gt;I sent this picture to my brother and father after I told them I had some news. Yes, they were mad, and it was great.&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &lt;div class=&quot;col-2&quot;&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;some-lessons&quot;&gt;Some lessons&lt;/h2&gt;

&lt;p&gt;This trip afforded me ample time for reflection and coincided with a more extended personal journey I’ve been on for the past year in which I’ve been figuring out how to live as an adult in ways feel good. I learned a number of things about roadtripping, including that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it’s very useful to run a tight ship logistically, taking care of problems as soon as they arise and keeping gear quite organized. I never had any major logistical problems, which is unusual for me, and it’s because I gave problems my attention when they were still small.&lt;/li&gt;
  &lt;li&gt;Talking to strangers is a great joy of travel, and a good way to quickly get the feel of a place. Unplanned conversations were highlights of the trip. (See &lt;a href=&quot;&quot;&gt;Why I Talk To Strangers&lt;/a&gt;.)&lt;/li&gt;
  &lt;li&gt;Driving on snow is way harder than I expected.&lt;/li&gt;
  &lt;li&gt;The land is big, but it’s fathomably big. Taking a plane from coast to coast gives little sense of how much land there is since you really have no physical feeling of how fast you’re traveling. Driving across the heartlands gave me a real sense of how big the land is: if I drive for an hour or two or three, the character of the land will differ – hills turn to forest, forest turns to grassland, grassland to plains, to prairie, to badlands – and if I go for ten or twenty of these changes I’ll go from one side of the country to the other. America feels like a much more tangible thing to me now.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And, even more valuable, some broader life lessons and realizations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A career has to be fun to be fulfilling. No matter how important I feel science is, and how convinced I am that I could make a valuable contribution, it’s not worth doing if it’s not fun.&lt;/li&gt;
  &lt;li&gt;Various realizations about software engineering thanks to Matthew, including that it’s a skill I simply don’t have but could develop.&lt;/li&gt;
  &lt;li&gt;I’m kinda lonely! This particularly solitary endeavor clarified for me that &lt;em&gt;most&lt;/em&gt; of my endeavors are actually solitary, and I want more collaboration and closeness with the people around me.&lt;/li&gt;
  &lt;li&gt;Nature is good medicine, and I need more of it in my life.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- divider above footnotes --&gt;
&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:a&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is the route I actually followed, not the route I’d planned, which was more stripped down, going north from Sioux Falls to Fargo to finish. I added on the big excursion to go see nature in the western Dakotas halfway through, which turned out to be a great idea. &lt;a href=&quot;#fnref:a&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 14 Apr 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/fifty-states-roadtrip/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/fifty-states-roadtrip/</guid>
        
        
        <category>random, personal</category>
        
      </item>
    
      <item>
        <title>Why I talk to strangers</title>
        <description>&lt;p&gt;i met someone today&lt;br /&gt;
and in the course of conversation&lt;br /&gt;
told her about you&lt;br /&gt;
and your love of people&lt;br /&gt;
and what I learned&lt;br /&gt;
and how it changed me&lt;/p&gt;

&lt;p&gt;and she said:&lt;br /&gt;
that sounds good and all&lt;br /&gt;
but i don’t see the point&lt;br /&gt;
in connecting with someone&lt;br /&gt;
i will never see again.&lt;/p&gt;

&lt;p&gt;and so i thought for a moment&lt;br /&gt;
and i said&lt;br /&gt;
when you look at a flower&lt;br /&gt;
do you fret&lt;br /&gt;
that your examination is a waste of time&lt;br /&gt;
because you will never see that flower again?&lt;/p&gt;

&lt;p&gt;and in any case&lt;br /&gt;
the things you see in that flower&lt;br /&gt;
the details you notice in its petals, its leaves, its shape, its hue&lt;br /&gt;
you will surely see in other flowers&lt;br /&gt;
and to look at one is in some sense to look at them all&lt;br /&gt;
and teaches you to better see&lt;br /&gt;
each future flower.&lt;/p&gt;

&lt;p&gt;returning to people&lt;br /&gt;
all you see in a stranger&lt;br /&gt;
you will surely see again in countless others –&lt;br /&gt;
strangers, friends, and loved ones –&lt;br /&gt;
and learning about a stranger&lt;br /&gt;
is in fact learning about humanity,&lt;br /&gt;
each individual a view of the whole&lt;br /&gt;
and so infinitely more resplendent&lt;br /&gt;
if you know that secret.&lt;/p&gt;

&lt;p&gt;but it goes deeper&lt;br /&gt;
because unlike with flowers,&lt;br /&gt;
here you are that type of thing&lt;br /&gt;
and you can learn about yourself&lt;br /&gt;
in a subway chat with a stranger&lt;br /&gt;
whose name you do not know&lt;br /&gt;
and whose path you do not share.&lt;br /&gt;
you may see yourself reflected in their eyes,&lt;br /&gt;
see a life you might have lived,&lt;br /&gt;
or a joy you have forgotten,&lt;br /&gt;
or a struggle you have not known but will soon.&lt;br /&gt;
humans are not so different&lt;br /&gt;
and i see now that each gives a view of me&lt;br /&gt;
and that, i said,&lt;br /&gt;
is why i talk to strangers.&lt;/p&gt;

&lt;p&gt;our conversation ran its course&lt;br /&gt;
and we went our separate ways&lt;br /&gt;
drifting back to our separate lives&lt;br /&gt;
and i may well never see her again&lt;br /&gt;
but that, of course, isn’t the point&lt;br /&gt;
and i am glowing regardless&lt;br /&gt;
with the feeling&lt;br /&gt;
of having caught&lt;br /&gt;
a glimpse of the whole&lt;/p&gt;
</description>
        <pubDate>Fri, 15 Mar 2024 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/why-i-talk-to-strangers/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/why-i-talk-to-strangers/</guid>
        
        
        <category>poetry, personal</category>
        
      </item>
    
      <item>
        <title>Creating and erasing AI watermarks</title>
        <description>&lt;p&gt;I’ve been sitting in on many of the guest lectures for &lt;a href=&quot;https://rdi.berkeley.edu/understanding_llms/s24&quot;&gt;CS 294&lt;/a&gt;, “Understanding LLMs.” To my excitement, yesterday’s speaker was &lt;a href=&quot;https://www.boazbarak.org/&quot;&gt;Boaz Barak&lt;/a&gt;, a Harvard professor who studies theoretical CS and machine learning foundations and whose work and lectures I often enjoy. He spoke about LLM watermarking schemes, and in particular a scheme for breaking more or less arbitrary watermarks. In this post, I’ll give a brief overview of the problem and their solution and offer a synthesis that, to me, gives a succinct way to think about what’s going on here.&lt;/p&gt;

&lt;h2 id=&quot;ai-watermarking&quot;&gt;AI “watermarking”&lt;/h2&gt;

&lt;p&gt;There are many settings in which we might like to ensure that some artifact is &lt;em&gt;not&lt;/em&gt; generated by an AI model — for example, ensuring that photos came from cameras and not diffusion models or that a homework assignment was written by a student and not an LLM. As generative models get better, it’s going to continue to get harder to make this distinction. The problem statement here is basically: “how can we modify generative AI systems so that essentially all generated content can later be identified as AI-generated, in a way that (a) doesn’t make the model’s outputs significantly worse and (b) can’t be removed by a simple transformation of the output?”&lt;/p&gt;

&lt;p&gt;To get a sense for what the landscape looks like here, it’s helpful to imagine some solutions. One naive solution for watermarking LLMs in particular might be: have the LLM only output paragraphs whose characters hash to strings ending in, say, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;. This could be achieved by just regenerating each paragraph until it satisfies this condition. A page of text generated by this model is going to be easily identifiable as such — you just compute the hash of each paragraph and see only ending &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;s, which would be unlikely for any text that wasn’t generated under that constraint — but it’s also very fragile: to break this condition, it’s enough to change one word in each paragraph.&lt;/p&gt;

&lt;p&gt;Fortunately, there are better ideas: a handful of papers, including one by &lt;a href=&quot;https://arxiv.org/abs/2301.10226&quot;&gt;a team from UMD&lt;/a&gt;, propose that an LLM might instead write with a statistical bias that applies at every word, but which is randomized enough to be undetectable to a human — for example, using the most recent token to seed a RNG that splits the dictionary into a “good” and a “bad” half, and weakly biasing the model to sample the &lt;em&gt;next&lt;/em&gt; token from the “good” half. This results in human-readable text which can nonetheless be identified as AI-generated with high statistical power (by just running back the “good-list” algorithm), and this “watermark” can’t be removed without changing a &lt;em&gt;lot&lt;/em&gt; of the tokens.&lt;/p&gt;

&lt;h2 id=&quot;a-generic-black-box-attack-on-ai-watermarks&quot;&gt;A generic black-box attack on AI watermarks&lt;/h2&gt;

&lt;p&gt;A watermark on AI text is only good if it is hard to remove. In particular, there shouldn’t exist an “attack” process which (1) is easy to compute and (2) transforms “watermarked” text $T$ into “unwatermarked” text $T’$, where $T$ and $T’$ are equally good answers to the original prompt.&lt;/p&gt;

&lt;p&gt;Remarkably, Boaz + coauthors &lt;a href=&quot;https://arxiv.org/abs/2311.04378&quot;&gt;propose a generic attack satisfying (1,2)&lt;/a&gt;. I’ll quickly describe it before offering a synthesis.&lt;/p&gt;

&lt;p&gt;Their strategy is pretty simple: &lt;em&gt;just make random small changes to the text, and eventually you’ll have changed enough that any watermark will be destroyed.&lt;/em&gt; For example, if you replace every other word with a random synonym, that would be enough to scramble a watermark planted by the UMD team.&lt;/p&gt;

&lt;p&gt;…of course, swapping in &lt;em&gt;random synonyms&lt;/em&gt; would lead to terrible text, so the authors assume the attacker also has an “quality oracle” that can tell you whether you’ve made a swap that unintentionally made things worse. And of course swapping out single words is a bit limited, so sometimes you might want to rephrase bigger chunks of text, so the authors assume that instead of thesaurus, you’re using a “perturbation oracle” that gives random small modifications. In practice, both of these are language models: a powerful model to tell you if your text is worse, and a weak model to propose local modifications.&lt;/p&gt;

&lt;p&gt;Their main result is that, given both a quality oracle and a perturbation oracle, you can remove the watermark by &lt;em&gt;rejection sampling:&lt;/em&gt; at every step, use the perturbation oracle to generate a random modification, and then accept the change only if the quality oracle confirms that the change hasn’t made the overall text worse. This provably results in a &lt;em&gt;random walk around the space of high-quality responses,&lt;/em&gt; and since only a small fraction of these high-quality responses are “watermarked,” after you follow this random walk for a while, you’re going to reach the promised land of watermark-free essays for your English class. Here’s a beautiful figure from their paper illustrating this scheme:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/llm_watermarks/harvard_random_walk_attack.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;!-- &lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt;  &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt; --&gt;

&lt;p&gt;The authors test their attack against several proposed watermarking schemes and find that it consistently works with little degradation in text quality.&lt;/p&gt;

&lt;p&gt;I really like this attack. The fact that such a naive search algorithm — a random walk with rejection sampling — works for this is beautiful (and appeals to me as a trained physicist!). The scheme does a great job of reminding you that watermarked samples are only a small fraction of all samples — indeed, they have to be to be a good watermark — and this very fact makes them fragile! Rarity and nonrobustness are two edges of the same sword.&lt;/p&gt;

&lt;p&gt;The one weakness is that it requires a lot of calls to a quality oracle, which in practice will be a fairly powerful language model, so this isn’t a very cheap attack. But then, if you imagine a near-future world in which AI calls are easily available but all AIs are watermarked, this attack works perfectly well!&lt;/p&gt;

&lt;p&gt;In all honesty, it doesn’t seem to me like breaking watermarking schemes would be particularly hard. For example, it seems like it’d fool the UMD watermarking scheme to just, say, ask the model to write the essay in Spanish and Google Translate it into English, or to generate the essay with a filler word between every content word. I really like this attack, though, not because it does something I thought was impossible, but because it works very generally and shines light on the landscape of attack and defense here.&lt;/p&gt;

&lt;h2 id=&quot;synthesis-bootstrapping-a-discriminator-into-a-generator&quot;&gt;Synthesis: bootstrapping a discriminator into a generator&lt;/h2&gt;

&lt;p&gt;To me, the deep idea behind this attack is that a “quality oracle” can be hooked up to a random process and used to generate high-quality samples — or, in the language of GANs, a “discriminator” can be bootstrapped into a “generator.” A system that merely &lt;em&gt;scores&lt;/em&gt; samples is in some sense almost as powerful as a system that generates them! An AI organization might &lt;em&gt;think&lt;/em&gt; it’s safe giving you only watermarked text and a powerful discriminator, but in fact that discriminator is enough to effectively regenerate the text.&lt;/p&gt;

&lt;p&gt;When can a discriminator be cheaply bootstrapped into a generator? &lt;strong&gt;In some sense, this is what every local optimization algorithm is doing: iterated queries to a loss function (the oracle) are converted into a &lt;em&gt;point&lt;/em&gt; with a low loss value (the generated sample)!&lt;/strong&gt; This “iterated bootstrapping of discriminators” is a motif we should be on the lookout for in optimization and learning generally.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;thought-experiment-how-far-can-this-bootstrapping-idea-go&quot;&gt;Thought experiment: how far can this bootstrapping idea go?&lt;/h2&gt;

&lt;p&gt;This de-watermarking attack effectively works on the principle that &lt;em&gt;a discriminator plus a source of random perturbations can be used as a generator.&lt;/em&gt; I wonder how far we can take this idea!&lt;/p&gt;

&lt;p&gt;Here’s a thought experiment which this principle suggests, and which is sort of the de-watermarking attack taken to the extreme:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Choose a prompt from which we’d like to generate some high-quality text.&lt;/li&gt;
  &lt;li&gt;Start with &lt;strong&gt;&lt;em&gt;totally random text&lt;/em&gt;&lt;/strong&gt; — just random tokens.&lt;/li&gt;
  &lt;li&gt;Make random perturbations and run rejection sampling as in this attack for a long time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Question: do you reach high-quality text after many iterations?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In some idealized mathematical setting, the answer &lt;em&gt;has&lt;/em&gt; to be yes: quality is only increasing as per the oracle, so eventually it will either reach some local maximum or cross an arbitrary threshold. However, this is a real interesting question in practice thanks to the fact that every component is flawed! &lt;em&gt;Is the oracle reliable enough? Are the perturbations accepted at a good enough rate? Do we reach the promised land of high-quality text in a &lt;strong&gt;reasonable amount of time?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I don’t know the answer here, but it seems like it’d be a really cool result if it worked! In particular, I’m curious how dumb the source of perturbations can be in order for it to work. It’d also be really cool to see how the text changes as the process runs — what does it look like, say, 10% of the way through? 25%? 75%? Does it first form good English sentences, then gradually get on topic, then finally become high quality? If it gets stuck, at what point in this progression?&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I’m pretty curious about this! It actually seems like an experiment that could be run by some entity with a lot of compute.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In a meta sense, this is also fundamentally how reinforcement learning works: the learning algorithm converts task feedback scores (the oracle) into a system that can generate new good actions (generated samples). (This process is more expensive than schemes that generate only one sample, but once you’re done you have a system for cheaply generating lots of samples.) It’s not totally clear to me how to relate this meta-bootstrapping — aka learning — to the base-level bootstrapping that generates only one sample, as in this attack and optimization algorithms, but they sure feel suspiciously related to me. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The main reason to doubt that this thought experiment would work is the lack of a gradient here — we’re taking random steps, most of which will be neutral or a little bit harmful. I’m guessing this is fine for the original attack scheme because we already start at a good point, and because the perturbation oracle is designed to generate samples that tend to neither help nor hurt quality. (Incidentally, this is similar to some physics-inspired sampling schemes, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo&quot;&gt;Hamiltonian Monte Carlo&lt;/a&gt;, which use sampling strategies biased towards regions of similar loss to get faster exploration.) Can you improve text in this way instead of just paraphrasing it? I don’t know, but it’d sure be interesting if you could! &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 06 Mar 2024 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/llm-watermarking/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/llm-watermarking/</guid>
        
        
        <category>deep learning, research</category>
        
      </item>
    
      <item>
        <title>Reflections on introductory neuroscience reading</title>
        <description>&lt;p&gt;Have you ever lived in a neighborhood for years and realized, as you prepare to move out, that you never got to know the guy who lives next door? For me, that unknown neighbor is the field of neuroscience. I am nearing the end of a PhD in &lt;a href=&quot;https://deweeselab.com&quot;&gt;a lab that does largely neuroscience&lt;/a&gt; and live under the umbrella of the &lt;a href=&quot;https://redwood.berkeley.edu/&quot;&gt;Redwood Center for Theoretical Neuroscience&lt;/a&gt;, but I confess I’ve never engaged with the field in any serious way.&lt;/p&gt;

&lt;p&gt;As with that neighbor you don’t really know, I’ve often &lt;em&gt;seen&lt;/em&gt; neuroscience — I regularly pass by it on the sidewalk, so to speak, exchanging cordial pleasantries but never really engaging, always with something just a little more important to do. Look, I’m sure neuroscience is a &lt;em&gt;nice guy&lt;/em&gt; and all that, but we just don’t have that much in common! A lot of people seem to like him, but we’ve just never really clicked – and besides, isn’t he the one with all those rats? It’s hard for me to understand what he’s saying or why, so when we pass each other on the street, I usually just smile and nod and continue on my way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No more. I’ve decided to learn some neuroscience. These past weeks, I’ve been doing basic reading in an effort to absorb some of the big ideas. This post will summarize some of what I’ve learned.&lt;/strong&gt; If you come from a similar academic background and have a similar curiosity, perhaps you’ll find some of this interesting.&lt;/p&gt;

&lt;h3 id=&quot;why-would-i-learn-neuroscience&quot;&gt;Why would I learn neuroscience?&lt;/h3&gt;

&lt;p&gt;I’m doing this for two main reasons. The first is self-knowledge: I’m in a reflective period in which I aim to better understand myself, and I suspected that some basic neuro- and cognitive science might help me better understand my own experience — and indeed it has! Seeing the important ways in which our brains are hacky really gives me a sense of humility and a feeling that human experience is often different and much stranger than we conceive it to be, so we ought to really look at it. The second is more general: if you’ve never touched a field, there are often big foundational ideas sitting on the surface for you to learn, and so a fairly short period of learning can give outsize returns because you’re on the steep part of the learning curve. That also turned out to be true — I’d underestimated how much we know about the brain and how easily some of my naïve notions could be improved.&lt;/p&gt;

&lt;h3 id=&quot;plan-of-attack&quot;&gt;Plan of attack&lt;/h3&gt;

&lt;p&gt;I’m a physicist by training, and I like to think about general principles and big ideas, so over the past few weeks I read Sterling and Laughlin’s &lt;em&gt;Principles of Neural Design.&lt;/em&gt; This is an introductory book focused less on the specifics of anatomy and much more on broad principles which govern neural circuitry across brain regions and across species. This was a good match for the level of detail at which I wanted to learn things — I don’t need to know about, say, the superior temporal gyrus or the difference between norepinephrine and epinephrine, but I do want to know that neural circuitry aggressively tries to minimize wire length and energy consumption.  It gives a nice overview of how someone familiar with physical or systems thinking could begin to start thinking about the brain. I also read some of Kandel et al.’s &lt;em&gt;Principles of Neural Science&lt;/em&gt;,&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; a classic introductory text.&lt;/p&gt;

&lt;h1 id=&quot;some-learnings&quot;&gt;Some learnings&lt;/h1&gt;

&lt;p&gt;Without further ado, here’s a bunch of stuff I learned.&lt;/p&gt;

&lt;h3 id=&quot;the-brain-is-very-structurally-complicated&quot;&gt;The brain is very structurally complicated&lt;/h3&gt;

&lt;p&gt;Coming from machine learning, my naive picture of the brain was basically that it’s a big homogeneous mass of neurons initially connected in a mostly random fashion, with inputs to some regions and outputs from others, and that learning from experience leads to gradual strengthening and weakening of neural connections so that this mass of neurons eventually knows and learns.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/neuro/self_conception.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In reality, the brain is highly structured. This is actually pretty apparent from anatomy, even at a coarse level: the inside of the brain is whitish (containing mostly long insulated communication channels, or &lt;em&gt;white matter&lt;/em&gt;), while the outer few millimeters is greyish (containing lots of neurons with dense short-range connections, or &lt;em&gt;grey matter&lt;/em&gt;). Different regions of the brain have different textures, with peculiarly-shaped masses on the inside and in the hindbrain.&lt;/p&gt;

&lt;p&gt;In fact, even the cerebral cortex — the big wrinkly part that covers most of the outside of the brain — is in reality made up of lots of distinct regions which differ in their cellular structure! Over a century ago, Korbinian Brodmann made close examination of the cytostructure of the cortex and identified some &lt;a href=&quot;https://en.wikipedia.org/wiki/Brodmann_area&quot;&gt;52 distinct regions&lt;/a&gt; with different cellular composition and patterning. Some of these regions have since been found to be robustly responsible for distinct brain functions, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Auditory_cortex&quot;&gt;processing sound&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Postcentral_gyrus&quot;&gt;processing touch&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Broca%27s_area&quot;&gt;language production&lt;/a&gt;. The fact that these different regions of the homogenous-looking cortex are physiologically distinct and consistently perform different roles is a surprise to me!&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/neuro/brodmann_areas.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; The homogeneous-looking cortex is actually comprised of many regions of differing functions and cellular properties. Surprisingly to me, these regions do not follow the folds of the cortex! &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This complexity extends down to the level of individual neurons. There isn’t just one type of “neuron” — rather, the brain has tens to hundreds of different types of firing and support cells with very different geometries adapted to different roles and parts of the brain. On the small end, cerebellar &lt;a href=&quot;https://en.wikipedia.org/wiki/Granule_cell&quot;&gt;granule cells&lt;/a&gt; are only ~5 µm in size with axons of width on the order of only 200 nm (!). These connect directly with &lt;a href=&quot;https://en.wikipedia.org/wiki/Purkinje_cell&quot;&gt;Purkinje cells&lt;/a&gt;, which have cell bodies about 10x wider and dendritic arbors that spread out in a striking planar shape as far as several millimeters. The retina, which translates incoming light into neural signals, contains multiple layers of many distinct cell types, starting with specialized rod and cone cells and leading up to the optic nerve, which is essentially a cable-like bundle of about $10^6$ axons which extend for several &lt;em&gt;centimeters&lt;/em&gt; (!) from the eye to the visual cortex. The largest neurons in the body stretch from the spinal cord to the ends of the limbs and can be over a meter in length. Thinking of these all as “neurons” seems as reductive as referring to all components of a mechanical system or electronic circuit as just “components” — it’s not wrong, but it’s almost always more useful to work at a finer level of abstraction.&lt;/p&gt;

&lt;h4 id=&quot;why-are-things-so-complicated&quot;&gt;Why are things so complicated?&lt;/h4&gt;

&lt;p&gt;Here’s another amazing fact which will turn out to be related: in the nematode C. elegans, every individual has exactly 302 neurons, and they’re always in the same place and connected the same way.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This might seem shocking — after all, aren’t central nervous systems these flexible, adaptive systems that differ between individuals? It certainly surprises me until I remember that, well, the adult human body has exactly 206 bones and some 600 muscles, and they’re always in the same place and connected the same way. Biology is certainly capable of specifying this level of detail. Since the nematode doesn’t really need to learn, why &lt;em&gt;not&lt;/em&gt; just hard-code a rigid rule-based control system for its simple body?&lt;/p&gt;

&lt;p&gt;The important fact here is that the main role of a central nervous system is not to &lt;em&gt;learn,&lt;/em&gt; it is to dictate global actions and share information between different parts of an organism’s body necessary for doing so. Sterling and Laughlin illustrate this beautifully by pointing to the rudimentary chemical and electrical signaling of the paramecium — which is hard-coded and includes commands like “back up” and “turn” — as the ancestor of our own nervous systems. Learning in its various forms is sort of a remarkable recent development which allows a nervous system to adapt to its environment. In this light, it makes a lot of sense that the brain would be so complicated — it’s evolved from nervous systems like those of C. elegans, except instead of 302 hard-coded neurons, our brains have a few hundred hard-coded parts, and the fact that some learning can occur within each part is somewhat secondary to the overall control flow.&lt;/p&gt;

&lt;h3 id=&quot;the-brain-generally-obeys-a-handful-of-low-level-design-principles&quot;&gt;The brain generally obeys a handful of low-level design principles&lt;/h3&gt;

&lt;p&gt;What does it mean to “understand the brain”? Coming from machine learning theory, I’d held this notion to a high bar: surely &lt;em&gt;understanding&lt;/em&gt; in neuroscience would mean we know the precise encoding scheme for memories, or can give a mathematical model of human reasoning played out in neural firings, or concretely explain what happens neurologically when you imagine something. I hadn’t appreciated the degree to which these cognitive, experiential things are really just one level of the layer cake of neuroscience. There are in fact many other levels of abstraction which we understand fairly well!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Principles of Neural Design&lt;/em&gt; sets out to collect some low-level principles, and I found many of these quite compelling! Here are a few:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Compute with chemistry whenever possible.&lt;/strong&gt; Neural firing is energetically expensive — it’s much cheaper to transmit signals via releasing chemicals which travel diffusively! However, diffusive signaling spreads only as [distance] ~ [time]${}^{1/2}$, which ends up meaning it’s usually not a good choice when you need to go, say, more than a millimeter in one direction. Chemical computing tends also to require fewer parts and take up less space. The animal nervous system reliably uses chemical signal for either short-distance processing (e.g., across synapses or within cells) or slow global signal transmission (e.g., hormones in the bloodstream), and makes surprising use of proteins which change conformation in the presence of a ligand in order to perform basic computational operations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Send only what is needed.&lt;/strong&gt; Signal transmission is expensive — the brain uses almost 20% of the body’s energy, over half of which goes to neurons’ ion pumps! It’s therefore important to be economical in what’s sent — sending half as much information roughly cuts the energy cost in half. Across areas, the brain has robustly developed to only transmit necessary information and compress it as much as possible — often in ways that seem surprising given our experience of the world! The classic example of this is the visual system — we only have detailed vision in the central region of our field of view, and it’s shockingly low-res in the periphery — &lt;a href=&quot;https://www.scientificamerican.com/article/put-your-peripheral-vision-to-the-test/&quot;&gt;people regularly underestimate how bad their peripheral vision is&lt;/a&gt;. I honestly find it a bit annoying when I notice it, but it makes a lot of sense evolutionarily — the point of the visual system isn’t to provide our brain with a beautiful detailed image of the outside world, it’s to give us enough information to do survival and social tasks, and we can do these tasks quite well with only a small area of high-res vision that we scan around! This principle also applies to the postprocessing that occurs in the retina, which famously &lt;a href=&quot;http://www.scholarpedia.org/article/Sparse_coding&quot;&gt;compresses visual information by translating to a sparse basis&lt;/a&gt; before transmission through the optic nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Send at the lowest acceptable rate.&lt;/strong&gt; A cool fact I hadn’t known: sending a neural signal faster requires a thicker axon, which for biophysical reasons turns out to require superlinearly more resources and energy! The brain’s thus incentivized to send information as slowly as possible. This is basically true everywhere in the brain — I suppose it’s evolutionarily “easy” to tweak axons to be thinner and slower, so they’ll always tend to settle down to the slowest rate that works well enough. An amazing example of this is the speeds of different sensory modalities: olfaction (smell) has no need to be fast, so it uses a cable of $10^7$ very thin axons to send information quite slowly, while on the other end, the vestibular sense (balance) needs to send little information but needs to send it fast to keep us upright, so it uses far fewer axons which are about 100x cross-sectionally larger.&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://james-simon.github.io/img/neuro/axon_sizes.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There are, of course, plenty of exceptions here: the design of the vertebrate retina is famously silly, with processing circuitry partially blocking the photosensitive cells and the optic nerve creating a blind spot in each eye. I’m not sure how to square these cases with the observation that the brain often does a good job finding efficient design. Maybe the satisfaction of these principles is achieved via a kind of local gradient descent – e.g., by making small adjustments to axon thickness, neuron count, connectivity, and so on – and these weird cases reflect evolutionary “local minima” that are hard to escape?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Minimize wire.&lt;/strong&gt; Rather intuitively, longer axons take up more space, cost more energy, and slow signal transmission, so the brain tends to shorten wires as much as possible. This is achieved partially at the level of individual neurons, which tend to take shapes and choose branching points that reach all their connections in a local minimum of total distance.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; It is also achieved via organization of neural regions into maps — for example, the early visual cortex is arranged spatially in a 2D way that mimics the retina, which minimizes diagonal cross-wiring. Some brain areas have peculiar neural organization, like the aforementioned &lt;a href=&quot;https://www.snexplores.org/wp-content/uploads/2020/04/1030_LL_trees-1028x579.png&quot;&gt;fan-shaped Purkinje cells stacked in the cerebellum&lt;/a&gt;, and this organization allows for lots of dense connections in a small volume.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Complicate and specialize.&lt;/strong&gt; The cost of a neural design is &lt;em&gt;not&lt;/em&gt; in its complexity, it’s in the amount of resources it uses — that is, space, energy, time, and physical materials. If a neural circuit can be complicated in exchange for using less of one of these, it often will! As a result, there are a huge number of different neurons with different geometries, firing rates, sensitivities, and so on, which economize some resource. Every part of a signaling pathway in the brain is thus adapted to its place in the chain, and will adapt so it fulfills its role — transmitting with a particular fidelity, across a particular distance, and in a particular time — as cheaply as possible. An example I like: opsin molecules in rod cells in the eye are occasionally activated by random thermal noise… but they’re designed to be just robust enough that this noise level is just a few times below the activity rate when looking around in starlight.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;there-are-many-levels-at-which-we-might-want-to-understand-the-brain&quot;&gt;There are many levels at which we might want to “understand” the brain&lt;/h3&gt;

&lt;p&gt;As aforementioned, I hail from physics, where the bar for understanding is quite high: one expects a tight, testable, ideally-mathematical theory before one believes one understands a complex system. Applied to the brain, a physicist might want, say, a clean, elegant, mathematical theory for how high-level concepts are represented and manipulated in the brain before saying we understand what it’s doing, perhaps using notions of sparsity, information theory, high-dimensional geometry, and so on. This still seems like a reasonable-albeit-distant dream to me,&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; but there are many more levels at which we could understand the brain. Here are two:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low-level component design.&lt;/strong&gt; As discussed in the previous section, we can very much ask questions like: “given that this component performs this signaling task, why is it designed like this?” Questions like this are among the most answerable in neuroscience — appealing to efficiency principles seems to work pretty often! Of course, we often don’t know what task a component performs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Broad stories about high-level information processing.&lt;/strong&gt; I hadn’t appreciated the degree to which you don’t &lt;em&gt;need&lt;/em&gt; a mathematical description of learning in order to find pretty compelling stories about what different parts of the brain are doing. For example, we can tell loose stories like “when engaged in conversation, the auditory cortex preprocesses incoming sound, Wernicke’s area processes the sound as speech, and Broca’s area is responsible for speech production.” From the perspective of a physicist, this is an &lt;em&gt;incredibly vague&lt;/em&gt; story: what do you &lt;em&gt;mean&lt;/em&gt; by “processing the sound as speech”? What is &lt;em&gt;involved&lt;/em&gt; in speech production? And how on earth is this all encoded in a bunch of noisy neurons? These are very real questions, but the important realization for me is, well, you don’t need to know that for a lot of stuff. For example, if a patient’s Broca’s area is lesioned, they’ll be unable to produce speech correctly. If a patient’s Wernicke’s area is lesioned, they’ll be unable to understand speech but, remarkably, able to produce it. These areas light up under brain scanning when performing relevant tasks. This incredibly vague story seems to work, actually — well enough to inform medical interventions! We didn’t actually &lt;em&gt;need&lt;/em&gt; to know how the neural circuits work: vague, high-level stories are useful enough for some real understanding.&lt;/p&gt;

&lt;p&gt;It seems to me like most of our high-level brain knowledge is of this form: we have stories like “Part A is responsible for task X. Task X also requires input from Part B, so Parts A and B are wired together, and that wiring strengthens as one performs more of Task X.” We pretty rarely have a low-level understanding of how neural circuitry is computing, but we have stories like this for lots and lots of parts and tasks. This view is reminscent of &lt;a href=&quot;https://notes.andymatuschak.org/zQnTUMfm4YPLyi8GFA1Ym1e&quot;&gt;Marr’s three levels of abstraction&lt;/a&gt;, which proposes that computing systems can be studied at the level of the task ultimately performed (the highest level), the algorithm used to perform that task (the middle level), or the hardware used to implement that algorithm (the lowest level). Perhaps a good story here is that neuroscience has made major progress on the lowest and highest levels, but it’s still unclear how to connect them: we cannot extract the algorithms &lt;em&gt;performed by&lt;/em&gt; neural hardware which &lt;em&gt;implement&lt;/em&gt; the high-level functions which we know brain regions to perform.&lt;/p&gt;

&lt;p&gt;One interesting takeaway I glean from all this is that it now seems like the thing I purport to want — a simple mathematical description of learning — actually lies not at the highest level of abstraction but at an intermediate level, above the level of small circuits but below the level of brain regions. I’m also less confident that it’s really a good goal!&lt;/p&gt;

&lt;h3 id=&quot;how-do-we-learn&quot;&gt;How do we learn?&lt;/h3&gt;

&lt;p&gt;The nature of learning is still pretty unclear to me from my reading. It seems like there are a bunch of different mechanisms — synapses that have fired recently more readily fire again in the following seconds and minutes, synapses that have fired many times tend to increase their sensitivity, dopamine release (which is globally mediated) tends to reinforce neurons to do whatever they were just doing. I’m confused as to how to think about these mechanisms — is there a sharp difference between short-term and lon-term memory? Is there a difference between short-term memory and “what you’re thinking about right now”? Is most learning distributed and reward-signal-free, or is it top-down modulated as in machine learning? What tasks even count as “learning” — I could believe there are many more than the typical testing suite I envision! I feel I’ve gotten a bit of flavor for some of these learning mechanisms, but not enough to have any real picture of learning in the brain.&lt;/p&gt;

&lt;p&gt;This set of questions seems particularly interesting where it intersects with our everyday human experience. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;What’s going on neurologically when we forget things?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When you hear or use an usual word, you’re more likely to notice or use it again soon after. Is this explicable through some known neuro learning mechanism?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What’s the difference between factual learning and wisdom? Why does some learning feel like it affects our worldview, while other learning feels like just memorizing facts?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fun-facts&quot;&gt;Fun facts&lt;/h3&gt;

&lt;p&gt;Some disconnected fun facts from my reading:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The brain has a region called the &lt;em&gt;suprachiasmatic nucleus&lt;/em&gt; which takes in input from the retina regarding how generally light a scene is and serves as the body’s 24-hour clock.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neurons in the brain are outnumbered by glial cells (i.e. everything else) by a factor between 10 and 50.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The skull is physically full of brain, and synapses expand when they get stronger, so it’s often the case that practicing one skill causes part of the brain to physically grow, which causes other parts to shrink! This can have expected-but-still-alarming consequences in which learning one skill directly degrades performance at another — for example, learning to read decreases one’s ability to recognize faces! (I’m confused as to what to take away from this in light of &lt;a href=&quot;https://www.scientificamerican.com/article/what-is-the-memory-capacity/&quot;&gt;the common claim that the brain’s storage capacity is so large it’s virtually unlimited&lt;/a&gt;.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h1&gt;

&lt;p&gt;I set out to see if I could glean a high-level view of the field of neuroscience from some foundational background reading. I actually feel I’ve managed to do that to a modest degree! It was an endeavor well worth the time investment; I’d recommend it to others who are interested, and will probably do it again myself with different fields.&lt;/p&gt;

&lt;p&gt;In general, I’d say we actually understand more about neuroscience than I’d thought! While the things I thought were mysteries generally do seem to be unknown, I was blind to the many levels at which modern neuroscience examines the brain and thus to the very real progress we’ve made towards characterizing the brain’s functioning and relating it to both evolutionary pressures and our own human experience. I’m left quite impressed by the sheer amount of work required to get to this point — as an acquaintance recently told me, it takes a PhD’s worth of work to get one line in a textbook.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The most profound conceptual discoveries in the sciences — perhaps simply in general — often build bridges between two things that previously seemed to live in different worlds. My interest in neuroscience basically stems from a hope of this nature. Our lives as human beings take place largely in the world of our own internal processing: our cognition, sensation, and action; our feelings, reactions, and dreams. However, we have far more concrete, reliable understanding of the physical world of atoms and molecules, proteins and neurons, circuits and brains. I’m excited (both selfishly and altruistically) by the prospects for building bridges between these realms, starting to understand our own human experience in terms of basic science.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&quot;https://csinva.io/&quot;&gt;Chandan Singh&lt;/a&gt; and Mike DeWeese for feedback on this post!&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yes, these two books have confusingly similar names. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It took a protracted debate to arrive at the modern view of the cortex as composed of distinct regions which perform different elementary processing operations. Interestingly, this idea originated with &lt;a href=&quot;https://en.wikipedia.org/wiki/Phrenology&quot;&gt;phrenology&lt;/a&gt;, which in the early 1800s posited that the brain is comprised of some 30+ regions responsible for personality traits like “wit,” “religiosity,” “benevolence,” and so on. This framework is now understood to be totally wrong and based in little to no evidence, but the idea of a few dozen distinct brain regions was right - by accident as far as I can tell! After that came an era dominated by the “aggregate field view” that essentially held that the whole brain does everything, after which the modern “cellular connectionist” view took hold. Most of the early evidence for the modern view came from studying patients with certain cognitive or motor impairments and consistently finding damage in the same part of their brains. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Fun fact: C. elegans neurons also use analog signaling, not pulsatile signaling (aka “firing”). &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Tree branches and root systems do something like this too! &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In all honesty, I personally find this question so appealing and seductive that it’s distracting from more concrete problems. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;That said, it’s likely I’m too optimistic about understanding here given that I’ve just read a compendium of lots of things we understand. It seems likely that there are lots of things we don’t understand, even at the lowest and highest levels of abstraction: plenty of microscopic neural processes remain difficult or impossible to characterize with current tools, and there are surely lots of macroscopic brain areas about which we don’t have really compelling stories as to their function. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 02 Mar 2024 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/neuro-learnings/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/neuro-learnings/</guid>
        
        
        <category>neuroscience, research</category>
        
      </item>
    
  </channel>
</rss>
