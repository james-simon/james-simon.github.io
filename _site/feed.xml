<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>J S</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 01 Jun 2021 13:26:31 -0700</pubDate>
    <lastBuildDate>Tue, 01 Jun 2021 13:26:31 -0700</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Could you propel a spacecraft using sports projectiles?</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;In the fall of 2020, Ryan Roberts and I gave a remote talk for high schoolers through Berkeley Splash, our third exploring real physics through absurd scenarios instead of technical math. His talk discussed methods and consequences of making the Moon as big as the Earth, while mine aimed to find the best way to propel a spacecraft using only sports equipment. This is a writeup of the answer.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since the dawn of civilization, humans have gathered for sports competitions, with rounds of faceoffs ultimately whittling the athletes down to a sole champion. This time-honored tradition has flowered throughout history; the modern Olympic Games involve dozens of sports split into hundreds of events. This explosion of athletic diversity has made determining an ultimate champion a lot more complicated, though; wouldn’t it be nice if there were some way we could determine who, of all the gold medalists, is actually the best? What if there were some grand final competition among the event champions to determine an ultimate victor?&lt;/p&gt;

&lt;p&gt;For the consideration of the International Olympic Committee, I hereby propose the event of Rocketball as the Games’ final event. The rules are simple. To start, every gold medalist will be launched into Earth orbit with nothing but their wits, a spacesuit, and an absurd amount of equipment from their sport. They have no traditional rockets; to change course, the athletes have to hit, throw, fire, bowl, or otherwise propel sports equipment the other way, powered solely by their bodies. Instead of a goal line, there’s a target speed: whoever can accelerate themselves to reach that final speed first wins the Olympics&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This proposal has one&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; minor logistical problem: some sports are better than others. For example, it’d be way easier to accelerate by hitting tennis balls than by hitting ping-pong balls; a hit tennis ball is both heavier and faster, so (as we’ll see) as a rocket propellant it’s strictly better. Actually, in some cases, which sport is best can even depend on how fast the target speed is! To understand this, figure out how matches would play out, and see if we can come up with a fix, first we need to understand a little about how rockets work.&lt;/p&gt;

&lt;p&gt;At their core, all rockets operate on the same principle: they fling matter in one direction, and the recoil force pushes the rocket in the opposite direction. This recoil force is the same sort of pushback you’d feel when firing a gun, holding a fire hose, or doing a fast chest pass in basketball, but in a typical rocket the matter being launched is exhaust from burning fuel moving several kilometers per second. True to this principle, rockets have two main parts: the &lt;em&gt;fuel,&lt;/em&gt; which is gradually fired to propel the rocket, and the &lt;em&gt;payload,&lt;/em&gt; which is the important other bits that the fuel’s there to accelerate. The fuel section also usually includes engines that help burn the fuel but fall off when they’re no longer needed. Here’s a diagram of the Saturn V rocket split into the launch vehicle (fuel + engines) and spacecraft (payload).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/saturn_v.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Our sportsball rockets will work basically the same way, but instead of liquid fuel they carry sports equipment, and instead of the most powerful engines ever built, they’ll be powered by lone humans flinging objects into the vacuum of space.&lt;/p&gt;

&lt;p&gt;If you’re not familiar with rocketry or are very used to cars, you might wonder why the Saturn V has so much fuel for such a small payload. At some level, the reason’s that we want the payload to reach a very high speed (over 10 km/s), so we need a lot of fuel, but there’s another reason specific to rockets: &lt;em&gt;the earlier fuel has to accelerate all the later fuel, so you need more of it.&lt;/em&gt; If a competitor gradually hit away a million golf balls, the first ball would give a very small bump in speed because it has to push the other 999999 balls, while the last one would give a relatively big bump in speed because it’s only accelerating the athlete. A rocket’s acceleration is inversely proportional to its current mass. Because of this, it turns out that the amount of fuel you need is an exponential function of the final speed, and ending up slightly faster can potentially take many times more fuel.&lt;/p&gt;

&lt;p&gt;Rockets are complicated, but for a rocket accelerating in a straight line, it turns out we can find its speed over time if we know just a few numbers. These are the payload mass (which we’ll call \(m\)), the starting mass of the fuel and payload together (\(M\)), the relative speed the propellant’s fired at (\(u\)), and the thrust of the rocket (this is the average kickback force the rocket feels; we’ll call it \(F\)). Here’s a diagram illustrating what some of these are.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocket_math_diagram.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;If you do the rocket science, you find that to reach a final speed \(v\), the rocket needs a starting mass of \(M = m e^{v/u}\), and it runs out of fuel and reaches speed \(v\) at a time \(t = \frac{(M-m)u}{F}\). For a given target speed \(v\), this choice of \(M\) is optimal - with less fuel, it won’t reach the target speed, but with any more, it’ll reach it slower.&lt;/p&gt;

&lt;p&gt;We can use these formulae to figure out roughly what’ll happen in our intersport showdown. For a given sport, \(u\) is just the projectile speed (around 76 m/s for a golf ball and 4 m/s for a pool ball) and the thrust is equal to (projectile speed)\(\times\)(projectile mass)\(\times\)(fire rate). If we assume a constant fire rate of 1 shot/s across all sports for simplicity, we can get the speed and thrust for every sport. Here’s a plot showing the results.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_scatterplot.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;A quick look at this might reveal some oddities. Jai alai (also called Basque pelota) was once an Olympic sport; it’s like wallball but with a giant curved launcher on your throwing arm. Bowling and pool I’ve included even though they’re not Olympic sports. Crossbow shooting I included because our question is ultimately about projectiles launched solely with energy from human muscles, and it’s one of the best examples of that. For the same reason, I didn’t include gun sports. Lastly, running and swimming events don’t have projectiles, so I assumed their respective gold medalists will be throwing shoes and spitting mouthfuls of pool water&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This plot makes it clear that some sports are better than others, but it’s not apparent which is best. To decide, we’ll have to pick a target speed. Let’s say the athlete and their suit are 100 kg, and they’re trying to reach a final speed of 10 m/s. We’ll also assume that the athlete can repeatedly launch projectiles in the same direction without uncontrollably spinning or getting knocked off, but that seems fair; after all, they are professionals. About how long will each sport’s champion take to finish?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_10.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This plot shows that the best sport to reach 10 m/s is shot put. It’d actually only take 24 solid throws to finish, which is 24 seconds with our assumption of one shot per second. Most other sports take a few minutes; the swimmers will be spitting into the void for several days.&lt;/p&gt;

&lt;p&gt;What if we instead want to reach 100 m/s?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_100.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now the best option is to accelerate with 2053 jai alai throws, which takes a little over half an hour. Due to the exponential in our time formula, however, accelerating with pool balls will now take over a hundred million years. Surprisingly, most high-energy projectile sports are all relatively close, with a time less than a few hours. Actually, at a target speed of 130 m/s, the entire swath from crossbow to javelin is about even! This, I propose, is the ideal choice for Rocketball; the sports without high-energy projectiles can get a handicap.&lt;/p&gt;

&lt;p&gt;What if we make the target 1000 m/s?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/rocketball_ts_1000.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;With this high target speed, the \(e^{v/u}\) term is so dominant that the thrust is basically irrelevant compared to the exhaust speed. The best option now is to use 35 million crossbow bolts, which will take around a year. This is one of the few situations where your best choice is to use a bow in space. All the sports slower than javelin will take longer than the current age of the universe.&lt;/p&gt;

&lt;p&gt;This is a decent analysis for the purposes of Rocketball, but these speeds are miniscule on the scale of the Solar system. What if we want to reach 10000 m/s, roughly Earth escape velocity? As you might guess, exhaust speed is even more dominant here, and crossbow bolts are still the best choice. However, you’d need \(10^{43}\) of them. The good news is that they’d hold together under their own gravity, so you wouldn’t need a quiver. The bad news is that the arrow ball would have a bigger diameter than the orbit of Saturn and would immediately collapse into a black hole.&lt;/p&gt;

&lt;p&gt;There is, however, a sport we’ve overlooked. The tires of a bike contain air under high pressure, and if there’s a hole or open valve, that pressure will accelerate the escaping air to high velocities. Normal bike valves are narrow and slow down the escaping air a lot, but if you designed a much bigger one (or just punched a hole in the tire) it’d come out much faster - a 50 psi tire would, in the ideal case, accelerate air to 760 m/s! It turns out that you could reach 10000 m/s by ideally releasing enough 50-psi air to fill a sphere with a diameter of 270 m.&lt;/p&gt;

&lt;p&gt;How long would that take? By our constraint of only man-powered projectiles, you’d have to pump up the air. This is about 47 billion bike tires’ worth of air, so it’d take around a millenium even if you’re the &lt;a href=&quot;https://www.guinnessworldrecords.com/world-records/419351-fastest-time-to-pump-a-bicycle-tyre&quot;&gt;world’s fastest tire pumper&lt;/a&gt;. The release, however, would be quick.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/rocketball/air_jetpack.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;and gets to come back to Earth. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(this is a lower bound) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Spacesuit helmets would admittedly block the spit-out pool water, but since humans can survive zero pressure for short periods and swimmers are already good at not breathing, let’s say they take their helmets off to spit. Another option’s to throw chunks of ice, which would fall between “shoe throw” and “shotput.” &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 29 Nov 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/physics/2020/11/29/rocketball.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics/2020/11/29/rocketball.html</guid>
        
        
        <category>physics</category>
        
      </item>
    
      <item>
        <title>The principle of least power dissipation</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;I’m currently in a class called Physics of Computation on new types of computing paradigms that don’t solve problems in discrete, sequential, logical steps like modern computers.  Instead, the whole computing system goes through a complex analog evolution, with every variable free to change simultaneously, and eventually settles into a stable final state representing the answer.  Naturally, one of the core challenges is in designing a system that predictably settles into a steady state with some desired properties.&lt;/p&gt;

&lt;p&gt;Our professor’s pointed out that many such computing systems can be seen to accomplish this by using a little-known concept called the ‘principle of least energy dissipation.’  When a battery or current source is hooked up to a DC circuit, a sudden flow of current flares through the different branches of the circuit, sometimes much more or less at first than is sustainable.  Eventually, though, these initial errors fade away, and the circuit ends up in a steady state, with currents and voltages everywhere that don’t change in time.  The principle of least energy claims that &lt;strong&gt;of all the possible steady states satisfying the voltage and current constraints, the real steady state dissipates the least amount of power.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’ve solved tons of DC circuits, and I’d never heard of this.  To my shock, though, it worked for a few simple circuits we discussed in class.  However, these simple examples neither showed it always works or explained why it does.  To try to understand, I read &lt;a href=&quot;https://www.feynmanlectures.caltech.edu/II_19.html#footnote_1&quot;&gt;Feynman’s lecture on minimization principles&lt;/a&gt;; it turns out that at the end he mentions this law but doesn’t really explain it, so I decided to fill in the gap.&lt;/p&gt;

&lt;p&gt;It’s actually easier to see that it’s true if you start with a general, continuous 3D system; everything’s captured in one differential equation instead of having to deal with discrete voltages and resistors.  Suppose we have a volume \(V\) filled with a resistive medium with resistivity \(\rho(\mathbf{r})\).  It could have different resistances at different points; that’ll be important later.  Now, suppose there’s an electric potential (a voltage) \(\phi(\mathbf{r})\) throughout the medium.  We also need some way to add a constraint to the system (like plugging a source into a circuit), so we’ll suppose that the potential at the surface is completely fixed; perhaps it’s higher at one end and lower at the other to cause a current flow.  In real circuits, currents cause magnetic fields that can affect charges a little, but we’ll ignore magnetism for this problem.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/electrostatic_setup.png&quot; width=&quot;20%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The currents and voltages throughout a resistive circuit are determined entirely by the sources and the circuit structure, so we expect that \(\phi\) and the current should be determined entrely by boundary conditions and \(\rho\).  How do we find them?  Well, the typical way is to say that the potential corresponds to an electric field \(\mathbf{E}(\mathbf{r}) = - \mathbf{\nabla}\phi\) which then creates a current \(\mathbf{J} = \rho^{-1} \mathbf{E}\) according to the continuous version of Ohm’s Law.  In the steady state, charge isn’t building up anywhere, which implies that \(\mathbf{\nabla} \cdot \mathbf{J} = -\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\).  Solving this to match the boundary conditions on \(\phi\) gives the potential, which gives the current.  Note that this would reduce to \(\nabla^2 \phi = 0\), which you might recognize as the equation voltage satisfies when there’s zero charge density, if \(\rho\) were a constant; the reason it doesn’t is that, even in steady-state DC circuits, there’s actually built-up charge on the interfaces between components of different resistivities.&lt;/p&gt;

&lt;p&gt;Let’s see if we can derive that differential equation not by assuming that \(\mathbf{\nabla} \cdot \mathbf{J} = 0\) but just by assuming that the power dissipation is as small as possible.  The total power dissipation is given by \(P = \int_V (\mathbf{J} \cdot \mathbf{E}) \mathop{}\!\mathrm{ \mathbf{dr} } = \int_V \rho^{-1} (\mathbf{\nabla}\phi)^2 \mathop{}\!\mathrm{ \mathbf{dr} }\).  Feynman’s lecture neatly explains how to find minimal solutions, so I’ll just tear through the steps quickly.  Since we’re looking for a minimum, we want to find a choice of \(\phi\) that makes \(P\) invariant to perturbations to first-order.  If we add a small perturbation \(\delta\phi\) to \(\phi\), so the new potential field is now \(\phi + \delta\phi\), the power dissipation becomes \(P + \delta P\), where \(P\) is the same as before and \(\delta P = 2 \int_V \rho^{-1} \mathbf{\nabla}\phi \cdot \mathbf{\nabla}(\delta\phi) \mathop{}\!\mathrm{ \mathbf{dr} }\).  The usual integration by parts trick gives \(\delta P = -2 \int_V \mathbf{\nabla} \cdot \big[ \rho^{-1} \mathbf{\nabla}\phi \big] \delta\phi \mathop{}\!\mathrm{ \mathbf{dr} }\), and since \(\delta\phi\) was arbitrary, this implies that \(-\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\)!  That’s it - minimizing power dissipated is equivalent to the same steady-state differential equation you get from working through the electrodynamics the normal way.  Any \(\phi\) that minimizes power dissipation is a real physical solution, and any physical solution minimizes power dissipation.&lt;/p&gt;

&lt;p&gt;The math is all well and good, but how can we really &lt;strong&gt;understand&lt;/strong&gt; it?  What’s the physical reason why power should be minimized?  Well, the differential equation we derived from power minimization is just the fact that the current \(\mathbf{J}\) has zero divergence.  That actually makes sense in terms of minimizing power dissipation!  Imagine a point in the medium with nonzero current divergence, so it’s a sink (or a source) of current.  In the vicinity of that point, on top of whatever divergence-free flow there is, there’s an extra part of the current that flows into or out of the point.  In a rough sense, it turns out that you can decrease the dissipated power by just dropping that extra part of the current.&lt;/p&gt;

&lt;p&gt;That’s the case of a problem where the voltage is prescribed.  Circuits can have both voltage and current sources, though; what about a case where the current on the boundary is given?  Well, first, it’s important to note that only the current &lt;em&gt;normal to the boundary&lt;/em&gt; can be prescribed; to illustrate why, if you could prescribe arbitrary currents on the boundary, you could have the current flow in a circle around the boundary, which we know is already unphysical.  We’ll also have to assume that \(\mathbf{\nabla} \cdot \mathbf{J} = 0\) - if we just let charge accumulate in the medium, the minimum power dissipation solution will just be to have zero current everywhere and have charge build up forever on the surface.  However, if we again minimize power dissipated using vector calculus tricks, we get a new, interesting condition: \(\mathbf{\nabla} \times (\rho\mathbf{J}) = 0\); the curl of \(\rho \mathbf{J}\) is zero.  Since curl-free vector fields are gradients of potentials, this gives us the condition that \(\mathbf{J} = -\rho^{-1}\mathbf{\nabla}\phi\) for some potential \(\phi\), and since we assumed the current has zero divergence, this gives us \(-\mathbf{\nabla} \cdot (\rho^{-1} \mathbf{\nabla} \phi) = 0\)!  Again, the new condition that minimizing power dissipation bought us actually makes physical sense; if the curl of \(\rho \mathbf{J} = \mathbf{E}\) weren’t zero, there would be current flowing uselessly around in loops and dissipating extra power.&lt;/p&gt;

&lt;p&gt;Alright, now we’ve decently explained why power dissipation is minimized in scenarios when current’s flowing through a continuous resistive medium.  We were originally interested in circuits, though; how does this relate?  We can actually cleverly fashion a resistive circuit by just changing \(\rho(\mathbf{r})\)!  For most of the volume, we’ll let \(\rho \rightarrow \infty\), which turns it into a very good insulator, like the air between the wires in a circuit.  Then we’ll let \(\rho \rightarrow 0\) in some long, thin regions that will become wires.  Along the wires we’ll make some regions have intermediate values of \(\rho\), which makes them resistors.  Lastly, if we want, we can choose our boundary so all of space &lt;em&gt;except&lt;/em&gt; where a source will go is enclosed in it, then specify the voltage or current along the boundary.  This gives a circuit of sources, wires and resistors that might look as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/resistive_circuit.png&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now we have a circuit.  How does minimal power dissipation explain its current and voltage distribution?  From the point of view of choosing voltages at each node of the circuit, any voltage distribution besides the true one will lead to charges accumulating somewhere, which requires some extra current and dissipates extra power.  From the point of view of choosing how currents branch at every node, any split besides the true one will correspond to extra current running around in a loop, which wastes power, and also an electric field with a nonzero curl, which doesn’t correspond to a potential.&lt;/p&gt;

&lt;p&gt;We’ve shown that the principle of least power dissipation gives the right answers for steady-state circuit problems.  If this is really a general law of nature, though, it’d probably show up outside of just this one narrow context.  It turns out that it can also be applied to &lt;em&gt;fluid circuits&lt;/em&gt;, which are sometimes studied as somewhat-similar sister systems to electric circuits.  In the sort of fluid circuit I’m imagining, there are networks of pipes of different thicknesses instead of resistors with different resistances.  Water flow rate \(Q\) replaces current, pressure \(P\) replaces voltage, the rate of power dissipation is \(PQ\) instead of \(VI\), and pipes have an effective resistance.  An ideal pipe with laminar flow has \(\Delta P\) proportional to \(Q\) in &lt;a href=&quot;https://en.wikipedia.org/wiki/Hagen%E2%80%93Poiseuille_equation&quot;&gt;an equation analogous to Ohm’s Law&lt;/a&gt;, and the quantity analogous to resistance depends on the pipe’s dimensions and the viscosity of the fluid.  The fact that these equations map onto each other so perfectly means that fluid through a network of pipes minimizes power dissipation just as electrical circuits do.  I’d be willing to bet that, just as we derived part of the differential equation for current flow, you could derive part of the Navier-Stokes equations just from minimizing power dissipation.&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Sep 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/physics/2020/09/06/least_power_dissipation.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics/2020/09/06/least_power_dissipation.html</guid>
        
        
        <category>physics</category>
        
      </item>
    
      <item>
        <title>Multiplicative neural networks</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the past decade, neural networks have proven to be stunningly effective tools for a slew of machine learning tasks as diverse as classifying images, generating 3D graphics, and playing games.  These incredible successes all stem from neural networks’ remarkable ability to perform a central task in machine intelligence: given complex data, find underlying patterns.&lt;/p&gt;

&lt;p&gt;Boiled down to their core, neural networks are just a series of &lt;em&gt;hierarchical nonlinear transformations&lt;/em&gt;.  Each layer of the neural network is one layer in this hierarchy, transforming a feature vector and passing it to the next layer up, until it reaches the output.  Each layer usually involves a step that mixes the elements of the feature vector according to a set of weights followed by a nonlinear function applied to each element of the vector.  All types of neural networks use a series of these layers to do the basic information processing.&lt;/p&gt;

&lt;p&gt;Within that hierarchical nonlinear structure, however, there are many variations.  There are a vast array of different ways layers can be connected and parameterized, giving different architectures like convolutional nets, recurrent nets, and resnets specialized to certain types of task.  However, there are also lower-level things that can be changed.  One example is the nonlinear function, usually denoted \(\sigma\).  The field’s explored many options for \(\sigma\) over the years, and the most popular choice has changed over time.  The \(\tanh\) function (and similar sigmoid) were once the leading choice, but the \(\text{ReLU}\) (defined by \(\sigma(x) = \max(x, 0)\)) is now more popular.  Choosing a different nonlinearity can improve performance slightly, but to me it is more striking that the choice of nonlinearity matters so &lt;em&gt;little&lt;/em&gt;.  On the face of it, the \(\text{ReLU}\) and \(\tanh\) functions are basically as different as they could be within reason.  \(\tanh\) is smooth, saturating, nonlinear on every finite set, and is symmetric about the origin.  \(\text{ReLU}\) has a kink, is unbounded, piecewise-linear, and never takes negative values, not to mention that it outputs zero on half of its domain.  The fact that both of these functions could work reasonably well speaks to the fact that even though the hierarchical nonlinear structure is core to the effectiveness of neural nets, the exact choice of nonlinearity is, surprisingly, not crucial.&lt;/p&gt;

&lt;p&gt;In fact, even the typical layer structure of matrix multiplication + elementwise nonlinearity isn’t crucial to performance.  It has been shown that tensor networks, a completely different type of hierarchical nonlinear model commonly used in physics, can also learn machine learning tasks.  &lt;a href=&quot;https://arxiv.org/abs/1605.05775&quot;&gt;This paper&lt;/a&gt; develops the idea and gets 1% test error on MNIST, which is pretty good for a totally new approach.  Interestingly, neural networks have recently been used instead of tensor networks in computational physics problems and achieved good performance there.  This raises a natural question: how do we know that the matrix multiplication + elementwise nonlinearity structure is really the best one?  To my knowledge, there’s no known fundamental, theoretical reason why it would be better than other options.&lt;/p&gt;

&lt;h2 id=&quot;multiplicative-nets&quot;&gt;Multiplicative nets&lt;/h2&gt;

&lt;p&gt;Here’s an idea for a different type of nonlinear hierarchical model.  What if we took a neural network and replaced the matrix multiplication step \(\sum_j w_{ij} x_j\) with a product step \(\prod_j x_j ^ {w_{ij}}\), with the weights as exponents in a product instead of coefficients in a sum?  This would still mix together the elements of the feature vector \(x\), just like a matrix operation, but in a different, nonlinear way.  Given the way deep learning systems can sometimes be surprisingly invariant to details of implementation, maybe this new, different sort of model could still work well.&lt;/p&gt;

&lt;p&gt;Architectures using this idea are called &lt;strong&gt;multiplicative neural networks&lt;/strong&gt;.  The idea was &lt;a href=&quot;https://dl.acm.org/doi/10.1162/neco.1989.1.1.133&quot;&gt;first proposed in 1989&lt;/a&gt; by neuroscientists seeing multiplication as more biologically plausible and potentially more powerful than addition.  They were &lt;a href=&quot;https://clgiles.ist.psu.edu/papers/NIPS94.product.units.pdf&quot;&gt;tested experimentally&lt;/a&gt; on some very small problems in the next decade and once &lt;a href=&quot;https://sci2s.ugr.es/keel/pdf/specific/articulo/Schmidtt%20on-the-complexity-of.pdf&quot;&gt;analyzed from a complexity standpoint&lt;/a&gt; in the decade after.  The architectures these papers describe are a little more general - they consider combining additive and multiplicative neurons in the same net and using hybrid terms with more weights of the form \(v_1 \prod_j x_j ^ {w_{ij1}} + v_2 \prod_j x_j ^ {w_{ij2}} + ... + v_n \prod_j x_j ^ {w_{ijn}}\) - but their key innovation is the multiplicative form.  However, despite this research, multiplicative nets never broke through to practical use; we still use additive neural networks with normal matrix operations.&lt;/p&gt;

&lt;p&gt;This is pretty understandable - at a first glance, multiplicative nets seem like they’d have some problems.  One immediate question is what to do when \(x_i &amp;lt; 0\) and \(w_{ij}\) isn’t an integer, in which case \(x_i ^ {w_{ij}}\) isn’t  uniquely defined, but to avoid that problem let’s just set things up so that all the \(x_i\) are positive, which I’ll show we can do with the right choice of data and nonlinearity.  Even dealing with this, though, these nets have some pretty prolific potential practical problems.  First, there’s been a lot of research and hardware development into making matrix operations faster, and this wouldn’t use them.  Second, replacing simple matrix operations with something more complex could lead to weird gradients and failed optimization.  Third, as a friend noted, even from a pure computational complexity standpoint, calculating \(x^y\) is more expensive than calculating \(x*y\), so this would be slow.  Finally, the advantages of switching to this architecture are far from clear, so there’s no incentive to overcome these problems.&lt;/p&gt;

&lt;p&gt;The purpose of this post is to explore something interesting that isn’t discussed in the above papers: &lt;strong&gt;the multiplicative nets you get by replacing \(\sum_j w_{ij} x_j\) with \(\prod_j x_j ^ {w_{ij}}\) are basically identical to normal, additive neural nets with a different choice of nonlinearity.&lt;/strong&gt;  The basic reason is that, as the 1989 paper did note, \(\prod_j x_j ^ {w_{ij}} = \exp \big( \sum_j w_{ij} \ln(x_j) \big)\), so a multiplicative layer is the same as an additive layer where you take the logarithm before and an exponential after.  The main trick we’ll use is the fact that you can then absorb the \(\ln\) and \(\exp\) into the nonlinearity to get a new nonlinearity \(\sigma'(x) = \exp\sigma(\ln(x)))\), or, with compositional notation, \(\sigma' = \exp \circ \sigma \circ \ln\).&lt;/p&gt;

&lt;p&gt;My argument could easily be laid out with standard symbolic math.  However, that’s not how I think about it, and I don’t think it’s the clearest way to explain it.  Notation matters; different ways of writing the same thing inspire different kinds of manipulations, and cleaner notations are better to learn with.  Instead of algebra, I’ll use diagrams.&lt;/p&gt;

&lt;p&gt;Basic neural nets are built of a few basic pieces.  There are &lt;em&gt;input nodes&lt;/em&gt;, where input data vectors go into the net.  There are &lt;em&gt;output nodes&lt;/em&gt;, where the net’s output comes out.  There are elementwise &lt;em&gt;nonlinear functions&lt;/em&gt;, mapping real numbers to real numbers.  There are &lt;em&gt;additive&lt;/em&gt; or &lt;em&gt;matrix operations&lt;/em&gt;, each with its own 2D array of parameters \(W\), which in normal neural nets come between applications of nonlinear functions.  Lastly, we’ll include &lt;em&gt;multiplicative operations&lt;/em&gt;, which also each have their own 2D array array of parameters &lt;span style=&quot;color:purple&quot;&gt;\(W\)&lt;/span&gt; as discussed above.  We’ll use color to differentiate between additive and multiplicative operations; additive will be black, multiplicative will be purple.  Note that changing an operation from additive to multiplicative or vice versa doesn’t change its parameter matrix; the same numerical weights are just interpreted differently in a different operation.  We could also add pieces like biases or specialized layers like softmax or maxpooling, but we’ll leave those out for now for simplicity.  We could cast these pieces into diagrammatic form as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_components.png&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We can now build neural nets out of these components.  For example, the following is a standard additive neural net with 3-dimensional input, 3-dimensional output, two hidden layers with width 3, and a nonlinearity \(\sigma\).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_example.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There are a few different ways we can manipulate these diagrams while still keeping the model the same.  The first way is &lt;em&gt;function composition&lt;/em&gt;.  If there are two nonlinear functions in a row, we can just replace them with one new function.  This is just saying that the double function application \(f(g(x))\) is equivalent to the single function application \(h(x)\), defining \(h = f \circ g\).  One special, familiar case is when the two functions are each other’s inverses, and the new function is simply the identity; for example, \(\ln(\exp(x)) = x\).  Instead of writing the identity function explicitly, we can just write a line.  Note that in the notation \(f \circ g\), functions are applied right to left, but in our diagramas, everything flows from left to right, so the ordering of the composed functions might seem backwards at first.  There’s also the multiplicative-additive net identity we talked about earlier: \(\prod_j x_j ^ {w_{ij}} = \exp \big( \sum_j w_{ij} \ln(x_j) \big)\).  A multiplicative layer is the same as an additive layer with logarithms before and exponentials afterwards.  We can write these rules like this:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/net_diagram_rules.png&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;These rules lay out ways we can manipulate diagrams.  As a test of this notation, with just these simple rules we can derive a companion rule to the multiplicative-additive identity.  Just as you can take a function of both sides of an algebraic equation, we’re allowed to modify a diagrammatic equation by attaching pieces to dangling ends of both diagrams in the same way.  In this case, we’ll attach exponentials on all dangling ends on the left and logarithms on the right of both diagrams.  This gives us a new diagrammatic equation.  We can then use function composition to simplify the adjacent logarithms and exponentials to get a new, simple identity.  This one tells us that \(\ln \big( \prod_j  \exp(x_j)^{w_{ij}} \big) = \sum_j w_{ij} x_j\), which you can algebraically check is true.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/manipulation_example.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Now, let’s put together a multiplicative net and see what we can derive.  Our starting point will be the same simple 3-3-3-3 net as before, but with multiplicative layers; it will be clear that our operations will generalize to different sizes.  First, we use the multiplicative-additive identity to get an additive net.  However, instead of just having one nonlinear function acting on each element of the feature vector, there are now three in succession.  Using function composition, we can just group these into a new nonlinearity we define as \(\sigma' = \ln \circ \sigma \circ \exp\).  We now arrive at an additive net &lt;em&gt;exactly equivalent to the multiplicative net&lt;/em&gt;.  The only major oddity of the new additive net is elementwise logarithms at the start and exponentials at the end.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/multiplicative_net_transformation.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;These logarithms and exponentials at the start and end aren’t surprising.  We’re requiring that multiplicative nets need positive input and give positive output, so since logarithms of negative numbers aren’t real, these logarithms enforce the positivity of the input.  The exponentials similarly enforce the positivity of the output.  Also, as long as \(\sigma\) maps positive numbers to positive numbers, the net only operates on positive numbers intermediately, and we don’t have the undefined power problem.  We can make our diagram even simpler by absorbing the logarithms and exponentials into the inputs as follows:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/end_absorption.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;It turns out that a multiplicative net is basically the same as an additive net &lt;em&gt;with the same weights!&lt;/em&gt;  Our choice of notation emphasizes the similarity of form.  However, is this really a meaningful, useful correspondence?  The main question is this: what’s the nonlinearity \(\sigma'\) like?  It’s possible that \(\sigma' = \ln \circ \sigma \circ \exp\) is some bizarre function that’d be totally nonfunctional in an additive net, which would bode badly for our multiplicative net.  To answer this, a few examples are plotted below.  The blue curves show different choices of nonlinearity for multiplicative nets, defined only for positive inputs, and the orange curves show the corresponding additive nonlinearities.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/net_diagrams/nonlinearities.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As shown in (a), when \(\sigma\) is the \(\tanh\) function, \(\sigma'\) looks a lot like a smoothed \(\text{ReLU}\), also called a softplus!  It’s flipped across the x- and y-axes, but that doesn’t change the usefulness of a nonlinearity in an additive neural net.  This would definitely work as an activation function.  Surprisingly, (b) shows that when \(\sigma\) is a sigmoid, \(\sigma'\) is also roughly a sigmoid (but again flipped across both axes), which also works as a nonlinearity.  Shown in (c) is the case when the multiplicative net uses a \(\text{ReLU}\) nonlinearity.  This is a distinctly horrendous choice for only positive inputs, since it’s the same as the identity on positive input, which is reflected in the fact that \(\sigma'(x) = x\).  A multiplicative neural net with \(\text{ReLU}\) nonlinearities is basically linear; adding extra layers doesn’t give it any power since, for additive networks, a multilayer linear net is reducible to a linear model.  With a slight modification, however, we get a very useful nonlinearity.  As shown in (d), if the nonlinear net uses a modified \(\text{ReLU}\) - specifically, \(\sigma(x) = \max(x, 1)\) - the corresponding linear net has exactly \(\text{ReLU}\) activations.&lt;/p&gt;

&lt;p&gt;It might not be clear what, exactly, this means for multiplicative nets.  Let’s suppose that we’re using a multiplicative neural net to do a task - say, classifying images - and we operate it by exponentiating the pixel values at input and taking the log of the output.  We have shown that, for a reasonable nonlinearity, the function the multiplicative net computes as a function of weights and data - say, \(f(X ; W)\) - is exactly the same as for a reasonable additive net.  That means that the loss surface is the same.  That means that the gradients are the same, and that means that the trainability is the same, and all this means that the theoretical usefulness of multiplicative neural nets, including their representational power and optimization behavior, are the same as the standard additive neural nets that have been the focus of such intense study.  Multiplicative nets are just as effective, there’s a correspondence to additive nets, and we have the conversion algorithm.&lt;/p&gt;

&lt;p&gt;This fact is intriguing, but what does it mean in a broader sense?  This doesn’t make multiplicative nets practically useful; it’s still faster to perform a matrix multiplication than a multiplicative step, and since they amount to the same thing there’s no reason to prefer the latter.  To me, the reason this fact bears consideration is the hint it might be towards what truly gives deep learning its unreasonable effectiveness.  Multiplicative and additive nets have starkly different functional forms, and at a first glance I’d guess they have wholly different behavior as models.  The fact that they actually have equivalent power and utility seems like a hint that the fundamental magic of deep learning doesn’t lie in the details of implementation, like the choice of activation function or even the choice of matrix multiplication for mixing; the latter’s just preferred for convenience.  It seems likely that the key to deep learning’s success is something more deep and general about hierarchical nonlinear structures, and wildly disparate hierarchical models, from standard neural nets to multiplicative nets to tensor networks, all succeed because in some deep way they all fit this broad category.  Perhaps efforts to understand deep learning will eventually uncover a mathematical understanding of something like this.&lt;/p&gt;

&lt;h2 id=&quot;extras&quot;&gt;Extras&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Wait,” you might say, “there’s no clean correspondence to an additive net if the multiplicative net has a more complicated form, like involving a sum of product terms.  This analysis only works in one case when it happens to be trivial!”  It’s true that the exact correspondence is easy to break by adding a bell or a whistle to the multiplicative net.  The question, however, is whether these modifications add anything fundamentally different, or whether they’d be incremental changes at most.  I can only conjecture for now - maybe I’ll do experiments if this becomes a paper - but I’d guess it’s likely that there are many ways to superficially change the form of multiplicative nets without fundamentally changing their behavior because the same is true of additive nets.  For example, you can add extra parameters to neural nets by parameterizing the activation functions themselves; &lt;a href=&quot;https://arxiv.org/pdf/2006.03179.pdf&quot;&gt;this paper&lt;/a&gt; not only tried that, but ran an evolutionary algorithm to build complicated activation functions, and their error rates were only a few percent lower than with simple \(\text{ReLU}\)s.  Many such engineering intricacies lead to no big change in performance.  For that reason, I think it’s a decent hypothesis that multiplicative nets behave similarly to additive nets even when there’s no exact correspondence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;In the diagrams I drew, the nets have only weights, not biases.  Since \(\exp \big( \sum_j w_{ij} \ln( x_j ) + b_j \big) = e^{b_j} \prod_j x_j ^ {w_{ij}}\), we can add them by multiplying the product by \(e^{b_j}\), where \(b_j\) is a new bias parameter.&lt;/li&gt;
  &lt;li&gt;The choice of base \(e\) (i.e. \(\exp\) and \(\ln\)) in this article was arbitrary, and any other base would’ve worked.&lt;/li&gt;
  &lt;li&gt;Many types of specialized neural net layer can also be translated into multiplicative nets.  For example, a softmax is similar, but it doesn’t even require taking the exponentals.  \(\exp\) and \(\ln\) are monotonic, so they commute through max operations, so maxpool layers are the same for multiplicate nets.&lt;/li&gt;
  &lt;li&gt;Are there other choices for the mixing operation besides the matrices of additive nets and the multiplication operation shown here?  One way to generalize the concept to include both is to make the mixing transform \(f \big( \sum_j w_{ij} f^{-1}(x_j) \big)\), with \(f\) an invertible function mapping reals to reals.  In the case of \(f = \text{identity}\), this gives additive nets.  In the case of \(f = \exp\), this gives multiplicative nets.  In a case like \(f(x) = x^3\), though, it would give something new, but still equivalent to an additive net by the same sort of proof.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sci2s.ugr.es/keel/pdf/specific/articulo/Schmidtt%20on-the-complexity-of.pdf&quot;&gt;The most recent paper I’ve found on these nets&lt;/a&gt;, which was mentioned above, argues that multiplicative neurons perform operations that need large numbers of summing units, stating that “networks of summing units do not seem to be an adequate substitute for genuine multiplicative units.”  This post basically disproves that!&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 31 Aug 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/deep%20learning/2020/08/31/multiplicative-neural-nets.html</link>
        <guid isPermaLink="true">http://localhost:4000/deep%20learning/2020/08/31/multiplicative-neural-nets.html</guid>
        
        
        <category>deep learning</category>
        
      </item>
    
      <item>
        <title>How would an upside-down candle burn?</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;As every child knows, sometimes it’s fun to do something exactly wrong.  In that spirit, what would happen if you burned a candle upside-down?&lt;/p&gt;

&lt;p&gt;To take a guess, we need to understand how candles work.  Their simplicity is deceptive; burning candles are actually pretty complex systems with many interacting parts and processes.  They illustrate a slew of different chemical and physical effects so elegantly that the famous physicist Michael Faraday gave a &lt;a href=&quot;https://www.youtube.com/watch?v=RrHnLXMTOWM&quot;&gt;series of lectures&lt;/a&gt; on candles using them to demonstrate effects from from turbulence to buoyancy to blackbody radiation and to explain the chemistry of fire.  Candle flames are such common sights that we often overlook their complicated anatomy; look at one closely and you’re likely to see a number of features, from the blue glow at the bottom to the tapering of the tip, that everybody recognizes but few people could actually explain if asked.&lt;/p&gt;

&lt;p&gt;Explaining all of candle physics would take a textbook, but for this question, we only need to think about things that change when the candle is turned upside down, which simplifies the problem a lot.  The most obvious is that the heat of the flame, normally drawn up and away because hot air’s less dense than cold air, will flow directly into the tip of the candle.  Under normal operation, the tip of the candle wax is just hot enough to melt a small pool of wax but not hot enough to boil it, putting it at between 50 °C and 370 °C.  By contrast, the spot one wick-length above the tip of the wick is in the core of the flame, with a temperature in the vicinity of 1000 °C.  If you could some how turn just the flame upside-down without turning the rest of the candle, the flame would melt the wax a lot.&lt;/p&gt;

&lt;p&gt;However, candles are cyclic systems with a lot of feedback - the flame melts the wax which feeds the flame and so on.  We’ve guessed how the change in the flame caused by flipping the candle might affect the wax, but how would that change in the wax then affect the flame in turn?&lt;/p&gt;

&lt;p&gt;The flame’s shape would obviously change since the hot gas would be deflected as it rises by the block of wax in its way.  How would the burning itself change, though?  An important fact is that in candles, the wax only combusts as a vapor.  It starts as a solid, is melted and drawn up into the wick and into the heart of the flame by capillary action, and is vaporized by the high heat.  It’s only then that combusion happens - the hot, vaporized wax molecules react with oxygen and give off heat, perpetuating the process.  In the upside-down candle, more wax would be in the region of high heat, so there would be more hot surface area for vapor to escape from.  This means that wax would vaporize and burn at a higher rate, which, all else being equal, should give a bigger flame.&lt;/p&gt;

&lt;p&gt;However, there’s a problem with this situation.  This augmented flame would melt a lot of wax; what if it melts wax faster than it can vaporize it?  In a normal candle, there’s a small cup in the solid wax that holds the melted wax, but in the upside-down candle, the cup’s turned over.  All the excess wax might run and fall off the candle unburnt, so the candle would both go through wax faster and drip a lot.  There’s a stranger possibility, though.  What if the flame melts wax that floods down over the wick too thickly for the flame to vaporize it in time and snuffs it out?  Like Icarus, it’d die from melting wax and its own zeal.  This was my guess for what would happen to the upside-down candle.&lt;/p&gt;

&lt;p&gt;I also asked friends to guess how an upside-down candle would burn and got around 10 responses.  Most people either gave the self-snuffing scenario - let’s call it scenario (1) - or the second, self-sustaining scenario - say, (2).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/candle_scenarios.png&quot; width=&quot;30%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;To answer this question, I dangled a candle, set up another for comparison, and lit them.  &lt;a href=&quot;https://www.youtube.com/watch?v=tJv4IOTvCWY&quot;&gt;A video of the result is here.&lt;/a&gt;  Forgive the subpar quality.&lt;/p&gt;

&lt;p&gt;This matched scenario (1) - the candle initially burns, but once it starts to melt wax it’s quickly put out.  Out of curiosity, though, I ran another trial with the same candle.  The result was wholly different, matching scenario (2).  It turns out that the same candle can follow either path, probably depending on the initial shape of the tip and wick.  The phase-space of this upside-down candle has two basins of attraction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XKAtwxw7SWk&quot;&gt;Here’s a video of the second trial.&lt;/a&gt;  Most people were right at least once.  Nobody, however, predicted the stalactite.&lt;/p&gt;

</description>
        <pubDate>Thu, 20 Aug 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/physics/2020/08/20/upside-down-candle.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics/2020/08/20/upside-down-candle.html</guid>
        
        
        <category>physics</category>
        
      </item>
    
      <item>
        <title>Messing with the postal service</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;The United States’ postal system is an institution of renown, a conduit for objects and information older than the Constitution.  Even now, amid financial difficulties and numerous competing ways to send things, it’s still remarkably effective - neither snow nor hail nor remote destinations nor heavy loads fazes the US’ army of postal workers.  This raises a natural question: can we find something that does?&lt;/p&gt;

&lt;p&gt;Out of related curiosity, over years I’ve mailed a slew of things I’d guess were new to postmen.  These included a letter with a hole in the middle (arrived), a pentagonal letter (arrived), an origami crane (arrived), a pack of sticky notes with an instruction on the back for any postal worker reading it to take one note (arrived missing many), a dollar bill addressed like a postcard (returned, in an envelope, due to not having an envelope), a note in an envelope made of dollar bills (arrived), and a literal stick with an address written on it (arrived in a bag with an apology for any damages).  A piece of crumbling bark, two letters tied together, and a spherical rock the size of a golf ball weren’t successfully delivered.&lt;/p&gt;

&lt;p&gt;Why’d some of these objects arrive while others didn’t?  The USPS’ regulations and pricing seem to assume that anything sent is either a letter or in a large envelope or box, but I haven’t found anything explicitly forbidding sending things without boxes.  The lack of explicit rules probably means postal workers have a lot of leeway in deciding what to do with weird items, which makes sense and fits the data - I don’t see a sharp line that separates the successes from the failures above, but the failed items do seem a little more obnoxious.&lt;/p&gt;

&lt;p&gt;There are an infinite number of ways to mess with the postal service, though, and alone I’ve tried only a handful.  The theme’s worth some more variations.  I once led a group of children at a summer camp in mailing random objects, but only so much is accessible in the middle of the woods.  At the end of the spring 2020 semester, I and 11 other members of my physics cohort formed a group to mail each other weird things in the style of a secret Santa.  The group, dubbed the University Society of Postal Shenanigans (USPS), finished its first round in June.  Here are our results, ordered subjectively by the mailed object’s ridiculousness.&lt;/p&gt;

&lt;h4 id=&quot;halloween-decoration&quot;&gt;Halloween decoration&lt;/h4&gt;
&lt;p&gt;Delivered after several weeks&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/ween.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;mint-box&quot;&gt;Mint box&lt;/h4&gt;
&lt;p&gt;Not accepted via mailbox delivery and rejected at a post office by a persnickety employee&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/mints.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;hand-sanitizer&quot;&gt;Hand sanitizer&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/sanitizer.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;bonne-maman-jam-lid&quot;&gt;Bonne Maman jam lid&lt;/h4&gt;
&lt;p&gt;Not delivered&lt;/p&gt;

&lt;h4 id=&quot;anticannibalism-bracelet&quot;&gt;Anticannibalism bracelet&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/strap.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;glove&quot;&gt;Glove&lt;/h4&gt;
&lt;p&gt;Not delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/glove.jpg&quot; width=&quot;300px&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/gloveback.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;lemon-from-my-backyard-with-address-and-urgent-written-on-it-in-sharpie-mailed-within-berkeley&quot;&gt;Lemon from my backyard with address and URGENT written on it in Sharpie mailed within Berkeley&lt;/h4&gt;
&lt;p&gt;Delivered in a few days&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/lemon.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;lemon-coincidentally-picked-from-another-bay-area-backyard-and-mailed-to-berkeley&quot;&gt;Lemon coincidentally picked from another Bay area backyard and mailed to Berkeley&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/lemon2.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;origami-box-containing-three-origami-cranes&quot;&gt;Origami box containing three origami cranes&lt;/h4&gt;
&lt;p&gt;Delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/origami_box.jpg&quot; width=&quot;300px&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/origami_crane.jpg&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;lone-stamp-with-address-and-odd-message-on-back&quot;&gt;Lone stamp with address and odd message on back&lt;/h4&gt;
&lt;p&gt;Not delivered&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/stamp_front.jpg&quot; height=&quot;350px&quot; /&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/stamp_back.jpg&quot; height=&quot;350px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;chicago-flag-socks-with-an-address-and-stamps-sewn-into-one&quot;&gt;Chicago flag socks with an address and stamps sewn into one&lt;/h4&gt;
&lt;p&gt;Delivered (credit to Madeline Bernstein for the quality item)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/sock.png&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;3d-ball-maze&quot;&gt;3D ball maze&lt;/h4&gt;
&lt;p&gt;Delivered despite having too little postage.  As the postman collected perhaps the best $2.55 I’ve ever spent, he made it clear with his manner that he’d never seen something like this and wasn’t going to miss the chance to make fun of it&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://james-simon.github.io/img/usps/maze.png&quot; width=&quot;300px&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;dehydrated-celery-stalk-partially-wrapped-in-duct-tape&quot;&gt;Dehydrated celery stalk partially wrapped in duct tape&lt;/h4&gt;
&lt;p&gt;Not Delivered&lt;/p&gt;

&lt;p&gt;There are a few clear conclusions we can draw from these experiments.  First, the sheer probability of the postal service delivering something weird is shockingly high.  Second, they’re more likely to deliver something when there’s no way to give it back to the sender - the mint box was refusable, and my single dollar had a return address, and both failed, while almost nothing else risky had a return address.  Third, though object ridiculousness wasn’t very correlated with success, the apparent value of objects was - the ball maze, socks, and hand sanitizer all have some obvious use, while the celery, jam lid, and single glove don’t.  A natural combination of these conclusions is that the postal service will probably deliver anything weird they can’t return as long as it seems important enough.  I’d bet you could mail basically anything - a seat cushion, a balloon, a constantly-beeping speaker - if you permanently attached it to something that looks like a bill with no return address.&lt;/p&gt;

&lt;p&gt;The USPS’ experiments will continue, but I’d love to hear from anyone else who’s tried pushing the envelope of the postal system.  Please tell me of your experiments by writing a message on the exterior of a full-size replica of a blue public mailbox and mailing it to the Berkeley physics department.&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Jul 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/mail/2020/07/12/mail-shenanigans.html</link>
        <guid isPermaLink="true">http://localhost:4000/mail/2020/07/12/mail-shenanigans.html</guid>
        
        
        <category>mail</category>
        
      </item>
    
      <item>
        <title>Common ground</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;America’s polarization is increasingly dire, and with national focus perpetually on divisive issues, both liberals and conservatives often see the other as genuine enemies with whom cooperation’s impossible. There’s a severe lack of sincere dialogue between the left and the right, but that fact makes honest, humanizing conversations with the opposition more precious now than ever. To that end, and to prove that common ground could be found, I, quite liberal, messaged a friend who’s quite conservative for a discussion. Though we began firing off opposing views on divisive topics, with a little thought we found much unexpected agreement and eventually reached a reasonably shared view of pressing problems. Here’s an outline of everything we agreed on, a map of some oft-undiscussed common ground most arguing liberals and conservatives might not realize they both stand on. My hope’s that it might serve as a blueprint for others’ conversations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Police brutality is wrong and should be punished&lt;/li&gt;
  &lt;li&gt;Racism exists in 2020 and is a problem&lt;/li&gt;
  &lt;li&gt;From Hollywood to academia to boardrooms, there are many sectors of society in which whiteness is implicitly the default, and even in 2020 this often leads to minorities feeling like outsiders due to their race&lt;/li&gt;
  &lt;li&gt;There are many policies from redlining to the War on Drugs to school funding mechanisms that disproportionately hurt poor communities, which are often largely black or hispanic&lt;/li&gt;
  &lt;li&gt;Peaceful protesting is commendable and should never be met with violence&lt;/li&gt;
  &lt;li&gt;The word ‘racist’ has extreme connotations, and it’s usually more productive to call someone’s implicit racism ‘ignorance’ or ‘insensitivity’&lt;/li&gt;
  &lt;li&gt;Police obviously do some good, and we shouldn’t rush to abolish all police&lt;/li&gt;
  &lt;li&gt;The slogan ‘fuck the police’ is unproductive&lt;/li&gt;
  &lt;li&gt;Issues often get politicized when they don’t need to be&lt;/li&gt;
  &lt;li&gt;Be careful not to share fake news&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Donating to groups like Equal Opportunity Schools, Children of Promise, and Wings for Kids is a clear, apolitical way to make a difference&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Police brutality isn’t the doing of a small fraction of ‘bad cops’ among the ‘good cops’; what’re the odds four for four of George Floyd’s attackers happened to be ‘bad’? They’re questions of group psychology and police culture. Real solutions should involve changing training, increasing police accountability and oversight, reversing unnecessary police militarization, and resolving the noted problem of using police to address too wide a variety of needs. The four-for-four probability argument is one I feel liberals could make more clearly against the ‘few bad cops’ statement.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Media severely biases people’s perceptions of police and protester violence. I’d seen a headline about a pregnant woman hit in the stomach by a rubber bullet, while she’d seen a story about protesters who’d refused to let a car with a sick child through. Given that there were protests in every major city in the US, it’s no shock things like these happened, but because they’re emotionally potent, it’s easy to get riled up when a news outlet or FB algorithm shows you one. It’s more important to focus on broad trends and statistics when possible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Much of racial inequality is due to racist history and cycles of poverty as opposed to current racism. She thought the national focus should be on investing in poor black communities to fix problems at their roots instead of pinning guilt on police and current racism. While plenty of studies have shown proof of current racism and that fact should be broadly understood, and police abuse of power is prevalent and merits reform, I basically agree; police reform alone won’t fix racial inequality. The question of police blame has become divisive, with conservatives largely defending the police; how’d that happen when there are clearly problems with law enforcement? Maybe it’s because police brutality is more glaring and simpler to address than broad socioeconomic issues, so it’s an easy target for both reform efforts and anger, making it look like the dominant liberal view is that police are the main problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rioting and looting shouldn’t be excused. Our society considers crimes less severe if they’re done in the heat of passion, but they’re still crimes, and though the recent riots are perhaps more understandable because of their motivation and the emotion they’ve brought to the nation’s attention, they’re still highly damaging and often selfish. Excusing these completely is a minority view among liberals but still seems to cause a lot of understandable frustration among conservatives who see it as a mainstream view, which detracts from discussion of more important issues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Liberals can be very intolerant of conservatives. I’d noticed this but was shocked at the extent my friend described; in her words, “if I shared a [BLM] post but shared the portions I disagree with I’d be called out for not agreeing wholeheartedly, but if I’m silent I’m called out for being silent.” “I fear that putting a conservative sticker will almost definitely lead to vandalism on my car in Richmond.” “I know conservatives who have to put up with liberal rants all day at work and can’t say a thing or else they’ll lose their work relationships and promotion opportunities or even their job.” That sort of thing’s much less obvious from a liberal perspective, so I, for one, believe her. There’s definitely a perception among some liberals that some ideas are so clearly right that anyone with an objection or caveat is automatically morally inferior, with no need for further investigation, and though I agree with most of said ideas, that attitude’s a problem and being aware of it might help give liberals some understanding of conservatives’ perspectives.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When a prominent leader makes a comment that’s widely interpreted as racist, if it wasn’t meant that way, that leader should be clear in explaining the intended meaning and disavowing the racist interpretation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;It’s worth listening to each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When factions face off, spears bristling, perhaps the boldest, most important place to stand is in the middle with hands extended to either side. Polarization in the US is as real a crisis as the coronavirus, but unlike the virus, you have significant power to fight it directly.&lt;/p&gt;

</description>
        <pubDate>Sat, 04 Jul 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/society/2020/07/04/common-ground.html</link>
        <guid isPermaLink="true">http://localhost:4000/society/2020/07/04/common-ground.html</guid>
        
        
        <category>society</category>
        
      </item>
    
      <item>
        <title>What would happen if you made a planet out of fish?</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;In the fall of 2019, Ryan Roberts and I gave a talk to high schoolers at Berkeley Splash on analyzing absurd scenarios with real physics.  His talk elucidated what would happen if you put an egg inside the beampipe of the Large Hadron Collider, but I wanted to answer a deep question even a small child could ask.  My talk was on that age-old conundrum: what would happen if you made a planet entirely out of fish?&lt;/p&gt;

&lt;p&gt;Part of the beauty of physics is that, if you have the right rules, you can consider absurd scenarios far beyond the realm of possibility and figure out &lt;em&gt;what would actually happen&lt;/em&gt;. In the history of physics, this has led to major breakthroughs; Einstein was famous for using imagined scenarios to derive new physics. Our goal here is less lofty: we want to know what would happen if you put enough fish together in space to make a planet.&lt;/p&gt;

&lt;p&gt;First, let’s imagine how this will happen. Our fish of choice will be the haddock (&lt;em&gt;Melanogrammus aeglefinus&lt;/em&gt;), a 10-kg-max fish well-adapted to the waters of the North Atlantic and poorly adapted to the vacuum of space. Real solar systems start as rotating disks of gas, with a star forming in the center and planets gradually condensing out of the matter in the disk. Let’s just remove a young star’s accretion disk and replace it with an equal mass of live haddock.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://lh3.googleusercontent.com/UPX6xTzmv-5uKYMGUgktYwdS36AJWZoLeQOoIOvbsAPPW-5sFaCBgU6af30GjaXDuwUqFh5Z5u5PI09i_J-giIqtAqIljJRZ3BYexq41VyC0iYYWrSaSNLcmNpYiZE_RMzNfn03M&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The first thing that’ll happen is that the fish will get much bigger. In a vacuum there’s no air to push against the internal pressure of the fish, so they’ll expand, as happened to dogs in &lt;a href=&quot;https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19660005052.pdf&quot;&gt;disturbing experiments in the 60s&lt;/a&gt;. The water in parts of the fish would also boil, not because it’s hot, but because it’s at a very low pressure. To understand why that happens, you can imagine that even with atmospheric-pressure water, occasionally little bubbles of vapor form inside it, but the pressure of the water compresses them back into nothing, the vapor’s reabsorbed, and the bubbles go away. If that water is in a zero-pressure environment, however, that opposing force is gone, and nothing prevents bubbles from forming, so it boils. The escaping vapor might then condense into tiny ice crystals depending on how far from the star this is happening.&lt;/p&gt;

&lt;p&gt;We now have a proto-solar-system of dead haddock orbiting a star, bathed in water vapor and ice crystals. Wonderful! Now what happens? As the cloud of fish orbits the star, fish will occasionally hit each other. Usually they’ll bounce off, but with each collision a small bit of kinetic energy will be lost, and the fish will have more similar velocities after the collision than before it. Eventually, fish will collide so gently that they’ll stick together due to their tiny gravitational attraction. Small blobs of fish will collide to form bigger blobs, while the biggest blobs pick up lots of individual fish and small blobs as they move through the cloud, like a combination of the games Katamari Damacy and Feeding Frenzy. Eventually, most of the fish will be concentrated in big asteroid-like blobs. Since astronomers like to name everything, let’s do the same; let’s focus on one fish-blob and call it &lt;strong&gt;Blobfish&lt;/strong&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://lh6.googleusercontent.com/H0JSdunlzk8eyowWVPB_N6MJEskml4JKeZ4n7NFR7HH0G9utLHMM4QBYMyJy4e0RRaz51BjvNl3Ts9YaUrncev3xnFCkdXr147JY3fKquHrf0K2E7KEQ_hUzW8NUa6OQLab_khbE&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The size of Blobfish depends on how many fish it contains; more precisely, the radius scales as \(N^{1/3}\), where N is the number of fish in it. If we have, say, a million fish, we’ll get a fish asteroid with a radius of about 10 meters. Depending on its orbit, this fish asteroid might eventually pass near the star it’s orbiting, trailing dust and gas behind it, and it could also be ejected from the star system if it passes too close to a much bigger body, but at that size there isn’t much more it would do that’s interesting. We’ll need to add more fish.
What about a trillion? With \(10^{12}\) haddock, we’d have a radius of a little over 1 km. Though much larger, this is still just a fish asteroid. Let’s jump up a lot. With \(10^{23}\) haddock - around one for every molecule of water in a cubic centimeter - we have a planet that’s a little like Earth. It has a radius of around 6000 km, and enough mass to be comparable to the rocky planets in our solar system. Let’s call it &lt;strong&gt;Rockfish&lt;/strong&gt;. We now have an astronomical body made entirely of fish!
Well, at least they started as fish. The inside of a planet after its formation can be very hot; the main reason is that the kinetic energy that the proto-planets lost in each collision while forming the planet was turned into heat. Because the fish attract each other gravitationally, there’s a lot of gravitational potential energy that’s now present as heat. The energy balance works out the same as if we’d built up this planet entirely by dropping meteors on it, which shows that each bit of mass should have enough thermal energy to move very fast.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://lh3.googleusercontent.com/W1s4UmENLaan6tI0RPEHeOWgeGrzlQ_bcyyfZbCTSzaIUczb1Y2YnhaAWdkdzbu60CNv83dWXQu7y6HsnCHAKexGXLVzQSKqD0MtwqqFwPJ_V2SPTEMC6Bzm-Yj3KlIRWAQRW_bb&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;How hot is Rockfish? If you form a planet with a mass M and a radius R by bringing together small bits of mass initially spread very far apart, you get a total potential energy loss of \(\frac{3M^2G}{5R}\), which we’ll assume gets turned into heat. For a rough approximation, let’s assume that our fish planet has the density and heat capacity of water. The resulting temperature scales as \(N^{2/3}\), so a 1 km asteroid would have negligible heat of much less than a degree, but Rockfish would have a temperature 1500 K, around the melting point of iron and definitely hot enough to do serious damage to a fish. Now we have enough mass and thermal energy for new things to happen on our planet.
In the intense heat, the fish are quickly reduced to their basic chemical constituents: the elements composing our fish will be mostly free to rearrange themselves to minimize their energy. People haven’t studied the basic chemical constituents of haddock closely, but the chemistry of humans is well-studied and pretty similar, geologically speaking. By mass, the human body is roughly 65% oxygen, 20% carbon, and 10% hydrogen, with at least 0.1% of nitrogen, calcium, phosphorous, potassium, sulfur, sodium, chlorine, and magnesium, with traces of many other elements. Let’s assume the fish are the same - they’ll have higher concentrations of chlorine, magnesium, and sodium from saltwater, but we won’t be precise enough for that to matter. The question of what happens to our planet is now mostly a question of geology and chemistry.
Planets naturally form layers of different densities, with the heaviest elements (iron and nickel in the Earth’s case) sinking to the planet’s core. Let’s give this newborn planet a few million years to separate into layers, then analyze it in layers from the core upwards.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://lh3.googleusercontent.com/l-5cT5tXLPu5FgHlSymTrj3mNRaWqr9tmtQTvwRbyrR2mNPQpiap7zPYD5_a1kipyH9zIqoAheKPNOiJZb3xjkAqoIKlAwJREh42BvbEomBPXDnad6quTSKStsV7Y52Ddkv1t-Ug&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;At the center of Rockfish, there would be a small inner core of the dense trace metals, like iron and mercury, found in fish. However, the densest thing likely to form in large quantities is actually pure carbon; graphite has a density roughly 2.3 times that of water, while the densest of the major metallic components is magnesium, which is about 1.5 times as dense as water when liquid. The core of this planet would be mostly carbon, and that carbon’s under very high pressure - the pressure at the core of a planet is proportional to M2/R4, which is again proportional to \(N^{2/3}\), so as we’ve added more fish that pressure has gotten large, reaching 50000 times the pressure of Earth’s atmosphere. Actually, at these temperatures and pressures, carbon is most stable as diamond! It turns out that this planet could easily have a core of solid diamond, which is probably its coolest feature.&lt;/p&gt;

&lt;p&gt;Just outside the carbon core, we’d find a layer containing calcium, potassium, magnesium, and other metals denser than water but less dense than carbon. It might also contain oxygen like the Earth’s mantle. This layer would be much thinner than the Earth’s outer core, but it might be similar in one key way. The Earth’s magnetic field is caused by a complicated, poorly-understood process called the geodynamo, in which a flowing, conducting fluid like Earth’s liquid Fe-Ni outer core generates a magnetic field in a feedback loop. Our molten metal layer could do the same; it’s entirely plausible that this planet would have a magnetic field.
Above that, there would be massive oceans. Water is most of the mass of a fish, so there would be enormous salty oceans with a depth of thousands of kilometers. If this planet ends up with an Earth-like moon, the tides would be massive. There would likely be undersea vents on the ocean floor. Above the ocean there’d be an atmosphere with tons of water and oxygen, possibly also with carbon dioxide and gases like methane that could be released by fish in the original accretion disk. Due to the water content in the atmosphere, there would be dense clouds and a strong greenhouse effect.&lt;/p&gt;

&lt;p&gt;What about life? Fish are good fertilizer: by definition, this planet contains abundances of all the necessary chemical ingredients for life, and it likely has a magnetic field to shield it from ionizing radiation. If it’s orbiting a star within the right distance range, there’s no reason this mass graveyard of fish couldn’t be a birthplace of life. Actually, it turns out some bacteria can survive in the vacuum of space, so maybe some bacteria would be dormant in frozen fish or form small ecosystems on fish asteroids near the star, and then seed life on Rockfish. That’s total speculation on my part, but there would be a lot of chances for it to happen - there’d be something like \(10^{40}\) live bacteria at the start of the experiment - and the creativity of the evolution of bacteria is surprising.&lt;/p&gt;

&lt;p&gt;What if we added more fish?&lt;/p&gt;

&lt;p&gt;As we keep adding fish to this body, the temperature and pressure at the center will increase. When there are around \(10^{28}\) fish, the body will have roughly 10% the mass of the Sun, and fusion of hydrogen can start in its core. Let’s call the resulting object &lt;strong&gt;Starfish&lt;/strong&gt;. Most of the hydrogen will likely have floated to the surface, however, so fusion would probably be slower than normal. However, if we increase this by a factor of 100, with \(10^{30}\) fish and 10 solar masses, Starfish will have enough heat and pressure to fuse carbon and oxygen, its main constituents. At that size, the star will be a blue giant that might live 30 million years before either dying in a supernova or by releasing its outer layers to form a stellar nebula.&lt;/p&gt;

&lt;p&gt;It’d be silly to stop now. What if we added more fish?&lt;/p&gt;

&lt;p&gt;Einstein’s equations tell us that enough mass-energy within a given radius will collapse into a black hole, so in theory, if we just keep adding fish, that’s what we’ll end up with. Let’s call it &lt;strong&gt;Blackfish&lt;/strong&gt;, an appropriate name for something that’s large, black, hungry, and ultimately composed of lots of small fish. Black holes are also created in supernovae of massive stars, and since we have a star anyways, let’s consider that case. The causes, results, and mechanisms of supernovae vary a lot between stars with different masses and compositions, but as a rough figure, a star with 100 solar masses is likely to form a black hole. This corresponds to \(2*10^{31}\) fish, roughly 1000 for every atom in a human body. This star will have a relatively short lifetime of millions of years, eventually dying in a supernova that leaves behind a black hole. As far as current physics knows, that’s the end; if you add any more fish, you just get a bigger black hole.&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/physics/2020/06/17/fish-planet.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics/2020/06/17/fish-planet.html</guid>
        
        
        <category>physics</category>
        
      </item>
    
      <item>
        <title>How hard do you have to hit a chicken to cook it?</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;Some questions are timeless, innocent yet penetrating in their simplicity. Why is the sky blue? Why do things fall? How hard must one hit a chicken to cook it? It is this last mystery of the universe that we discuss today.&lt;/p&gt;

&lt;p&gt;There’s a &lt;a href=&quot;https://www.boredpanda.com/physics-major-calculates-how-hard-to-slap-chicken-to-cook-it/?utm_source=google&amp;amp;utm_medium=organic&amp;amp;utm_campaign=organic&quot;&gt;classic solution&lt;/a&gt; in which someone calculated that, if you slap a chicken at 3726 mph, it will be cooked. However, this analysis just calculates how hard you’d have to hit a chicken to get it to cooking temperature; you need to keep it at that temperature for it to cook. One slap won’t work unless you get it so hot that it cooks while it’s cooling.&lt;/p&gt;

&lt;p&gt;A real answer to this vital conundrum needs to consider how fast a chicken cools. A body at a nonzero temperature is constantly radiating energy as blackbody radiation; this is what you see in incandescent lightbulbs or when glass glows during glassblowing. To keep an object at a given temperature, you have to continuously give it the same energy it’s radiating away. A typical-sized chicken at 165 F is radiating away roughly 2000 watts of power, around 300 times the power used in a fluorescent lightbulb. To avoid losing any heat to contact with the air, let’s assume we dangle the chicken from a string in a large vacuum chamber. Let’s also assume you and a few friends are hitting the chicken with baseball bats like a pinata. In order to keep the chicken at 165 F for the minutes needed to cook it, it would be enough to have four people each hitting it once a second with a bat swung at 75 mph, about the speed with which a pro swings. Four major-league baseball players wearing pressure suits in a vacuum chamber each hitting a dangling chicken with a baseball bat once a second could cook it in a few minutes.&lt;/p&gt;

</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/physics/2020/06/17/chicken-cooking.html</link>
        <guid isPermaLink="true">http://localhost:4000/physics/2020/06/17/chicken-cooking.html</guid>
        
        
        <category>physics</category>
        
      </item>
    
      <item>
        <title>The expected cost of breaking quarantine</title>
        <description>&lt;!-- ![grid26] --&gt;
&lt;!--exc--&gt;

&lt;p&gt;In countries around the world, a common message has issued from government to populace: isolate yourselves, or covid-19 will take more lives.  Most of the US is now under some degree of quarantine.  For the average American, though, the risks of coronavirus are minimal; isolation serves more as a means to shield society at large than as individual protection.  That raises a natural question, though: since quarantine in the US isn’t enforced, how bad would it be to see people anyways?&lt;/p&gt;

&lt;p&gt;There’s a worldwide effort to predict the effects of coronavirus given different courses of societal action, but there’s been less focus on the risks of individual actions.  With the climate crisis, we have both global predictions and ways to gauge individuals’ impacts; a person’s carbon footprint is quantifiable.  With this pandemic, however, we agree that social distancing is crucial, but personal impact isn’t quantified.  As every person strikes a balance between isolation and normalcy - few people simply never go outside - it would help to have some way to gauge just how bad social interaction is.&lt;/p&gt;

&lt;p&gt;One way to get a rough sense of the effects of different actions is with simple models of disease spread through a population.  As a first stab at a calculation, consider a population of \(N\) people of which \(c\) currently have a disease.  This disease shows no symptoms, so infected people don’t know to isolate, and it never goes away - these \(c\) people are contagious forever.  As a model for spreading, imagine people interact in randomly-drawn pairs, and if one person has the disease and the other doesn’t, the first will give it to the second.&lt;/p&gt;

&lt;p&gt;It’s clear from the start that the whole population’s eventually going to be infected, but one thing we can quantify is &lt;em&gt;how many people get the disease earlier than they would’ve otherwise if one person doesn’t isolate.&lt;/em&gt;  Every infected person’s going to have a number of people downstream from them including people they’ve infected, people those people have infected, and so on.  In this model, people downstream of you would’ve stayed healthy longer if you’d isolated.  Because all the \(N - c\) uninfected people will eventually be downstream from one of the \(c\) infected people, each of the \(c\) infected people expects to have \(\frac{N - c}{c}\) people downstream on average.&lt;/p&gt;

&lt;p&gt;Now imagine person \(i\) and person \(j\) have contact.  If one is infected and the other is uninfected, which happens with new-case probability \(p_{\text{nc}} = \frac{2c(N - c)}{N^2}\), a new infected person is created; there are now \(c + 1\) sick people, and the extra infected person has on average \(\frac{N - c - 1}{c + 1}\) people downstream who will get sick sooner than they would’ve if person \(i\) and person \(j\) hadn’t interacted.  Including the one of those two who got sick, that’s \(x = \frac{N - c - 1}{c + 1} + 1 = \frac{N}{c + 1}\) infections sped up per new case.  Multiplying by the probability and assuming that \(N, c \gg 1\) gives \(p_{\text{nc}} x = \frac{2c(N - c)}{N^2}\frac{N}{c + 1} \approx \frac{2(N - c)}{N}\).  When \(c \ll N\), like it is for the coronavirus, this is roughly \(2\); in this model, interacting with a random person you could’ve avoided leads to two cases of the disease happening earlier than they would’ve otherwise.  This is approximately true whether the current rate is one in fifty or one in a million; if the probability of a stranger having the disease is lower, then the expected number of downstream infections is proportionally higher.  This number of two infections is just an average - it has a high statistical uncertainty - but reasoning based on averages makes sense in this case.&lt;/p&gt;

&lt;p&gt;This isn’t really how coronavirus works, though; covid doesn’t last forever, and people with symptoms can isolate.  To make the model more realistic, let’s say there are \(c\) \(c\)urrently \(c\)orona-\(c\)ontagious people who have the disease but either haven’t shown symptoms yet or won’t ever and \(r\) people who are either currently showing symptoms and isolating or have already gotten better, so they can’t infect anyone else and are \(r\)emoved from the calculation.  For now, we’ll still assume every person eventually gets the disease.  The new probability of a new case when two people interact is \(p'_{\text{nc}} = \frac{2c(N - c - r)}{N^2}\), and the expected number of downstream people from a new infected person (plus that person) is \(x' = \frac{N - r}{c + 1}\).  The expected number of accelerated infections per interaction is \(p'_{\text{nc}}x' = \frac{2(N - r)(N - c - r)}{N^2}\).  For the coronavirus, \(r,c \ll N\) - only a small fraction of the population has been infected - so this simplifies to 2 again.  Even allowing for isolation and resistance, interacting with a random person leads to, on average, two cases of coronavirus happening earlier than they would’ve. &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;So far, we’ve assumed that everybody’s going to get the coronavirus.  That’s not likely, though; in a worst-case scenario, once ~70% of the population has been infected, herd immunity should extinguish the virus, and before then either a vaccine or extensive contact tracing could stop the virus’ spread.  If the endgame of the pandemic is herd immunity, individual actions will affect when people get the disease, but the total number of cases will mostly be set by the herd immunity threshold.  However, if a vaccine stops the spread, for example, timing matters more.  Suppose that 1000 people in a large state currently have the virus, and by the time a vaccine stops the pandemic 100000 new people have gotten it.  If the population’s much bigger than 100000, the cases don’t interact, and we can estimate that if there were 1001 contagious people to begin with, there would’ve been 100100 new cases instead; every current case causes 100 future cases.  As before, let’s say \(N\) is the population, \(r\) people are recovered or symptomatic, \(c\) people are contagious and asymptomatic, and \(s\) people are currently uninfected but will eventually get the disease (\(s\)urely \(s\)ometime \(s\)icken).  Assuming \(s\) is small compared to the total number of uninfected people as discussed (\(s \ll N - r - c\)), the average current infection causes \(y = s/c\) preventable cases.  The probability of an interaction generating a new case is still \(p'_{\text{nc}} = \frac{2c(N - c - r)}{N^2}\), which means that, in this model of a terminating pandemic, each extra interaction between strangers causes \(p'_{\text{nc}} y = \frac{2s(N - c - r)}{N^2} \approx \frac{2s}{N}\) preventable cases.  This means that, optimistically, if ~25% of the population eventually gets coronavirus and a much smaller percentage has it now, every unnecessary interaction with a stranger will cause ~0.5 extra cases of the disease.&lt;/p&gt;

&lt;p&gt;This model has a few key shortcomings:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Interactions aren’t all or nothing; most interactions fall between six feet of separation and sharing bodily fluids, so the final numbers should be reduced proportional to how likely the interaction in question is to spread coronavirus.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actual estimates of the number of people who’ll get coronavirus are often over 50%, in which case cases interfere with each other and the final result will be less than \(\frac{2s}{N}\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interactions aren’t random, and interacting with someone you’ve interacted with before isn’t as bad as talking to a stranger.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If society works harder to stop the pandemic the worse the pandemic is, then there’s an unconsidered second order effect in which more cases of the coronavirus now can lead to fewer later.  Also, as mentioned, if the endgame of the pandemic is herd immunity, then in terms of total number of cases, it’s almost irrelevant what an individual does.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These are all reasons why our calculation of \(\frac{2s}{N}\) extra cases per interaction is an overestimate.  A slightly more complicated calculation might be able to account for (2), and we can deal with (3) by saying that this calculation only applies to interactions with people you don’t already see very frequently and counts back-to-back interactions with the same person as one interaction.  (4) deals with different ways the pandemic could play out; nobody can predict that, but for now let’s suppose we’re in a regime where the calculation applies (where more cases now basically lead to more total cases); perhaps there’s only a 50% chance of that, in which case you could reduce the final expected cost by a factor of two.&lt;/p&gt;

&lt;p&gt;(1), the most glaring flaw, is one we can account for, though.  This analysis assumed every contaminated interaction had 100% probability of spreading the disease; what is that probability in reality?  It would take an expert and a lot of information on the exact details of the interaction to give a precise answer, but we can make a decent approximation.  Epidemiologists use \(R_0\) to represent the average number of people each person with coronavirus infects.  If \(R_0 &amp;gt; 1\), the pandemic is getting exponentially worse, and if \(R_0 &amp;lt; 1\), it’s getting better.  \(R_0\) is estimated at 2 to 3 before social distancing, but it now varies greatly; however, since the number of new cases per day in the US &lt;a href=&quot;https://news.google.com/covid19/map?hl=en-US&amp;amp;gl=US&amp;amp;ceid=US%3Aen&amp;amp;mid=%2Fm%2F09c7w0&quot;&gt;has been pretty constant for a few weeks&lt;/a&gt; (though it varies by state), we can estimate \(R_0 \approx 1\).  If the average US citizen with coronavirus interacts with \(q\) people while infectious, then each interaction has  a \(1/q\) probability of transmission.  Asymptomatic patients are contagious for about two weeks, while symptomatic patients are contagious for a few days before their symptoms start and they hopefully isolate, and with a &lt;a href=&quot;https://www.hopkinsguides.com/hopkins/view/Johns_Hopkins_ABX_Guide/540747/all/Coronavirus_COVID_19__SARS_CoV_2_&quot;&gt;25-50% probability of a case being asymptomatic&lt;/a&gt; let’s estimate that the average contagious period is five days.  How many interactions do you think the average American has in five days during this quarantine?  I couldn’t find polling data estimating this, but in Berkeley it seems like most people only get close to a handful of other people without a mask.  Let’s give a highball estimate of 20 relatively close interactions over five days and suppose that only these close interactions will spread the virus.  That puts each close, maskless interaction at a 5% chance of spreading the virus.&lt;/p&gt;

&lt;p&gt;With this estimate, we can adjust our final number: assuming 25% of the population gets coronavirus before it’s stopped and each contaminated interaction has a 5% chance of spreading the disease, we calculate that each time you have a close interaction with a new person it causes 0.5*0.05 = 0.025 new, preventable cases of coronavirus.  With a death rate of 1%, this gives 0.00025 deaths/interaction.  That’s quite high - odds of 1 in 4000 - and with the assumptions we’ve made, it might even be an underestimate.  If a fraction of a life seems intangible, Prof. Oskar Hallatschek pointed out that governments quantify the &lt;a href=&quot;https://en.wikipedia.org/wiki/Value_of_life&quot;&gt;value of a human life&lt;/a&gt;; it usually falls around a few million dollars.  If the government fined people for interactions in proportion to their risk to human life, with this estimate the fine would be around $1000.&lt;/p&gt;

&lt;p&gt;These calculations are rough; though I think their logic is sound, they are carved with a heavy chisel designed only to give the right basic shape.  That is far better than nothing, though, and the hazy figure of a 0.025% chance of causing the end of a human life when interacting with a stranger is a far more tangible motivation than a government mandate to stay inside for the vague public good.  It gives a tool for reasoning; some things, like doctor’s visits, are clearly worth the risk, while others clearly aren’t, and though in most cases the right thing might’ve been intuitive, in borderline cases it helps to have a risk in mind.  At one step above the individual level, with restaurants and their ilk considering reopening, that risk of 1 in 4000 quickly compounds; if just 10 people all interact, that’s \(10 + 9 + ... + 1 = 55\) interactions, with over a 1% chance of causing a death, and that’s not accounting for the fact that interactions with people who’ve just interacted are riskier than normal.  If 100 people share germs at a concert, that gives 5050 interactions, over one statistical death, but that’s a wild underestimate for the same reason.  At a time when the US population is getting anxious and rash, it helps to have even a rough estimate of the risks involved as an aid to rationality.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Someone I talked to after posting this pointed out that the number of downstream people equalling the number of accelerated infections doesn’t make sense.  He’s right - it’s actually possible for the original infection tree to catch up to the new branch.  The simplest case is when the first random contact in the model happens to be between the two people who had the unnecessary contact we’re considering - it doesn’t matter if they interacted because they’re about to interact again, and everybody who’s downstream of the extra patient would’ve gotten sick at the same time anyways.  The numbers of accelerated cases are actually just upper bounds.  I’ve left the analysis there because it explains variables and ideas used in the rest of the post, which this problem doesn’t affect. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 17 May 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/stats/2020/05/17/covid-calculation.html</link>
        <guid isPermaLink="true">http://localhost:4000/stats/2020/05/17/covid-calculation.html</guid>
        
        
        <category>stats</category>
        
      </item>
    
  </channel>
</rss>
